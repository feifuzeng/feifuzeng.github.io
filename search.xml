<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker学习笔记-命令大全]]></title>
    <url>%2F2020%2F02%2F04%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[简介Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从 Apache2.0 协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 Docker 从 17.03 版本之后分为 CE（Community Edition: 社区版） 和 EE（Enterprise Edition: 企业版），我们用社区版就可以了。 这里主要介绍docker相关的基本命令 容器生命周期管理Docker run 命令docker run ：创建一个新的容器并运行一个命令 语法1docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； –name=”nginx-lb”: 为容器指定一个名称； –dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； –dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h “mars”: 指定容器的hostname； -e username=”ritchie”: 设置环境变量； –env-file=[]: 从指定文件读入环境变量； –cpuset=”0-2” or –cpuset=”0,1,2”: 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； –net=”bridge”: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； –link=[]: 添加链接到另一个容器； –expose=[]: 开放一个端口或一组端口； –volume , -v: 绑定一个卷 实例使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。1docker run --name mynginx -d nginx:latest 使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。1docker run -P -d nginx:latest 使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。1docker run -p 80:80 -v /data:/data -d nginx:latest 绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。1$ docker run -p 127.0.0.1:80:8080/tcp ubuntu bash 使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。12runoob@runoob:~$ docker run -it nginx:latest /bin/bashroot@b8573233d675:/# Docker start/stop/restart 命令docker start :启动一个或多个已经被停止的容器docker stop :停止一个运行中的容器docker restart :重启容器 语法1docker start [OPTIONS] CONTAINER [CONTAINER...] 1docker stop [OPTIONS] CONTAINER [CONTAINER...] 1docker restart [OPTIONS] CONTAINER [CONTAINER...] 实例启动已被停止的容器myrunoob1docker start myrunoob 停止运行中的容器myrunoob1docker stop myrunoob 重启容器myrunoob1docker restart myrunoob Docker kill 命令docker kill:杀掉一个运行中的容器。 语法1docker kill [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -s :向容器发送一个信号实例杀掉运行中的容器mynginx12runoob@runoob:~$ docker kill -s KILL mynginxmynginx Docker rm 命令docker rm ：删除一个或多个容器。 语法1docker rm [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -f :通过 SIGKILL 信号强制删除一个运行中的容器。 -l :移除容器间的网络连接，而非容器本身。 -v :删除与容器关联的卷。实例强制删除容器 db01、db02： 1docker rm -f db01 db02 移除容器 nginx01 对容器 db01 的连接，连接名 db：1docker rm -l db 删除容器 nginx01, 并删除容器挂载的数据卷：1docker rm -v nginx01 删除所有已经停止的容器：1docker rm $(docker ps -a -q) Docker pause/unpause 命令docker pause;:暂停容器中所有的进程。.docker unpause;:恢复容器中所有的进程。 语法1docker pause [OPTIONS] CONTAINER [CONTAINER...] 1docker unpause [OPTIONS] CONTAINER [CONTAINER...] 实例暂停数据库容器db01提供服务。1docker pause db01 恢复数据库容器db01提供服务。1docker unpause db01 Docker create 命令docker create ：创建一个新的容器但不启动它用法同docker run 语法1docker create [OPTIONS] IMAGE [COMMAND] [ARG...] 语法同docker run 实例使用docker镜像nginx:latest创建一个容器,并将容器命名为myrunoob12runoob@runoob:~$ docker create --name myrunoob nginx:latest 09b93464c2f75b7b69f83d56a9cfc23ceb50a48a9db7652ee4c27e3e2cb1961f Docker exec 命令docker exec ：在运行的容器中执行命令 语法1docker exec [OPTIONS] CONTAINER COMMAND [ARG...] OPTIONS说明： -d :分离模式: 在后台运行 -i :即使没有附加也保持STDIN 打开 -t :分配一个伪终端实例在容器 mynginx 中以交互模式执行容器内 /root/runoob.sh 脚本:12runoob@runoob:~$ docker exec -it mynginx /bin/sh /root/runoob.shhttp://www.runoob.com/ 在容器 mynginx 中开启一个交互模式的终端:12runoob@runoob:~$ docker exec -i -t mynginx /bin/bashroot@b1a0703e41e7:/# 也可以通过&nbsp;docker ps -a命令查看已经在运行的容器，然后使用容器 ID 进入容器。查看已经在运行的容器 ID：123# docker ps -a ...9df70f9a0714 openjdk "/usercode/script.sh…"... 第一列的 9df70f9a0714 就是容器 ID。通过 exec 命令对指定的容器执行 bash:1# docker exec -it 9df70f9a0714 /bin/bash 容器操作Docker ps 命令docker ps :列出容器 语法1docker ps [OPTIONS]OPTIONS 说明： -a :显示所有的容器，包括未运行的。 -f :根据条件过滤显示的内容。 –format :指定返回值的模板文件。 -l :显示最近创建的容器。 -n :列出最近创建的n个容器。 –no-trunc :不截断输出。 -q :静默模式，只显示容器编号。 -s :显示总的文件大小。实例列出所有在运行的容器信息。123runoob@runoob:~$ docker psCONTAINER ID IMAGE COMMAND ... PORTS NAMES09b93464c2f7 nginx:latest &amp;quot;nginx -g &amp;apos;daemon off&amp;quot; ... 80/tcp, 443/tcp myrunoob 输出详情介绍： CONTAINER ID: 容器 ID。 IMAGE:使用的镜像。 COMMAND:启动容器时运行的命令。 CREATED:容器的创建时间。 STATUS:容器状态。状态有7种： created（已创建） restarting（重启中） running（运行中） removing（迁移中） paused（暂停） exited（停止） dead（死亡） PORTS:容器的端口信息和使用的连接类型（tcp\udp）。 NAMES:自动分配的容器名称。 列出最近创建的5个容器信息。1234567runoob@runoob:~$ docker ps -n 5CONTAINER ID IMAGE COMMAND CREATED 09b93464c2f7 nginx:latest &amp;quot;nginx -g &amp;apos;daemon off&amp;quot; 2 days ago ... b8573233d675 nginx:latest &amp;quot;/bin/bash&amp;quot; 2 days ago ... b1a0703e41e7 nginx:latest &amp;quot;nginx -g &amp;apos;daemon off&amp;quot; 2 days ago ... f46fb1dec520 5c6e1090e771 &amp;quot;/bin/sh -c &amp;apos;set -x \t&amp;quot; 2 days ago ... a63b4a5597de 860c279d2fec &amp;quot;bash&amp;quot; 2 days ago ... 列出所有创建的容器ID。123runoob@runoob:~$ docker ps -a -q09b93464c2f7b8573233d675 Docker inspect 命令docker inspect : 获取容器/镜像的元数据。 语法1docker inspect [OPTIONS] NAME|ID [NAME|ID...] OPTIONS说明： -f :指定返回值的模板文件。 -s :显示总的文件大小。 –type :为指定类型返回JSON。 实例获取镜像mysql:5.6的元信息。 1234567891011121314151617181920212223runoob@runoob:~$ docker inspect mysql:5.6[ &#123; "Id": "sha256:2c0964ec182ae9a045f866bbc2553087f6e42bfc16074a74fb820af235f070ec", "RepoTags": [ "mysql:5.6" ], "RepoDigests": [], "Parent": "", "Comment": "", "Created": "2016-05-24T04:01:41.168371815Z", "Container": "e0924bc460ff97787f34610115e9363e6363b30b8efa406e28eb495ab199ca54", "ContainerConfig": &#123; "Hostname": "b0cf605c7757", "Domainname": "", "User": "", "AttachStdin": false, "AttachStdout": false, "AttachStderr": false, "ExposedPorts": &#123; "3306/tcp": &#123;&#125; &#125;,... 获取正在运行的容器mymysql的 IP。 12runoob@runoob:~$ docker inspect --format='&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' mymysql172.17.0.3 Docker top 命令docker top :查看容器中运行的进程信息，支持 ps 命令参数。 语法1docker top [OPTIONS] CONTAINER [ps OPTIONS] 容器运行时不一定有/bin/bash终端来交互执行top命令，而且容器还不一定有top命令，可以使用docker top来实现查看container中正在运行的进程。 实例查看容器mymysql的进程信息。 123runoob@runoob:~/mysql$ docker top mymysqlUID PID PPID C STIME TTY TIME CMD999 40347 40331 18 00:58 ? 00:00:02 mysqld 查看所有运行容器的进程信息。 1for i in `docker ps |grep Up|awk '&#123;print $1&#125;'`;do echo \ &amp;&amp;docker top $i; done Docker export 命令docker export :将文件系统作为一个tar归档文件导出到STDOUT。 语法1docker export [OPTIONS] CONTAINER OPTIONS说明： -o :将输入内容写到文件。 实例将id为a404c6c174a2的容器按日期保存为tar文件。 123runoob@runoob:~$ docker export -o mysql-`date +%Y%m%d`.tar a404c6c174a2runoob@runoob:~$ ls mysql-`date +%Y%m%d`.tarmysql-20160711.tar Docker port 命令docker port :列出指定的容器的端口映射，或者查找将PRIVATE_PORT NAT到面向公众的端口。 语法1docker port [OPTIONS] CONTAINER [PRIVATE_PORT[/PROTO]] 实例查看容器mynginx的端口映射情况。 12runoob@runoob:~$ docker port mymysql3306/tcp -&gt; 0.0.0.0:3306 容器rootfs命令Docker commit 命令docker commit :从容器创建一个新的镜像。 语法1docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] OPTIONS说明： -a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 实例将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。 12345runoob@runoob:~$ docker commit -a "runoob.com" -m "my apache" a404c6c174a2 mymysql:v1 sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057runoob@runoob:~$ docker images mymysql:v1REPOSITORY TAG IMAGE ID CREATED SIZEmymysql v1 37af1236adef 15 seconds ago 329 MB Docker cp 命令docker cp :用于容器与主机之间的数据拷贝。 语法12docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH OPTIONS说明： -L :保持源目标中的链接 实例将主机/www/runoob目录拷贝到容器96f7f14e99ab的/www目录下。 1docker cp /www/runoob 96f7f14e99ab:/www/ 将主机/www/runoob目录拷贝到容器96f7f14e99ab中，目录重命名为www。 1docker cp /www/runoob 96f7f14e99ab:/www 将容器96f7f14e99ab的/www目录拷贝到主机的/tmp目录中。 1docker cp 96f7f14e99ab:/www /tmp/ Docker diff 命令docker diff : 检查容器里文件结构的更改。 语法1docker diff [OPTIONS] CONTAINER 实例查看容器mymysql的文件结构更改。 12345678runoob@runoob:~$ docker diff mymysqlA /logsA /mysql_dataC /runC /run/mysqldA /run/mysqld/mysqld.pidA /run/mysqld/mysqld.sockC /tmp 镜像仓库Docker login/logout 命令docker login : 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub docker logout : 登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub 语法12docker login [OPTIONS] [SERVER]docker logout [OPTIONS] [SERVER] OPTIONS说明： -u :登陆的用户名 -p :登陆的密码 实例登陆到Docker Hub 1docker login -u 用户名 -p 密码 登出Docker Hub 1docker logout Docker pull 命令docker pull : 从镜像仓库中拉取或者更新指定镜像 语法1docker pull [OPTIONS] NAME[:TAG|@DIGEST] OPTIONS说明： -a :拉取所有 tagged 镜像 –disable-content-trust :忽略镜像的校验,默认开启 实例从Docker Hub下载java最新版镜像。 1docker pull java 从Docker Hub下载REPOSITORY为java的所有镜像。 1docker pull -a java Docker push 命令docker push : 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库 语法1docker push [OPTIONS] NAME[:TAG] OPTIONS说明： –disable-content-trust :忽略镜像的校验,默认开启 实例上传本地镜像myapache:v1到镜像仓库中。 1docker push myapache:v1 Docker search 命令docker search : 从Docker Hub查找镜像 语法1docker search [OPTIONS] TERM OPTIONS说明： –automated :只列出 automated build类型的镜像； –no-trunc :显示完整的镜像描述； -s :列出收藏数不小于指定值的镜像。 实例从Docker Hub查找所有镜像名包含java，并且收藏数大于10的镜像 12345678runoob@runoob:~$ docker search -s 10 javaNAME DESCRIPTION STARS OFFICIAL AUTOMATEDjava Java is a concurrent, class-based... 1037 [OK] anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC ... 115 [OK]develar/java 46 [OK]isuper/java-oracle This repository contains all java... 38 [OK]lwieske/java-8 Oracle Java 8 Container - Full + ... 27 [OK]nimmis/java-centos This is docker images of CentOS 7... 13 [OK] 参数说明： NAME: 镜像仓库源的名称 DESCRIPTION: 镜像的描述 OFFICIAL: 是否 docker 官方发布 stars: 类似 Github 里面的 star，表示点赞、喜欢的意思。 AUTOMATED: 自动构建。 本地镜像管理Docker images 命令docker images : 列出本地镜像。 语法1docker images [OPTIONS] [REPOSITORY[:TAG]] OPTIONS说明： -a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； –digests :显示镜像的摘要信息； -f :显示满足条件的镜像； –format :指定返回值的模板文件； –no-trunc :显示完整的镜像信息； -q :只显示镜像ID。 实例查看本地镜像列表。 123456789101112runoob@runoob:~$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmymysql v1 37af1236adef 5 minutes ago 329 MBrunoob/ubuntu v4 1c06aa18edee 2 days ago 142.1 MB&lt;none&gt; &lt;none&gt; 5c6e1090e771 2 days ago 165.9 MBhttpd latest ed38aaffef30 11 days ago 195.1 MBalpine latest 4e38e38c8ce0 2 weeks ago 4.799 MBmongo 3.2 282fd552add6 3 weeks ago 336.1 MBredis latest 4465e4bcad80 3 weeks ago 185.7 MBphp 5.6-fpm 025041cd3aa5 3 weeks ago 456.3 MBpython 3.5 045767ddf24a 3 weeks ago 684.1 MB... 列出本地镜像中REPOSITORY为ubuntu的镜像列表。 1234root@runoob:~# docker images ubuntuREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 14.04 90d5884b1ee0 9 weeks ago 188 MBubuntu 15.10 4e3b13c8a266 3 months ago 136.3 MB Docker rmi 命令docker rmi : 删除本地一个或多少镜像。 语法1docker rmi [OPTIONS] IMAGE [IMAGE...] OPTIONS说明： -f :强制删除； –no-prune :不移除该镜像的过程镜像，默认移除； 实例强制删除本地镜像 runoob/ubuntu:v4。 1234root@runoob:~# docker rmi -f runoob/ubuntu:v4Untagged: runoob/ubuntu:v4Deleted: sha256:1c06aa18edee44230f93a90a7d88139235de12cd4c089d41eed8419b503072beDeleted: sha256:85feb446e89a28d58ee7d80ea5ce367eebb7cec70f0ec18aa4faa874cbd97c73 Docker tag 命令docker tag : 标记本地镜像，将其归入某一仓库。 语法1docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG] 实例将镜像ubuntu:15.10标记为 runoob/ubuntu:v3 镜像。 1234root@runoob:~# docker tag ubuntu:15.10 runoob/ubuntu:v3root@runoob:~# docker images runoob/ubuntu:v3REPOSITORY TAG IMAGE ID CREATED SIZErunoob/ubuntu v3 4e3b13c8a266 3 months ago 136.3 MB Docker build 命令docker build 命令用于使用 Dockerfile 创建镜像。 语法1docker build [OPTIONS] PATH | URL | - OPTIONS说明： –build-arg=[] :设置镜像创建时的变量； –cpu-shares :设置 cpu 使用权重； –cpu-period :限制 CPU CFS周期； –cpu-quota :限制 CPU CFS配额； –cpuset-cpus :指定使用的CPU id； –cpuset-mems :指定使用的内存 id； –disable-content-trust :忽略校验，默认开启； -f :指定要使用的Dockerfile路径； –force-rm :设置镜像过程中删除中间容器； –isolation :使用容器隔离技术； –label=[] :设置镜像使用的元数据； -m :设置内存最大值； –memory-swap :设置Swap的最大值为内存+swap，”-1”表示不限swap； –no-cache :创建镜像的过程不使用缓存； –pull :尝试去更新镜像的新版本； –quiet, -q :安静模式，成功后只输出镜像 ID； –rm :设置镜像成功后删除中间容器； –shm-size :设置/dev/shm的大小，默认值是64M； –ulimit :Ulimit配置。 –tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 –network: 默认 default。在构建期间设置RUN指令的网络模式 实例使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1。 1docker build -t runoob/ubuntu:v1 . 使用URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像。 1docker build github.com/creack/docker-firefox 也可以通过 -f Dockerfile 文件的位置： 1$ docker build -f /path/to/a/Dockerfile . 在 Docker 守护进程执行 Dockerfile 中的指令前，首先会对 Dockerfile 进行语法检查，有语法错误时会返回： 123$ docker build -t test/myapp .Sending build context to Docker daemon 2.048 kBError response from daemon: Unknown instruction: RUNCMD Docker history 命令docker history : 查看指定镜像的创建历史。 语法1docker history [OPTIONS] IMAGE OPTIONS说明： -H :以可读的格式打印镜像大小和日期，默认为true； –no-trunc :显示完整的提交记录； -q :仅列出提交记录ID。 实例查看本地镜像runoob/ubuntu:v3的创建历史。 123456root@runoob:~# docker history runoob/ubuntu:v3IMAGE CREATED CREATED BY SIZE COMMENT4e3b13c8a266 3 months ago /bin/sh -c #(nop) CMD ["/bin/bash"] 0 B &lt;missing&gt; 3 months ago /bin/sh -c sed -i 's/^#\s*\(deb.*universe\)$/ 1.863 kB &lt;missing&gt; 3 months ago /bin/sh -c set -xe &amp;&amp; echo '#!/bin/sh' &gt; /u 701 B &lt;missing&gt; 3 months ago /bin/sh -c #(nop) ADD file:43cb048516c6b80f22 136.3 MB Docker save 命令docker save : 将指定镜像保存成 tar 归档文件。 语法1docker save [OPTIONS] IMAGE [IMAGE...] OPTIONS 说明： -o :输出到的文件。 实例将镜像 runoob/ubuntu:v3 生成 my_ubuntu_v3.tar 文档 123runoob@runoob:~$ docker save -o my_ubuntu_v3.tar runoob/ubuntu:v3runoob@runoob:~$ ll my_ubuntu_v3.tar-rw------- 1 runoob runoob 142102016 Jul 11 01:37 my_ubuntu_v3.ta Docker load 命令docker load : 导入使用 docker save 命令导出的镜像。 语法1docker load [OPTIONS] OPTIONS 说明： –input , -i : 指定导入的文件，代替 STDIN。 –quiet , -q : 精简输出信息。 实例导入镜像： 12345678910111213141516171819202122232425$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE$ docker load &lt; busybox.tar.gzLoaded image: busybox:latest$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest 769b9341d937 7 weeks ago 2.489 MB$ docker load --input fedora.tarLoaded image: fedora:rawhideLoaded image: fedora:20$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest 769b9341d937 7 weeks ago 2.489 MBfedora rawhide 0d20aec6529d 7 weeks ago 387 MBfedora 20 58394af37342 7 weeks ago 385.5 MBfedora heisenbug 58394af37342 7 weeks ago 385.5 MBfedora latest 58394af37342 7 weeks ago 385.5 MB Docker import 命令docker import : 从归档文件中创建镜像。 语法1docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]] OPTIONS说明： -c :应用docker 指令创建镜像； -m :提交时的说明文字； 实例从镜像归档文件my_ubuntu_v3.tar创建镜像，命名为runoob/ubuntu:v4 12345runoob@runoob:~$ docker import my_ubuntu_v3.tar runoob/ubuntu:v4 sha256:63ce4a6d6bc3fabb95dbd6c561404a309b7bdfc4e21c1d59fe9fe4299cbfea39runoob@runoob:~$ docker images runoob/ubuntu:v4REPOSITORY TAG IMAGE ID CREATED SIZErunoob/ubuntu v4 63ce4a6d6bc3 20 seconds ago 142.1 MB 参考 https://www.runoob.com/docker/docker-command-manual.html]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis进阶使用-一级缓存与二级缓存]]></title>
    <url>%2F2020%2F02%2F03%2Fmybatis%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8-%E4%B8%80%E7%BA%A7%E7%BC%93%E5%AD%98%E4%B8%8E%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[简介缓存是一般的ORM 框架都会提供的功能，目的就是提升查询的效率和减少数据库的压力。跟Hibernate 一样，MyBatis 也有一级缓存和二级缓存，并且预留了集成第三方缓存的接口。 一级缓存 什么是一级缓存？ Mybatis一级缓存实际上就是一个依赖于SqlSession的缓存对象，PerpetualCache里面的结构很简单，通过一个k-v结构的cache维护缓存数据。一级缓存默认开启123public class PerpetualCache implements Cache &#123; private final String id; private Map&lt;Object, Object&gt; cache = new HashMap&lt;Object, Object&gt;(); 一级缓存的生命周期？ PerpetualCache的生命周期是和SqlSession相关的，即只有在同一个SqlSession中，一级缓存才会用到。 如果SqlSession调用了close()方法，会释放掉一级缓存PerpetualCache对象，一级缓存将不可用； 如果SqlSession调用了clearCache()，会清空PerpetualCache对象中的数据，但是该对象仍可使用； SqlSession中执行了任何一个update操作(update()、delete()、insert()) ，都会清空PerpetualCache对象的数据，但是该对象可以继续使用； 开发时，如何才能真正用到一级缓存？ 我们经常在某个方法中进行多次数据库查询，在实际场景中，每次的数据库查询都会开启一个新的会话（SqlSession）。这种情况下我们是没有用到一级缓存的，因为根本就没有复用到SqlSession。 那么我们怎样控制程序复用SqlSession，使get()能用到一级缓存呢?其中一种办法就是开启一个事务。 二级缓存 什么是二级缓存？ 二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。二级缓存 底层还是 HashMap 架构。 二级缓存的生命周期？ 映射语句中的所有select语句将会被缓存 映射语句中的所有insert,update和delete语句会刷新缓存。 缓存会使用默认的Latest Recently Used(LRU,最近最少使用的)算法来回收 根据时间表，比如No Flush Interval,(CNFI,没有刷新间隔)，缓存不会以任何时间顺序来刷新。 缓存会存储列表集合或对象(无论查询方法返回什么)的1024个引用 缓存会被视为是read/write(可读/可写)的缓存，意味着对象检索不是共享的，而且可以安全的被调用者修改，不干扰其他调用者或者线程所做的潜在修改。 如何才能用到二级缓存？ 二级缓存默认是不开启的，需要手动开启二级缓存，实现二级缓存的时候，MyBatis要求返回的POJO必须是可序列化的,否则会抛出异常。开启二级缓存的条件也是比较简单，通过直接在 MyBatis 配置文件中通过1&lt;setting name="cacheEnabled" value="true"/&gt; &lt;!-- 二级缓存开启 --&gt; 或者在springboot项目配置文件中增加如下配置项：1mybatis.configuration.cache-enabled=true 来开启二级缓存，还需要在 Mapper 的xml 配置文件中加入 &lt;cache&gt;标签 设置 cache 标签的属性 cache 标签有多个属性，一起来看一些这些属性分别代表什么意义 eviction: 缓存回收策略，有这几种回收策略 LRU - 最近最少回收，移除最长时间不被使用的对象 FIFO - 先进先出，按照缓存进入的顺序来移除它们 SOFT - 软引用，移除基于垃圾回收器状态和软引用规则的对象 WEAK - 弱引用，更积极的移除基于垃圾收集器和弱引用规则的对象 默认是 LRU 最近最少回收策略 flushinterval 缓存刷新间隔，缓存多长时间刷新一次，默认不清空，设置一个毫秒值 readOnly: 是否只读；true 只读，MyBatis 认为所有从缓存中获取数据的操作都是只读操作，不会修改数据。MyBatis 为了加快获取数据，直接就会将数据在缓存中的引用交给用户。不安全，速度快。读写(默认)：MyBatis 觉得数据可能会被修改 size : 缓存存放多少个元素 type: 指定自定义缓存的全类名(实现Cache 接口即可) blocking： 若缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存。 参考 https://blog.csdn.net/n950814abc/article/details/97611712]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁初窥-分布式锁的三种实现方式]]></title>
    <url>%2F2020%2F02%2F03%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%88%9D%E7%AA%A5-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E4%B8%89%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[分布式锁应该具备哪些条件在分析分布式锁的三种实现方式之前，先了解一下分布式锁应该具备哪些条件： 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行； 高可用的获取锁与释放锁； 高性能的获取锁与释放锁； 具备可重入特性； 具备锁失效机制，防止死锁； 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败。 分布式锁的三种实现方式目前几乎很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。分布式的CAP理论告诉我们“任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。”所以，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。 在很多场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。有的时候，我们需要保证一个方法在同一时间内只能被同一个线程执行。123基于数据库实现分布式锁；基于缓存（Redis等）实现分布式锁；基于Zookeeper实现分布式锁； 基于数据库的实现方式基于数据库的实现方式的核心思想是：在数据库中创建一个表，表中包含方法名等字段，并在方法名字段上创建唯一索引，想要执行某个方法，就使用这个方法名向表中插入数据，成功插入则获取锁，执行完成后删除对应的行数据释放锁。 创建一个表 123456789DROP TABLE IF EXISTS `method_lock`;CREATE TABLE `method_lock` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` varchar(64) NOT NULL COMMENT '锁定的方法名', `desc` varchar(255) NOT NULL COMMENT '备注信息', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='锁定中的方法'; 想要执行某个方法，就使用这个方法名向表中插入数据： 1INSERT INTO method_lock (method_name, desc) VALUES ('methodName', '测试的methodName'); 因为我们对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。 成功插入则获取锁，执行完成后删除对应的行数据释放锁：1delete from method_lock where method_name ='methodName'; 注意：这只是使用基于数据库的一种方法，使用数据库实现分布式锁还有很多其他的玩法！ 使用基于数据库的这种实现方式很简单，但是对于分布式锁应该具备的条件来说，它有一些问题需要解决及优化： 因为是基于数据库实现的，数据库的可用性和性能将直接影响分布式锁的可用性及性能，所以，数据库需要双机部署、数据同步、主备切换； 不具备可重入的特性，因为同一个线程在释放锁之前，行数据一直存在，无法再次成功插入数据，所以，需要在表中新增一列，用于记录当前获取到锁的机器和线程信息，在再次获取锁的时候，先查询表中机器和线程信息是否和当前机器和线程相同，若相同则直接获取锁； 没有锁失效机制，因为有可能出现成功插入数据后，服务器宕机了，对应的数据没有被删除，当服务恢复后一直获取不到锁，所以，需要在表中新增一列，用于记录失效时间，并且需要有定时任务清除这些失效的数据； 不具备阻塞锁特性，获取不到锁直接返回失败，所以需要优化获取逻辑，循环多次去获取。 在实施的过程中会遇到各种不同的问题，为了解决这些问题，实现方式将会越来越复杂；依赖数据库需要一定的资源开销，性能问题需要考虑。 基于Redis实现分布式锁 选用Redis实现分布式锁原因： Redis有很高的性能； Redis命令对此支持较好，实现起来比较方便 使用命令介绍： SETNX SETNX key val：当且仅当key不存在时，set一个key为val的字符串，返回1；若key存在，则什么都不做，返回0。 expire expire key timeout：为key设置一个超时时间，单位为second，超过这个时间锁会自动释放，避免死锁。 delete delete key：删除key 在使用Redis实现分布式锁的时候，主要就会使用到这三个命令。 实现思想： 获取锁的时候，使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的value值为一个随机生成的UUID，通过此在释放锁的时候进行判断。 获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。 释放锁的时候，通过UUID判断是不是该锁，若是该锁，则执行delete进行锁释放。 基于Zookeeper实现分布式锁ZooKeeper是一个为分布式应用提供一致性服务的开源组件，它内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名。基于ZooKeeper实现分布式锁的步骤如下： （1）创建一个目录mylock；（2）线程A想获取锁就在mylock目录下创建临时顺序节点；（3）获取mylock目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁；（4）线程B获取所有节点，判断自己不是最小节点，设置监听比自己次小的节点；（5）线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是不是最小的节点，如果是则获得锁。 这里推荐一个Apache的开源库Curator，它是一个ZooKeeper客户端，Curator提供的InterProcessMutex是分布式锁的实现，acquire方法用于获取锁，release方法用于释放锁。 优点：具备高可用、可重入、阻塞锁特性，可解决失效死锁问题。 缺点：因为需要频繁的创建和删除节点，性能上不如Redis方式。 参考 https://blog.csdn.net/xlgen157387/article/details/79036337]]></content>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简述透明代理、正向代理、反向代理]]></title>
    <url>%2F2019%2F11%2F20%2F%E7%AE%80%E8%BF%B0%E9%80%8F%E6%98%8E%E4%BB%A3%E7%90%86%E3%80%81%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E3%80%81%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[概述什么是代理？代理本质上是一个服务器，可以类比为一个中介。 为了A访问到B，中间插入一个 C，C 就是代理。 分类透明代理应用： 防火墙、行为管理软件 主要作用： 透明代理的意思是客户端根本不需要知道有代理服务器的存在，它改变你的request fields（报文），并会传送真实IP，多用于路由器的NAT转发中。注意，加密的透明代理则是属于匿名代理，意思是不用设置使用代理了，例如Garden 2程序。 正向代理应用： vpn、翻墙上网 主要作用： 正向代理的典型用途是为在防火墙内的局域网客户端提供访问Internet的途径。 正向代理还可以使用缓冲特性(由mod_cache提供)减少网络使用率。 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息。 工作原理： 如下图我们可以通过配置代理服务器来将我们的请求代为发送到目标服务器，再由代理服务器将目标服务器的响应转发回本地。 特点： 1、隐藏了真实的请求客户端，服务端不知道真实的客户端是谁，客户端请求的服务都由代理服务器代替来请求。 2、客户端必须设置正向代理服务器，当然前提是要知道正向代理服务器的IP地址，还有代理程序的端口。 举个例子，国内的用户想要访问 Google 时，会被阻挡。所以这个时候把请求发送到另外一个代理服务器（可以访问 Google 的服务器）上，由其代为转发请求和接收响应内容。 反向代理应用： 安全、负载均衡方面的常用配置 主要作用： 保证证内网的安全，可以使用反向代理提供WAF功能，阻止web攻击，大型网站，通常将反向代理作为公网访问地址，Web服务器是内网。 负载均衡，通过反向代理服务器来优化网站的负载。 工作原理： 反向代理对外的表现都是透明的，客户不知道自己访问的是代理服务器，客户端也不需要任何配置就可以访问。当我们配置好代理服务器后，反向代理（Reverse Proxy）实际运行方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 正向代理和反向代理区别 正向代理中 ，proxy和client同属一个lan，对server透明。正向代理需要配置在client端； 反向代理中，proxy和server同属一个lan，对client透明。 反向代理需要配置在proxy端； 实际上proxy在两种代理中做的事都是代为请求和响应，不过从结构上看正好左右相反，故称为正反向代理。 参考 https://www.cnblogs.com/-abm/p/9894289.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[微服务实战SpringCloud之Spring Cloud Feign替代HTTP Client]]></title>
    <url>%2F2019%2F10%2F23%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9E%E6%88%98SpringCloud%E4%B9%8BSpring-Cloud-Feign%E6%9B%BF%E4%BB%A3HTTP-Client%2F</url>
    <content type="text"><![CDATA[简介在项目中我们有时候需要调用第三方的 API，微服务架构中这种情况则更是无法避免——各个微服务之间通信。比如一般的项目中，有时候我们会使用 HTTP Client 发送 HTTP 请求来进行调用，而在微服务架构，Spring Cloud 全家桶中，Spring Cloud Feign 则是更常见的选择。那么，我如何只使用 Spring Cloud Feign 而不引入整个 Spring Cloud 呢？ 什么是Feign?Feign是一个声明式的Web Service客户端，它的目的就是让Web Service调用更加简单。Feign提供了HTTP请求的模板，通过编写简单的接口和插入注解，就可以定义好HTTP请求的参数、格式、地址等信息。 而Feign则会完全代理HTTP请求，我们只需要像调用方法一样调用它就可以完成服务请求及相关处理。Feign整合了Ribbon和Hystrix，可以让我们不再需要显式地使用这两个组件。 总起来说，Feign具有如下特性： 可插拔的注解支持，包括Feign注解和JAX-RS注解; 支持可插拔的HTTP编码器和解码器; 支持Hystrix和它的Fallback; 支持Ribbon的负载均衡; 支持HTTP请求和响应的压缩。 这看起来有点像我们springmvc模式的Controller层的RequestMapping映射。这种模式是我们非常喜欢的。Feign是用@FeignClient来映射服务的。 首先找一个AIP免费的 API 特别多，github 上也有免费 API 地址汇总的 repo，但这些都太正式了。有趣的事物总是会相互吸引的，无意间我发现了这么一个网站，“渣男：说话的艺术”（lovelive.tools) ，每次请求都可以获取一句甜言蜜语（渣男语录）,特别良心的是，作者提供了 API 列表，给作者点赞！ 如何调用第三方服务？首先，我们先快速构建一个 Spring Boot 的 web 项目，这里我就省略了。然后在pom中添加feign的相关依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; 然后，在启动类添加响应的注解 @EnableFeignClients： 1234567@SpringBootApplication@EnableFeignClientspublic class SpringbootMiddlewareFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootMiddlewareFeignApplication.class, args); &#125;&#125; 接着，我们便可以配置我们的 Client 了，我们先创建一个接口类，比如叫 BadGuyFeignClient ，并声明为 FeignClient： 1234@FeignClientpublic interface BadGuyFeignClient &#123;&#125; @FeignClient有以下几个较常用属性： 属性名 默认值 作用 备注 value 空字符串 调用服务名称，和name属性相同，如果项目使用了 Ribbon，name属性会作为微服务的名称，用于服务发现； serviceId 空字符串 服务id，作用和name属性相同 已过期 name 空字符串 调用服务名称，和value属性相同，如果项目使用了 Ribbon，name属性会作为微服务的名称，用于服务发现； url 空字符串 url一般用于调试，可以手动指定@FeignClient调用的地址 decode404 false 配置响应状态码为404时是否应该抛出FeignExceptions configuration {} Feign配置类，可以自定义 Feign的 Encoder、Decoder、LogLevel、Contract； 参考FeignClientsConfiguration fallback void.class 定义容错的处理类，当调用远程接口失败或超时时，会调用对应接口的容错逻辑，fallback指定的类必须实现@FeignClient标记的接口 底层依赖hystrix，启动类要加上@EnableHystrix fallbackFactory void.class 工厂类，用于生成fallback类示例，通过这个属性我们可以实现每个接口通用的容错逻辑，减少重复的代码 path 空字符串 自动给所有方法的requestMapping前加上前缀，类似与controller类上的requestMapping 然后，我们便可以配置对应的属性，这里我们只是用来实现类似于 HTTP Client 的功能，所以只是简单配置了 url 和 path 这些属性： 123456789101112131415161718192021@FeignClient(name = "badGuy", url = "$&#123;bab.guy.url&#125;", path = "api")public interface BadGuyFeignClient &#123; /** * 随机获取一句甜言蜜语 * * @return */ @GetMapping("SweetNothings") String getSweetNothings(); /** * 获取 count 条甜言蜜语 * * @param count 获取甜言蜜语条数 * @return Json 格式的结果 */ @GetMapping("SweetNothings/&#123;count&#125;/Serialization/Json") ReturnResult&lt;List&lt;String&gt;&gt; getSweetNothingsJsonByCount(@PathVariable("count") Integer count);&#125; 声明为 FeignClient 之后，我们便可以在代码中使用 @Resource 或者 @Autowire 进行注入使用了： 1234567891011121314151617@Componentpublic class BadServiceImpl implements BadGuyService &#123; @Autowired private BadGuyFeignClient badGuyFeignClient; @Override public List&lt;String&gt; getQuotations(Integer count) &#123; if (count == null || count &lt;= 0) &#123; String singleQuotation = badGuyFeignClient.getSweetNothings(); return new ArrayList&lt;String&gt;() &#123;&#123; add(singleQuotation); &#125;&#125;; &#125; return badGuyFeignClient.getSweetNothingsJsonByCount(count).getReturnObj(); &#125;&#125; 然后 Controller 中是这么写的： 123456789101112131415161718192021222324@RestController@Log4j2@RequestMapping("/api/badGuy")public class BadGuyController &#123; @Resource private BadGuyService badGuyService; @GetMapping(&#123;"quotations", "quotations/&#123;count&#125;"&#125;) public PlainResult&lt;List&lt;String&gt;&gt; getBadGuyQuotations( @PathVariable(value = "count", required = false) Integer count ) &#123; PlainResult&lt;List&lt;String&gt;&gt; result = new PlainResult&lt;&gt;(); try &#123; List&lt;String&gt; resultStrings = badGuyService.getQuotations(count); result.setData(resultStrings); &#125; catch (Exception e) &#123; log.error("Failed to get bad guy quotations.", e); result.setErrorMessage("error"); &#125; return result; &#125;&#125; 启动项目之后，我们可以访问 http://localhost:8080/api/badGuy/quotations 或者 http://localhost:8080/api/badGuy/quotations/10 后面跟数字，即可得到对应条目数的结果 1234567891011121314151617&#123; "success":true, "code":0, "message":"successful", "data":[ "我从未拥有过你一秒钟，心里却失去过你千万次。", "现在几点了？是我们幸福的起点。", "我不很快乐，因为你不很爱我。但所谓不很快乐者，并不等于不快乐，正如不很爱我不等于不爱我一样。", "我看你挺笨的吹口哨都不会，要不要我嘴对嘴教你。", "我玩了六年英雄联盟，后来才发现你才是我的英雄。", "这里一切都丑的，风、雨、太阳，都丑，人也丑，我也丑得很。只有你是青天一样可羡。", "他对她说，他依然爱她，和过去一样，至死不渝", "你猜我喜欢什么制服” “被你制服”", "人说红颜薄命，你做我的红颜，我愿为你薄命。", "有时候我想比你晚出生一百年，你的一生被拍成一部电影，而我一生只做一件事：独自坐在房间，面对墙上的荧光屏，用我的一生把你的一生看完。" ]&#125; Feign client 如何设置请求头信息？在调用http接口时，一般都需要在请求头里面添加相应鉴权参数，类似于ak、sk之类的密钥或者某个动态参数等，那么在使用Feign client调用时，该如何添加请求头信息呢？ 准备接口首先，准备一个第三方服务接口，我这里直接在rap上定义了一个查询用户信息接口，调用该接口必须在请求头传一个token参数和请求体中传userId，postman请求示例如下图： 如何添加请求有呢，这里提供两种方式。 在请求方法上添加请求头参数示例代码如下： 1234567891011121314@FeignClient(name = "apiClient", url = "$&#123;test.url&#125;", path = "api")public interface BasicApiClient &#123; /** * 查询用户信息接口-第一种方式 * * @param userId * @param token * @return */ @GetMapping(value = "/queryUser") PlainResult&lt;User&gt; queryUser(@RequestParam("userId") String userId, @RequestHeader(name = "token") String token);&#125; 注意queryUser方法中多了一个@RequestHeader参数token，这个就相当于往请求头添加了一个token参数，这种情况适用于该token是一个在业务中经常动态变化的参数，需要在接口调用方动态获取。 利用@FeignClient的configuration属性新建一个配置类如下 12345678910111213141516171819public class ClientConfiguration &#123; @Value("$&#123;test.token&#125;") private String token; @Bean public RequestInterceptor headerInterceptor() &#123; return new RequestInterceptor() &#123; @Override public void apply(RequestTemplate template) &#123; List&lt;String&gt; authorizationList = Lists.newArrayList(token); List&lt;String&gt; contentTypeList = Lists.newArrayList("application/x-www-form-urlencoded;charset=utf-8"); Map&lt;String, Collection&lt;String&gt;&gt; headers = ImmutableMap.of("token", authorizationList, "Content-Type", contentTypeList); template.headers(headers); &#125; &#125;; &#125;&#125; 修改请求类如下： 123456789101112@FeignClient(name = "apiClientTwo", url = "$&#123;test.url&#125;", path = "api", configuration = ClientConfiguration.class)public interface CommonApiClient &#123; /** * 查询用户信息接口-第二种方式 * * @param userId * @return */ @GetMapping(value = "/queryUser") PlainResult&lt;User&gt; queryUser(@RequestParam("userId") String userId);&#125; 注意：这里使用了@FeignClient的configuration属性，并在该配置类中往请求头添加了token入参，这种方式适用于往请求头中存放的参数是固定的，类似于ak、sk或者用于授权的应用ID密钥之类的。 完整项目源码请参考：springboot-middleware-feign FeignClient与 HttpClient的区别是什么？HttpClient与之同样实现的还有 Okhttp、Httpurlconnection、RestTemplate等等，其 URL 参数是以编程方式构造的，数据被发送到其他服务。在更复杂的情况下，我们将不得不RestTemplate深入到更低级别的 API提供的甚至是 API的细节。 FeignClient则更像是在基于 REST 的服务调用上提供更高级别的抽象，在客户端编写声明式REST 服务接口，并使用这些接口来编写客户端程序。开发人员不用担心这个接口的实现。这将在运行时由 Spring 动态配置。通过这种声明性的方法，开发人员不需要深入了解由 HTTP 提供的 HTTP 级别 API的细节的RestTemplate。 总的来讲，FeignClient更具抽象性，也更简单、灵活。 总结本文简单介绍了如何使用 Spring Cloud Feign组件来替代 HttpClient来实现简单调用第三方服务的方法，除了集成 Feign组件，我们也可以在项目中加入 Ribbon用于服务发现，加入 Hystrix用于服务熔断等等，这样就会完整地构建出一个基本服务了。 参考 https://juejin.im/post/5daf10836fb9a04e054da1b5 https://blog.csdn.net/weixin_38809962/article/details/80354878 https://www.jianshu.com/p/8bca50cb11d8]]></content>
  </entry>
  <entry>
    <title><![CDATA[云计算的三种服务模式：IaaS，PaaS和SaaS]]></title>
    <url>%2F2019%2F09%2F26%2F%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9A%84%E4%B8%89%E7%A7%8D%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%BC%8F%EF%BC%9AIaaS%EF%BC%8CPaaS%E5%92%8CSaaS%2F</url>
    <content type="text"><![CDATA[概述云服务”现在已经快成了一个家喻户晓的词了。如果你不知道PaaS, IaaS 和SaaS的区别，那么也没啥，因为很多人确实不知道。 “云”其实是互联网的一个隐喻，“云计算”其实就是使用互联网来接入存储或者运行在远程服务器端的应用，数据，或者服务。 任何一个使用基于互联网的方法来计算，存储和开发的公司，都可以从技术上叫做从事云的公司。然而，不是所有的云公司都一样。不是所有人都是CTO，所以有时候看到云技术背后的一些词可能会比较头疼。 云也是分层的 任何一个在互联网上提供其服务的公司都可以叫做云计算公司。其实云计算分几层的，分别是Infrastructure（基础设施）-as-a-Service，Platform（平台）-as-a-Service，Software（软件）-as-a-Service。基础设施在最下端，平台在中间，软件在顶端。别的一些“软”的层可以在这些层上面添加。 IaaS: Infrastructure-as-a-Service（基础设施即服务） 第一层叫做IaaS，有时候也叫做Hardware-as-a-Service，几年前如果你想在办公室或者公司的网站上运行一些企业应用，你需要去买服务器，或者别的高昂的硬件来控制本地应用，让你的业务运行起来。 但是现在有IaaS，你可以将硬件外包到别的地方去。IaaS公司会提供场外服务器，存储和网络硬件，你可以租用。节省了维护成本和办公场地，公司可以在任何时候利用这些硬件来运行其应用。 一些大的IaaS公司包括Amazon, Microsoft, VMWare, Rackspace和Red Hat.不过这些公司又都有自己的专长，比如Amazon和微软给你提供的不只是IaaS，他们还会将其计算能力出租给你来host你的网站。 PaaS: Platform-as-a-Service（平台即服务） 第二层就是所谓的PaaS，某些时候也叫做中间件。你公司所有的开发都可以在这一层进行，节省了时间和资源。 PaaS公司在网上提供各种开发和分发应用的解决方案，比如虚拟服务器和操作系统。这节省了你在硬件上的费用，也让分散的工作室之间的合作变得更加容易。网页应用管理，应用设计，应用虚拟主机，存储，安全以及应用开发协作工具等。 一些大的PaaS提供者有Google App Engine,Microsoft Azure，Force.com,Heroku，Engine Yard。最近兴起的公司有AppFog, Mendix 和 Standing Cloud SaaS: Software-as-a-Service（软件即服务） 第三层也就是所谓SaaS。这一层是和你的生活每天接触的一层，大多是通过网页浏览器来接入。任何一个远程服务器上的应用都可以通过网络来运行，就是SaaS了。 你消费的服务完全是从网页如Netflix, MOG, Google Apps, Box.net, Dropbox或者苹果的iCloud那里进入这些分类。尽管这些网页服务是用作商务和娱乐或者两者都有，但这也算是云技术的一部分。 一些用作商务的SaaS应用包括Citrix的GoToMeeting，Cisco的WebEx，Salesforce的CRM，ADP，Workday和SuccessFactors。 Iaas和Paas之间的比较 PaaS的主要作用是将一个开发和运行平台作为服务提供给用户，而IaaS的主要作用是提供虚拟机或者其他资源作为服务提供给用户。接下来，将在七个方面对PaaS和IaaS进行比较： 开发环境：PaaS基本都会给开发者提供一整套包括IDE在内的开发和测试环境，而IaaS方面用户主要还是沿用之前比较熟悉那套开发环境，但是因为之前那套开发环境在和云的整合方面比较欠缺，所以使用起来不是很方便。 支持的应用：因为IaaS主要是提供虚拟机，而且普通的虚拟机能支持多种操作系统，所以IaaS支持的应用的范围是非常广泛的。但如果要让一个应用能跑在某个PaaS平台不是一件轻松的事，因为不仅需要确保这个应用是基于这个平台所支持的语言，而且也要确保这个应用只能调用这个平台所支持的API，如果这个应用调用了平台所不支持的API，那么就需要对这个应用进行修改。 开放标准：虽然很多IaaS平台都存在一定的私有功能，但是由于OVF等协议的存在，使得IaaS在跨平台和避免被供应商锁定这两面是稳步前进的。而PaaS平台的情况则不容乐观，因为不论是Google的App Engine，还是Salesforce的Force.com都存在一定的私有API。 可伸缩性：PaaS平台会自动调整资源来帮助运行于其上的应用更好地应对突发流量。而IaaS平台则需要开发人员手动对资源进行调整才能应对。 整合率和经济性： PaaS平台整合率是非常高，比如PaaS的代表Google App Engine能在一台服务器上承载成千上万的应用，而普通的IaaS平台的整合率最多也不会超过100，而且普遍在10左右，使得IaaS的经济性不如PaaS。 计费和监管：因为PaaS平台在计费和监管这两方面不仅达到了IaaS平台所能企及的操作系统层面，比如，CPU和内存的使用量等，而且还能做到应用层面，比如，应用的反应时间（Response Time）或者应用所消耗的事务多少等，这将提高计费和管理的精确性。 学习难度：因为在IaaS上面开发和管理应用和现有的方式比较接近，而PaaS上面开发则有可能需要学一门新的语言或者新的框架，所以IaaS学习难度更低。 参考 https://www.cnblogs.com/beanmoon/archive/2012/12/10/2811547.html]]></content>
      <tags>
        <tag>云计算</tag>
        <tag>PaaS</tag>
        <tag>IaaS</tag>
        <tag>SaaS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA远程调试（Debug）Tomcat]]></title>
    <url>%2F2019%2F09%2F25%2FIntelliJ-IDEA%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95%EF%BC%88Debug%EF%BC%89Tomcat%2F</url>
    <content type="text"><![CDATA[为什么需要这么做？ 解决 在我本地是好的啊 这个世界性难题～ 测试环境碰到问题，直接连上debug，不用再测试本地，再查看测试环境日志 遇到一些诡异的问题，日志是看不出端倪的 调试一些只能在测试环境执行的流程，如：调用微信/支付宝付款 配置配置主要分为两步： 服务器tomcat配置 本地idea配置 服务器tomcat配置环境: CentOS7+apache-tomcat-8.5.38 步骤: 编辑tomcat执行程序catalina.sh,该文件位于tomcat的bin目录下JPDA_ADDRESS配置项，大约在339行，默认端口设置的是8000，可自行修改，只要与后续idea里配置的端口一致就行。 修改之前： 注：里面的地址前面加了localhost，要想我们本地远程debug，需要去掉该部分 修改之后： 这种远程DEBUG并不是官方推荐的修改方式，官方推荐的修改方式为在catalina.sh文件的文件头加上如下配置项即可： 1export JAVA_OPTS='-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000' 本地idea配置环境: macOS+IntelliJ Idea 2019.1 这里提供两种方式 方式一1.进入Edit Configurations页面，选择Remote选项 2.配置远程服务器IP和DEBUG端口，以及选择本地项目 3.本地DEBUG启动,启动成功之后，控制台会输出相应链接成功日志 方式二 这里以应用部署端口8081，debug端口5005为例 1.进入Edit Configurations页面，选择Tomcat Server -&gt;Remote选项 2.配置远程服务器IP和应用端口 3.配置远程服务器DEBUG端口 4.debug启动项目 ⚠️注意事项：远程debug前记得开放相应debug端口！ 大功告成，这样就可以利用idea远程DEBUG了！！！ 参考 IntelliJ IDEA远程调试（Debug）Tomcat]]></content>
      <tags>
        <tag>远程DEBUG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LocalDateTime这么好，你还在犹豫什么？]]></title>
    <url>%2F2019%2F09%2F17%2FLocalDateTime%E8%BF%99%E4%B9%88%E5%A5%BD%EF%BC%8C%E4%BD%A0%E8%BF%98%E5%9C%A8%E7%8A%B9%E8%B1%AB%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[概述 在项目开发过程中经常遇到时间处理，但是你真的用对了吗，理解阿里巴巴开发手册中禁用static修饰SimpleDateFormat吗 通过阅读本篇文章你将了解到： LocalDate、LocalTime、LocalDateTime有哪些优势？ java8新的时间API的使用方式，包括创建、格式化、解析、计算、修改 Date基本用法Date如果不格式化，打印出的日期可读性差1Tue Sep 17 14:48:07 CST 2019 使用SimpleDateFormat对时间进行格式化，但SimpleDateFormat是线程不安全的SimpleDateFormat的format方法最终调用代码：1234567891011121314151617181920212223242526272829303132private StringBuffer format(Date date, StringBuffer toAppendTo, FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo; &#125; calendar是共享变量，并且这个共享变量没有做线程安全控制。当多个线程同时使用相同的SimpleDateFormat对象【如用static修饰的SimpleDateFormat】调用format方法时，多个线程会同时调用calendar.setTime方法，可能一个线程刚设置好time值另外的一个线程马上把设置的time值给修改了导致返回的格式化时间可能是错误的。在多并发情况下使用SimpleDateFormat需格外注意SimpleDateFormat除了format是线程不安全以外，parse方法也是线程不安全的。parse方法实际调用alb.establish(calendar).getTime()方法来解析，alb.establish(calendar)方法里主要完成了 重置日期对象cal的属性值 使用calb中中属性设置cal 返回设置好的cal对象 但是这三步不是原子操作多线程并发如何保证线程安全 避免线程之间共享一个SimpleDateFormat对象，每个线程使用时都创建一次SimpleDateFormat对象 =&gt; 创建和销毁对象的开销大 对使用format和parse方法的地方进行加锁 =&gt; 线程阻塞性能差 使用ThreadLocal保证每个线程最多只创建一次SimpleDateFormat对象 =&gt; 较好的方法 Date对时间处理比较麻烦，比如想获取某年、某月、某星期，以及n天以后的时间，如果用Date来处理的话真是太难了，你可能会说Date类不是有getYear、getMonth这些方法吗，获取年月日很Easy，但都被弃用了啊 LocalDate、LocalTime、LocalDateTimeLocalDate 只会获取年月日 创建LocalDate 1234//获取当前年月日LocalDate localDate = LocalDate.now();//构造指定的年月日LocalDate localDate1 = LocalDate.of(2019, 9, 10); 获取年、月、日、星期几 12345678int year = localDate.getYear();int year1 = localDate.get(ChronoField.YEAR);Month month = localDate.getMonth();int month1 = localDate.get(ChronoField.MONTH_OF_YEAR);int day = localDate.getDayOfMonth();int day1 = localDate.get(ChronoField.DAY_OF_MONTH);DayOfWeek dayOfWeek = localDate.getDayOfWeek();int dayOfWeek1 = localDate.get(ChronoField.DAY_OF_WEEK); LocalTime 只会获取时分秒 创建LocalTime 12LocalTime localTime = LocalTime.of(13, 51, 10);LocalTime localTime1 = LocalTime.now(); 获取时分秒 123456789//获取小时int hour = localTime.getHour();int hour1 = localTime.get(ChronoField.HOUR_OF_DAY);//获取分int minute = localTime.getMinute();int minute1 = localTime.get(ChronoField.MINUTE_OF_HOUR);//获取秒int second = localTime.getMinute();int second1 = localTime.get(ChronoField.SECOND_OF_MINUTE); LocalDateTime 获取年月日时分秒，等于LocalDate+LocalTime 创建LocalDateTime 12345LocalDateTime localDateTime = LocalDateTime.now();LocalDateTime localDateTime1 = LocalDateTime.of(2019, Month.SEPTEMBER, 10, 14, 46, 56);LocalDateTime localDateTime2 = LocalDateTime.of(localDate, localTime);LocalDateTime localDateTime3 = localDate.atTime(localTime);LocalDateTime localDateTime4 = localTime.atDate(localDate); 获取LocalDate 1LocalDate localDate2 = localDateTime.toLocalDate(); 获取LocalTime 1LocalTime localTime2 = localDateTime.toLocalTime(); Instant 获取秒数 创建Instant对象 1Instant instant = Instant.now(); 获取秒数 1long currentSecond = instant.getEpochSecond(); *获取毫秒数1long currentMilli = instant.toEpochMilli(); 个人觉得如果只是为了获取秒数或者毫秒数，使用System.currentTimeMillis()来得更为方便 修改LocalDate、LocalTime、LocalDateTime、Instant LocalDate、LocalTime、LocalDateTime、Instant为不可变对象，修改这些对象对象会返回一个副本. 增加、减少年数、月数、天数等以LocalDateTime为例 12345678LocalDateTime localDateTime = LocalDateTime.of(2019, Month.SEPTEMBER, 10, 14, 46, 56);//增加一年localDateTime = localDateTime.plusYears(1);localDateTime = localDateTime.plus(1, ChronoUnit.YEARS);//减少一个月localDateTime = localDateTime.minusMonths(1);localDateTime = localDateTime.minus(1, ChronoUnit.MONTHS); 通过with修改某些值 1234//修改年为2020localDateTime = localDateTime.withYear(2020);//修改为2022localDateTime = localDateTime.with(ChronoField.YEAR, 2022); 还可以修改月、日 时间计算比如有些时候想知道这个月的最后一天是几号、下个周末是几号，通过提供的时间和日期API可以很快得到答案 12LocalDate localDate = LocalDate.now();LocalDate localDate1 = localDate.with(firstDayOfYear()); 复制代码比如通过firstDayOfYear()返回了当前日期的第一天日期，还有很多方法这里不在举例说明 格式化123456LocalDate localDate = LocalDate.of(2019, 9, 10);String s1 = localDate.format(DateTimeFormatter.BASIC_ISO_DATE);String s2 = localDate.format(DateTimeFormatter.ISO_LOCAL_DATE);//自定义格式化DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");String s3 = localDate.format(dateTimeFormatter); DateTimeFormatter默认提供了多种格式化方式，如果默认提供的不能满足要求，可以通过DateTimeFormatter的ofPattern方法创建自定义格式化方式 解析时间12LocalDate localDate1 = LocalDate.parse("20190910", DateTimeFormatter.BASIC_ISO_DATE);LocalDate localDate2 = LocalDate.parse("2019-09-10", DateTimeFormatter.ISO_LOCAL_DATE); 和SimpleDateFormat相比，DateTimeFormatter是线程安全的。 小结LocalDateTime：Date有的我都有，Date没有的我也有，日期选择请选择我！ 拓展SpringBoot中应用LocalDateTime 将LocalDateTime字段以时间戳的方式返回给前端添加日期转化类12345public class LocalDateTimeConverter extends JsonSerializer&lt;LocalDateTime&gt; &#123; @Override public void serialize(LocalDateTime value, JsonGenerator gen, SerializerProvider serializers) throws IOException &#123; gen.writeNumber(value.toInstant(ZoneOffset.of("+8")).toEpochMilli()); &#125;&#125; 并在LocalDateTime字段上添加@JsonSerialize(using = LocalDateTimeConverter.class)注解，如下：12@JsonSerialize(using = LocalDateTimeConverter.class)protected LocalDateTime gmtModified; 将LocalDateTime字段以指定格式化日期的方式返回给前端在LocalDateTime字段上添加@JsonFormat(shape=JsonFormat.Shape.STRING, pattern=&quot;yyyy-MM-dd HH:mm:ss&quot;)注解即可，如下：12@JsonFormat(shape=JsonFormat.Shape.STRING, pattern="yyyy-MM-dd HH:mm:ss")protected LocalDateTime gmtModified; 对前端传入的日期进行格式化 在LocalDateTime字段上添加@DateTimeFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;)注解即可，如下：12@DateTimeFormat(pattern = "yyyy-MM-dd HH:mm:ss")protected LocalDateTime gmtModified; 参考 https://juejin.im/post/5d7787625188252388753eae]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式入门介绍与使用]]></title>
    <url>%2F2019%2F09%2F16%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[概述什么是正则表达式? 正则表达式是一组由字母和符号组成的特殊文本, 它可以用来从文本中找出满足你想要的格式的句子. 一个正则表达式是在一个主体字符串中从左到右匹配字符串时的一种样式. “Regular expression”这个词比较拗口, 我们常使用缩写的术语”regex”或”regexp”. 正则表达式可以从一个基础字符串中根据一定的匹配模式替换文本中的字符串、验证表单、提取字符串等等. 想象你正在写一个应用, 然后你想设定一个用户命名的规则, 让用户名包含字符,数字,下划线和连字符,以及限制字符的个数,好让名字看起来没那么丑. 我们使用以下正则表达式来验证一个用户名: 以上的正则表达式可以接受 john_doe, jo-hn_doe, john12_as. 但不匹配Jo, 因为它包含了大写的字母而且太短了. 基本匹配正则表达式其实就是在执行搜索时的格式, 它由一些字母和数字组合而成.例如: 一个正则表达式 the, 它表示一个规则: 由字母t开始,接着是h,再接着是e. "the" => The fat cat sat on mat. 正则表达式123匹配字符串123. 它逐个字符的与输入的正则表达式做比较. 正则表达式是大小写敏感的, 所以The不会匹配the. "The" => The fat cat sat on the mat. 元字符正则表达式主要依赖于元字符.元字符不代表他们本身的字面意思, 他们都有特殊的含义. 一些元字符写在方括号中的时候有一些特殊的意思. 以下是一些元字符的介绍: 元字符 描述 . 句号匹配任意单个字符除了换行符. [ ] 字符种类. 匹配方括号内的任意字符. [^ ] 否定的字符种类. 匹配除了方括号里的任意字符 * 匹配&gt;=0个重复的在*号之前的字符. + 匹配&gt;=1个重复的+号前的字符. ? 标记?之前的字符为可选. {n,m} 匹配num个大括号之前的字符 (n &lt;= num &lt;= m). (xyz) 字符集, 匹配与 xyz 完全相等的字符串. &#124; 或运算符,匹配符号前或后的字符. &#92; 转义字符,用于匹配一些保留的字符 [ ] ( ) { } . * + ? ^ $ \ &#124; ^ 从开始行开始匹配. $ 从末端开始匹配. 点运算符*.是元字符中最简单的例子..匹配任意单个字符, 但不匹配换行符.例如, 表达式.ar匹配一个任意字符后面跟着是a和r的字符串. ".ar" => The car parked in the garage. 在线练习 字符集字符集也叫做字符类.方括号用来指定一个字符集.在方括号中使用连字符来指定字符集的范围.在方括号中的字符集不关心顺序.例如, 表达式[Tt]he 匹配 the 和 The. "[Tt]he" => The car parked in the garage. 在线练习 方括号的句号就表示句号.表达式 ar[.] 匹配 ar.字符串 "ar[.]" => A garage is a good place to park a car. 在线练习 否定字符集一般来说 ^ 表示一个字符串的开头, 但它用在一个方括号的开头的时候, 它表示这个字符集是否定的.例如, 表达式[^c]ar 匹配一个后面跟着ar的除了c的任意字符. "[^c]ar" => The car parked in the garage. 在线练习 重复次数后面跟着元字符 +, * or ? 的, 用来指定匹配子模式的次数.这些元字符在不同的情况下有着不同的意思. * 号*号匹配 在*之前的字符出现大于等于0次.例如, 表达式 a* 匹配以0或更多个a开头的字符, 因为有0个这个条件, 其实也就匹配了所有的字符. 表达式[a-z]* 匹配一个行中所有以小写字母开头的字符串. "[a-z]*" => The car parked in the garage #21. 在线练习 *字符和.字符搭配可以匹配所有的字符.*.*和表示匹配空格的符号\s连起来用, 如表达式\s*cat\s*匹配0或更多个空格开头和0或更多个空格结尾的cat字符串. "\s*cat\s*" => The fat cat sat on the concatenation. 在线练习 + 号+号匹配+号之前的字符出现 &gt;=1 次.例如表达式c.+t 匹配以首字母c开头以t结尾,中间跟着任意个字符的字符串. "c.+t" => The fat cat sat on the mat. 在线练习 ? 号在正则表达式中元字符 ? 标记在符号前面的字符为可选, 即出现 0 或 1 次.例如, 表达式 [T]?he 匹配字符串 he 和 The. "[T]he" => The car is parked in the garage. 在线练习 "[T]?he" => The car is parked in the garage. 在线练习 {} 号在正则表达式中 {} 是一个量词, 常用来一个或一组字符可以重复出现的次数.例如, 表达式 [0-9]{2,3} 匹配最少 2 位最多 3 位 0~9 的数字. "[0-9]{2,3}" => The number was 9.9997 but we rounded it off to 10.0. 在线练习 我们可以省略第二个参数.例如, [0-9]{2,} 匹配至少两位 0~9 的数字. "[0-9]{2,}" => The number was 9.9997 but we rounded it off to 10.0. 在线练习 如果逗号也省略掉则表示重复固定的次数.例如, [0-9]{3} 匹配3位数字 "[0-9]{3}" => The number was 9.9997 but we rounded it off to 10.0. 在线练习 (...) 特征标群特征标群是一组写在 (...) 中的子模式. 例如之前说的 {} 是用来表示前面一个字符出现指定次数. 但如果在 {} 前加入特征标群则表示整个标群内的字符重复 N 次. 例如, 表达式 (ab)* 匹配连续出现 0 或更多个 ab. 我们还可以在 () 中用或字符 | 表示或. 例如, (c|g|p)ar 匹配 car 或 gar 或 par. "(c|g|p)ar" => The car is parked in the garage. 在线练习 | 或运算符或运算符就表示或, 用作判断条件. 例如 (T|t)he|car 匹配 (T|t)he 或 car. "(T|t)he|car" => The car is parked in the garage. 在线练习 转码特殊字符反斜线 \ 在表达式中用于转码紧跟其后的字符. 用于指定 { } [ ] / \ + * . $ ^ | ? 这些特殊字符. 如果想要匹配这些特殊字符则要在其前面加上反斜线 \. 例如 . 是用来匹配除换行符外的所有字符的. 如果想要匹配句子中的 . 则要写成 \. 以下这个例子 \.?是选择性匹配. "(f|c|m)at\.?" => The fat cat sat on the mat. 在线练习 锚点在正则表达式中, 想要匹配指定开头或结尾的字符串就要使用到锚点. ^ 指定开头, $ 指定结尾. ^ 号^ 用来检查匹配的字符串是否在所匹配字符串的开头. 例如, 在 abc 中使用表达式 ^a 会得到结果 a. 但如果使用 ^b 将匹配不到任何结果. 因为在字符串 abc 中并不是以 b 开头. 例如, ^(T|t)he 匹配以 The 或 the 开头的字符串. "(T|t)he" => The car is parked in the garage. 在线练习 "^(T|t)he" => The car is parked in the garage. 在线练习 $ 号同理于 ^ 号, $ 号用来匹配字符是否是最后一个. 例如, (at\.)$ 匹配以 at. 结尾的字符串. "(at\.)" => The fat cat. sat. on the mat. 在线练习 "(at\.)$" => The fat cat. sat. on the mat. 在线练习 简写字符集正则表达式提供一些常用的字符集简写. 如下: 简写 描述 . 除换行符外的所有字符 \w 匹配所有字母数字, 等同于 [a-zA-Z0-9_] \W 匹配所有非字母数字, 即符号, 等同于: [^\w] \d 匹配数字: [0-9] \D 匹配非数字: [^\d] \s 匹配所有空格字符, 等同于: [\t\n\f\r\p{Z}] \S 匹配所有非空格字符: [^\s] \f 匹配一个换页符 \n 匹配一个换行符 \r 匹配一个回车符 \t 匹配一个制表符 \v 匹配一个垂直制表符 \p 匹配 CR/LF (等同于 \r\n)，用来匹配 DOS 行终止符 零宽度断言(前后预查)先行断言和后发断言都属于非捕获簇(不捕获文本 ，也不针对组合计进行计数).先行断言用于判断所匹配的格式是否在另一个确定的格式之前, 匹配结果不包含该确定格式(仅作为约束). 例如, 我们想要获得所有跟在 $ 符号后的数字, 我们可以使用正后发断言 (?&lt;=\$)[0-9\.]*.这个表达式匹配 $ 开头, 之后跟着 0,1,2,3,4,5,6,7,8,9,. 这些字符可以出现大于等于 0 次. 零宽度断言如下: 符号 描述 ?= 正先行断言-存在 ?! 负先行断言-排除 ?&lt;= 正后发断言-存在 ?&lt;! 负后发断言-排除 ?=... 正先行断言?=... 正先行断言, 表示第一部分表达式之后必须跟着 ?=...定义的表达式. 返回结果只包含满足匹配条件的第一部分表达式.定义一个正先行断言要使用 (). 在括号内部使用一个问号和等号: (?=...). 正先行断言的内容写在括号中的等号后面.例如, 表达式 (T|t)he(?=\sfat) 匹配 The 和 the, 在括号中我们又定义了正先行断言 (?=\sfat) ,即 The 和 the 后面紧跟着 (空格)fat. "(T|t)he(?=\sfat)" => The fat cat sat on the mat. 在线练习 ?!... 负先行断言负先行断言 ?! 用于筛选所有匹配结果, 筛选条件为 其后不跟随着断言中定义的格式.正先行断言 定义和 负先行断言 一样, 区别就是 = 替换成 ! 也就是 (?!...). 表达式 (T|t)he(?!\sfat) 匹配 The 和 the, 且其后不跟着 (空格)fat. "(T|t)he(?!\sfat)" => The fat cat sat on the mat. 在线练习 ?&lt;= ... 正后发断言正后发断言 记作(?&lt;=...) 用于筛选所有匹配结果, 筛选条件为 其前跟随着断言中定义的格式.例如, 表达式 (?&lt;=(T|t)he\s)(fat|mat) 匹配 fat 和 mat, 且其前跟着 The 或 the. "(? 在线练习 ?&lt;!... 负后发断言负后发断言 记作 (?&lt;!...) 用于筛选所有匹配结果, 筛选条件为 其前不跟随着断言中定义的格式.例如, 表达式 (?&lt;!(T|t)he\s)(cat) 匹配 cat, 且其前不跟着 The 或 the. "(?&lt;!(T|t)he\s)(cat)" => The cat sat on cat. 在线练习 标志标志也叫模式修正符, 因为它可以用来修改表达式的搜索结果.这些标志可以任意的组合使用, 它也是整个正则表达式的一部分. 标志 描述 i 忽略大小写. g 全局搜索. m 多行的: 锚点元字符 ^ $ 工作范围在每行的起始. 忽略大小写 (Case Insensitive)修饰语 i 用于忽略大小写.例如, 表达式 /The/gi 表示在全局搜索 The, 在后面的 i 将其条件修改为忽略大小写, 则变成搜索 the 和 The, g 表示全局搜索. "The" => The fat cat sat on the mat. 在线练习 "/The/gi" => The fat cat sat on the mat. 在线练习 全局搜索 (Global search)修饰符 g 常用于执行一个全局搜索匹配, 即(不仅仅返回第一个匹配的, 而是返回全部).例如, 表达式 /.(at)/g 表示搜索 任意字符(除了换行) + at, 并返回全部结果. "/.(at)/" => The fat cat sat on the mat. 在线练习 "/.(at)/g" => The fat cat sat on the mat. 在线练习 多行修饰符 (Multiline)多行修饰符 m 常用于执行一个多行匹配. 像之前介绍的 (^,$) 用于检查格式是否是在待检测字符串的开头或结尾. 但我们如果想要它在每行的开头和结尾生效, 我们需要用到多行修饰符 m. 例如, 表达式 /at(.)?$/gm 表示小写字符 a 后跟小写字符 t , 末尾可选除换行符外任意字符. 根据 m 修饰符, 现在表达式匹配每行的结尾. "/.at(.)?$/" => The fat cat sat on the mat. 在线练习 "/.at(.)?$/gm" => The fat cat sat on the mat. 在线练习 贪婪匹配与惰性匹配 (Greedy vs lazy matching)正则表达式默认采用贪婪匹配模式，在该模式下意味着会匹配尽可能长的子串。我们可以使用 ? 将贪婪匹配模式转化为惰性匹配模式。 "/(.*at)/" => The fat cat sat on the mat. 在线练习 "/(.*?at)/" => The fat cat sat on the mat. 在线练习 参考 Github learn-regex]]></content>
      <categories>
        <category>正则</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM-Java内存区域]]></title>
    <url>%2F2019%2F09%2F12%2FJVM-Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[概述对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 运行时数据区域Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8之前： JDK 1.8 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存 (非运行时数据区的一部分) 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 Java 虚拟机栈与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 异常。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 扩展：那么方法/函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java 方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 上图所示的 eden 区、s0 区、s1 区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 方法区和永久代的关系《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 常用参数JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 12-XX:PermSize=N //方法区 (永久代) 初始大小-XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： 12-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）-XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢?整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，并且永远不会得到 java.lang.OutOfMemoryError。你可以使用 -XX：MaxMetaspaceSize标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 当然这只是其中一个原因，还有很多底层的原因，这里就不提了。 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel）与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 其它 此部分主要是实际面试或者工作中遇到的基于Java内存区域遇到的问题，看到这一部分讲的很使用 String类与常量池String 对象的两种创建方式： 12345String str1 = "abcd";//先检查字符串常量池中有没有"abcd"，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向"abcd""；String str2 = new String("abcd");//堆中创建一个新的对象String str3 = new String("abcd");//堆中创建一个新的对象System.out.println(str1==str2);//falseSystem.out.println(str2==str3);//false 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用 new 方法，便需要创建新的对象。 再给大家一个图应该更容易理解 String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7之前（不包含1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7以及之后的处理方式是在常量池中记录此字符串的引用，并返回该引用。 123456String s1 = new String("计算机");String s2 = s1.intern();String s3 = "计算机";System.out.println(s2);//计算机System.out.println(s1 == s2);//false，因为一个是堆内存中的 String 对象一个是常量池中的 String 对象，System.out.println(s3 == s2);//true，因为两个都是常量池中的 String 对象 字符串拼接: 12345678String str1 = "str";String str2 = "ing";String str3 = "str" + "ing";//常量池中的对象String str4 = str1 + str2; //在堆上创建的新的对象String str5 = "string";//常量池中的对象System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 String s1 = new String(“abc”);这句话创建了几个字符串对象？将创建 1 或 2 个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。 验证：1234String s1 = new String("abc");// 堆内存的地址值String s2 = "abc";System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。System.out.println(s1.equals(s2));// 输出 true 结果：12falsetrue 8 种基本类型的包装类和常量池 Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte,Short,Integer,Long,Character,Boolean；这 5 种包装类默认创建了数值[-128，127] 的相应类型的缓存数据，但是超出此范围仍然会去创建新的对象。 两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。 123456789Integer i1 = 33;Integer i2 = 33;System.out.println(i1 == i2);// 输出 trueInteger i11 = 333;Integer i22 = 333;System.out.println(i11 == i22);// 输出 falseDouble i3 = 1.2;Double i4 = 1.2;System.out.println(i3 == i4);// 输出 false Integer 缓存源代码： 12345678/***此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。*/ public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 应用场景： Integer i1=40；Java 在编译的时候会直接将代码封装成 Integer i1=Integer.valueOf(40);，从而使用常量池中的对象。 1234Integer i1 = new Integer(40);这种情况下会创建新的对象。Integer i1 = 40;Integer i2 = new Integer(40);System.out.println(i1==i2);//输出 false Integer 比较更丰富的一个例子: 12345678910111213Integer i1 = 40;Integer i2 = 40;Integer i3 = 0;Integer i4 = new Integer(40);Integer i5 = new Integer(40);Integer i6 = new Integer(0);System.out.println("i1=i2 " + (i1 == i2));System.out.println("i1=i2+i3 " + (i1 == i2 + i3));System.out.println("i1=i4 " + (i1 == i4));System.out.println("i4=i5 " + (i4 == i5));System.out.println("i4=i5+i6 " + (i4 == i5 + i6));System.out.println("40=i5+i6 " + (40 == i5 + i6)); 结果： 123456i1=i2 truei1=i2+i3 truei1=i4 falsei4=i5 falsei4=i5+i6 true40=i5+i6 true 解释： 语句 i4 == i5 + i6，因为+这个操作符不适用于 Integer 对象，首先 i5 和 i6 进行自动拆箱操作，进行数值相加，即 i4 == 40。然后 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 == 40 进行数值比较。 参考 Java内存区域]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[何谓悲观锁与乐观锁?]]></title>
    <url>%2F2019%2F09%2F10%2F%E4%BD%95%E8%B0%93%E6%82%B2%E8%A7%82%E9%94%81%E4%B8%8E%E4%B9%90%E8%A7%82%E9%94%81%2F</url>
    <content type="text"><![CDATA[概述乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 两种锁的使用场景从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 CAS算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 关于自旋锁，大家可以看一下这篇文章，非常不错：《 面试必备之深入理解自旋锁》 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 参考 原文：何谓悲观锁与乐观锁]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【待完善】使用Guava RateLimiter限流入门到深入]]></title>
    <url>%2F2019%2F09%2F03%2F%E4%BD%BF%E7%94%A8Guava-RateLimiter%E9%99%90%E6%B5%81%E5%85%A5%E9%97%A8%E5%88%B0%E6%B7%B1%E5%85%A5%2F</url>
    <content type="text"><![CDATA[前言在开发高并发系统时有三把利器用来保护系统：缓存、降级和限流 缓存: 缓存的目的是提升系统访问速度和增大系统处理容量 降级: 降级是当服务出现问题或者影响到核心流程时，需要暂时屏蔽掉，待高峰或者问题解决后再打开 限流: 限流的目的是通过对并发访问/请求进行限速，或者对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待、降级等处理 常见限流算法 漏桶算法 漏桶算法思路很简单，水（请求）先进入到漏桶里，漏桶以一定的速度出水，当水流入速度过大会直接溢出，可以看出漏桶算法能强行限制数据的传输速率。 令牌桶算法 对于很多应用场景来说，除了要求能够限制数据的平均传输速率外，还要求允许某种程度的突发传输。这时候漏桶算法可能就不合适了，令牌桶算法更为适合。如图所示，令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。 RateLimiter使用以及源码解析Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法实现流量限制，使用十分方便，而且十分高效。 RateLimiter使用首先简单介绍下RateLimiter的使用1234567public void testAcquire() &#123; RateLimiter limiter = RateLimiter.create(1); for(int i = 1; i &lt; 10; i = i + 2 ) &#123; double waitTime = limiter.acquire(i); System.out.println("cutTime=" + System.currentTimeMillis() + " acq:" + i + " waitTime:" + waitTime); &#125; &#125; 输出结果：12345cutTime=1535439657427 acq:1 waitTime:0.0cutTime=1535439658431 acq:3 waitTime:0.997045cutTime=1535439661429 acq:5 waitTime:2.993028cutTime=1535439666426 acq:7 waitTime:4.995625cutTime=1535439673426 acq:9 waitTime:6.999223 首先通过RateLimiter.create(1)创建一个限流器，参数代表每秒生成的令牌数，通过limiter.acquire(i)来以阻塞的方式获取令牌，当然也可以通过tryAcquire(int permits, long timeout, TimeUnit unit)来设置等待超时时间的方式获取令牌，如果超timeout为0，则代表非阻塞，获取不到立即返回。 从输出来看，RateLimiter支持预消费，比如在acquire(5)时，等待时间是3秒，是上一个获取令牌时预消费了3个两排，固需要等待3*1秒，然后又预消费了5个令牌，以此类推 RateLimiter通过限制后面请求的等待时间，来支持一定程度的突发请求(预消费)，在使用过程中需要注意这一点，具体实现原理后面再分析。 RateLimiter实现原理Guava有两种限流模式，一种为稳定模式(SmoothBursty:令牌生成速度恒定)，一种为渐进模式(SmoothWarmingUp:令牌生成速度缓慢提升直到维持在一个稳定值) 两种模式实现思路类似，主要区别在等待时间的计算上，本篇重点介绍SmoothBursty 参考 使用Guava RateLimiter限流以及源码解析 使用Guava的RateLimiter做限流 SpringBoot使用RateLimiter通过AOP方式进行限流 Guava RateLimiter源码解析]]></content>
      <categories>
        <category>高并发</category>
      </categories>
      <tags>
        <tag>guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生命周期管理-索引策略配置与操作]]></title>
    <url>%2F2019%2F08%2F30%2F%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%AE%A1%E7%90%86-%E7%B4%A2%E5%BC%95%E7%AD%96%E7%95%A5%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[概述本文是在本人学习研究ElasticSearch的生命周期管理策略时，发现官方未提供中文文档，有的也是零零散散，此文主要是翻译官方文档Policy phases and actions模块。 注：基于6.7版本 索引生命周期中有四个阶段，按执行顺序排列。 名称 描述 hot 该索引正在积极写入 warm 索引通常不会被写入，但仍然会被查询 cold 索引不再更新，很少查询。信息仍然需要搜索，但如果这些查询速度较慢也没关系。 delete 不再需要索引，可以安全删除 ​ 这些阶段中的每一个都称为阶段。策略不需要为索引配置每个阶段。例如，一个策略可以仅定义热阶段和删除阶段，而另一个策略可以定义所有四个阶段。 时间（Timing）​ 索引进入某阶段是基于该阶段设置的min_age参数。当索引的“年龄”大于该阶段设置的的min_age之前，索引才会进入阶段min_age。使用持续时间格式配置参数（请参阅时间单位）。 min_age如果未指定，则每个阶段的默认值为零秒0s。 示例 123456789101112131415161718192021PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "warm": &#123; "min_age": "1d", "actions": &#123; "allocate": &#123; "number_of_replicas": 1 &#125; &#125; &#125;, "delete": &#123; "min_age": "30d", "actions": &#123; "delete": &#123;&#125; &#125; &#125; &#125; &#125;&#125; 以上示例策略，在一天之后将索引移动到暖阶段。在此之前，该索引处于等待状态。进入暖阶段后，它将等待30天后才能进入删除阶段并删除索引。 min_age通常是从创建索引的时间开始经过的时间。如果索引被翻转，那么min_age是从索引翻转开始经过的时间。这里的目的是执行以下阶段和操作，这些阶段和操作相对于数据最后写入滚动索引的时间。 在索引生命周期管理将检查min_age 并转换到下一阶段之前，必须完成上一阶段的操作。 Phase Execution（阶段执行）​ 正在执行的索引策略的当前阶段定义存储在索引的元数据中。阶段及其动作被编译成一系列按顺序执行的离散步骤。由于某些ILM操作更复杂并且涉及针对索引的多个操作，因此这些操作中的每一个都在称为“步骤”的单元中单独完成。该 解释生命周期API公开这些信息给我们，看看哪些步骤我们的索引或者接下来执行或正在执行。 Actions（操作）下列清单展示每个阶段可配置的动作。 在每个阶段中执行已配置操作的顺序由ILM自动确定，并且无法通过更改策略定义进行更改。 热阶段（Hot） 设置优先级（Set Priority） 滚动更新（Rollover） 取消关注（Unfollow） 温阶段（Warm） 设置优先级（Set Priority） 分配（Allocate） 只读（Read-Only） 强制合并（Force Merge） 收缩（Shrink） 取消关注（Unfollow） 冷阶段（Cold） 设置优先级（Set Priority） 分配（Allocate） 冻结（Freeze） 取消关注（Unfollow） 删除阶段（Delete） 删除（Delete） Allocate阶段允许：warm，cold ​ Allocate操作允许您指定允许哪些节点托管索引的分片并设置副本数。在这些场景背后，它正在修改分片过滤和/或副本计数的索引设置。更新副本数时，配置分配规则是可选的。配置分配规则时，设置副本数是可选的。虽然此操作可以视为两个单独的索引设置更新，但两者都可以一次配置。 配置选项 Name Required Default Description number_of_replicas no - The number of replicas to assign to the index include no - assigns an index to nodes having at least one of the attributes exclude no - assigns an index to nodes having none of the attributes require no - assigns an index to nodes having all of the attributes 如果number_of_replicas未配置，那么include， exclude和require中的至少一个是必需的。没有配置的空Allocate Action是无效的。 示例：更改副本在此示例中，索引的副本数已更改为2，而分配规则未更改。1234567891011121314PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "warm": &#123; "actions": &#123; "allocate" : &#123; "number_of_replicas" : 2 &#125; &#125; &#125; &#125; &#125;&#125; Delete阶段允许：delete 删除操作就是这样，它会删除索引。 此操作没有与之关联的任何选项。 示例123456789101112PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "delete": &#123; "actions": &#123; "delete" : &#123; &#125; &#125; &#125; &#125; &#125;&#125; Force-Merge阶段允许：delete 执行此操作时，索引将变成只读。请参考 强制合并操作Force Merge会将索引合并为最多特定数量的segments。 配置选项 名称 是否必须 默认 描述 max_num_segments 是 - 要合并的段数。要完全合并索引，请将其设置为1 示例1234567891011121314PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "warm": &#123; "actions": &#123; "forcemerge" : &#123; "max_num_segments": 1 &#125; &#125; &#125; &#125; &#125;&#125; Freeze阶段允许：cold 该动作将通过调用Freeze Index API的api来freeze索引 冻结索引将关闭索引并在同一API调用中重新打开它。这导致初选不能在短时间内分配导致群集变红，直到再次分配原色。将来可能会删除此限制。 示例123456789101112PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "cold": &#123; "actions": &#123; "freeze" : &#123; &#125; &#125; &#125; &#125; &#125;&#125; Read-Only阶段允许：warm 此操作将索引设置为只读（请参阅：index.blocks.write） 此操作没有与之关联的任何选项。 示例123456789101112PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "warm": &#123; "actions": &#123; "readonly" : &#123; &#125; &#125; &#125; &#125; &#125;&#125; Rollover阶段允许：hot 索引格式必须匹配模式^。\ - \ d + $*，例如（logs-000001）。 托管的索引必须设置index.lifecycle.rollover_alias为别名进行Rollover，且索引还必须是以别名的写入。 例如，如果要管理的索引具有别名my_data。托管的索引“my_index”必须是别名的写入索引。有关更多信息，请阅读 写入索引别名行为。123456789101112PUT my_index&#123; "settings": &#123; "index.lifecycle.name": "my_policy", "index.lifecycle.rollover_alias": "my_data" &#125;, "aliases": &#123; "my_data": &#123; "is_write_index": true &#125; &#125;&#125; 当现有索引满足其中一个滚动更新条件时，“Rollover Action”会将别名转到新索引。 配置选项 Name Required Default Description max_size no - 最大主分片索引存储大小。请参见字节单位 以进行格式化 max_docs no - 滚动前索引要包含的最大文档数。 max_age no - 索引创建过去的最长时间。请参阅 时间单位 以进行格式 中的至少一个max_size，max_docs，max_age或需要这三者的任意组合来进行指定。 示例：索引过大时滚动此示例在至少100千兆字节时滚动索引。 1234567891011121314PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "rollover" : &#123; "max_size": "100GB" &#125; &#125; &#125; &#125; &#125;&#125; 示例：索引包含太多文档时滚动此示例在包含至少100000000个文档时将索引翻转。 1234567891011121314PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "rollover" : &#123; "max_docs": 100000000 &#125; &#125; &#125; &#125; &#125;&#125; 示例：索引太旧时滚动此示例至少在7天前创建索引。 1234567891011121314PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "rollover" : &#123; "max_age": "7d" &#125; &#125; &#125; &#125; &#125;&#125; 示例：索引太旧或太大时的翻转此示例在至少7天前创建索引或至少100千兆字节时将索引卷起。在这种情况下，当满足任何条件时，索引将被翻转。 123456789101112131415PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "rollover" : &#123; "max_age": "7d", "max_size": "100GB" &#125; &#125; &#125; &#125; &#125;&#125; Set-Priority阶段允许: hot, warm, cold. 一旦策略进入hot，warm或cold阶段，此操作就会为索引设置索引优先级。具有较高优先级的索引将在节点重启后具有较低优先级的索引之前恢复。通常，hot阶段的指数应具有最高值，而cold阶段的指数应具有最低值。例如：hot阶段设置为100，warm阶段设置为50，cold阶段设置为0。未设置此值的指标的默认优先级为1。 配置选项 名称 需要 默认 描述 priority 是 - 索引的优先级。必须为0或更大。该值也可以设置为null以删除优先级。 示例1234567891011121314PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "warm": &#123; "actions": &#123; "set_priority" : &#123; "priority": 50 &#125; &#125; &#125; &#125; &#125;&#125; Shrink 运行此操作时，索引将变为只读（请参阅：index.blocks.write） 此操作会将现有索引缩减为具有较少主分片的新索引。它调用Shrink API来缩小索引。由于将索引的所有主分片分配给一个节点是先决条件，因此该操作将首先将主分片分配给有效节点。收缩后，它会将指向原始索引的别名交换到新的收缩指数中。新索引还将有一个新名称：“shrink- ”。因此，如果原始索引称为“logs”，则新索引将命名为“shrink-logs”。 配置选项 名称 需要 默认 描述 number_of_shards 是 - 要缩小的分片数。必须是源索引中分片数量的一个因子。 示例1234567891011121314PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "warm": &#123; "actions": &#123; "shrink" : &#123; "number_of_shards": 1 &#125; &#125; &#125; &#125; &#125;&#125; Unfollow 此操作可以显式使用，如下所示，但此操作也在Rollover操作和Shrink操作之前运行，如这些操作的文档中所述。 取消关注操作没有任何选项，如果遇到非关注者索引，则取消关注操作会保持索引不变，并允许下一个操作对此索引进行操作。 123456789101112PUT _ilm/policy/my_policy&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "unfollow" : &#123;&#125; &#125; &#125; &#125; &#125;&#125; 完整策略（Full Policy）​ 通过所有这些操作，我们可以为我们的索引支持复杂的管理策略。此策略将定义一个将在热阶段开始的索引，每50 GB或7天滚动一次。30天后，它进入温暖阶段并将副本增加到2，强制合并和收缩。60天后进入冷期并分配到“冷”节点，90天后删除索引。 123456789101112131415161718192021222324252627282930313233343536373839404142434445PUT _ilm/policy/full_policy&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "rollover": &#123; "max_age": "7d", "max_size": "50G" &#125; &#125; &#125;, "warm": &#123; "min_age": "30d", "actions": &#123; "forcemerge": &#123; "max_num_segments": 1 &#125;, "shrink": &#123; "number_of_shards": 1 &#125;, "allocate": &#123; "number_of_replicas": 2 &#125; &#125; &#125;, "cold": &#123; "min_age": "60d", "actions": &#123; "allocate": &#123; "require": &#123; "type": "cold" &#125; &#125; &#125; &#125;, "delete": &#123; "min_age": "90d", "actions": &#123; "delete": &#123;&#125; &#125; &#125; &#125; &#125;&#125; 参考 官方手册：Policy phases and actions 官方源码文档：policy-definitions.asciidoc]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JDBC之PreparedStatement 详解]]></title>
    <url>%2F2019%2F08%2F27%2FJDBC%E4%B9%8BPreparedStatement-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介PreparedStatement 是一个特殊的Statement对象，如果我们只是来查询或者更新数据的话，最好用PreparedStatement代替Statement，因为它有以下有点： 简化Statement中的操作 提高执行语句的性能 可读性和可维护性更好 安全性更好。 使用PreparedStatement能够预防SQL注入攻击，所谓SQL注入，指的是通过把SQL命令插入到Web表单提交或者输入域名或者页面请求的查询字符串，最终达到欺骗服务器，达到执行恶意SQL命令的目的。注入只对SQL语句的编译过程有破坏作用，而执行阶段只是把输入串作为数据处理，不再需要对SQL语句进行解析，因此也就避免了类似select * from user where name=’aa’ and password=’bb’ or 1=1的sql注入问题的发生。 Statement 和 PreparedStatement之间的关系和区别. 关系：PreparedStatement继承自Statement,都是接口 区别：PreparedStatement可以使用占位符，是预编译的，批处理比Statement效率高 入门使用创建PreparedStatement创建一个PreparedStatement PreparedStatement对象的创建也同样离不开 DriverManger.getConnect()对象，因为它也是建立在连接到数据库之上的操作。12345678/** 1. init PreparedStatement*/ Class.forName("com.mysql.jdbc.Driver"); String url = "jdbc:mysql://localhost:3306/db_test?useSSL=false"; String username = "root"; String password = "root"; Connection connection = DriverManager.getConnection(url, username, password); String sql = "update user set username=? where id = ?"; PreparedStatement preparedStatement = connection.prepareStatement(sql); 设置入参往PreparedStatement里写入参数 看上面那个sql 字符串，中间有几个?，它在这里有点占位符的意思，然后我们可以通过PreparedStatement的setString(),等方法来给占位符进行赋值，使得sql语句变得灵活。 123/** 2. prepare param*/preparedStatement.setString(1, "feifz");preparedStatement.setInt(2, 2); 参数中的第一个参数分别是1和2，它代表的是第几个问号的位置。如果sql语句中只有一个问号，那就不用声明这个参数。 执行更新123/** 3. execute update*/ int result = preparedStatement.executeUpdate(); System.out.printf("更新记录数："+result+"\n"); 结果： 1更新记录数：1 执行查询如果是执行查询数据库的话，也像Statement对象执行excuteQuery()一样返回一个ResultSet结果集。 123456789/** 4. execute select*/String sql2 = "select * from user"; ResultSet resultSet = preparedStatement.executeQuery(sql2); while (resultSet.next()) &#123; int id = resultSet.getInt("id"); String username = resultSet.getString("username"); String dept = resultSet.getString("dept"); System.out.println("id:"+id +"username-&gt;"+ username + ",dept-&gt; " + dept ); &#125; 执行结果： 12345id:1username-&gt;Fant.J,dept-&gt; 测试部id:2username-&gt;feifz,dept-&gt; 研发部id:3username-&gt;xixi,dept-&gt; 产品部id:4username-&gt;hah,dept-&gt; 集成部id:5username-&gt;zeze,dept-&gt; 研发部 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.github.feifuzeng.middleware.mybatis.jdbc;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;/** * @author feifz * @version 1.0.0 * @Description PreparedStatement简单demo * @createTime 2019年08月29日 20:21:00 */public class PrepareStatementSimpleDemo &#123; public static void main(String[] args) throws Exception &#123; /** 1. init PreparedStatement*/ Class.forName("com.mysql.jdbc.Driver"); String url = "jdbc:mysql://localhost:3306/db_test?useSSL=false"; String user = "root"; String password = "root"; Connection connection = DriverManager.getConnection(url, user, password); String sql = "update user set username=? where id = ?"; PreparedStatement preparedStatement = connection.prepareStatement(sql); /** 2. prepare param*/ preparedStatement.setString(1, "feifz"); preparedStatement.setInt(2, 2); /** 3. execute update*/ int result = preparedStatement.executeUpdate(); System.out.printf("更新记录数："+result+"\n"); /** 4. execute select*/ String sql2 = "select * from user"; ResultSet resultSet = preparedStatement.executeQuery(sql2); while (resultSet.next()) &#123; int id = resultSet.getInt("id"); String username = resultSet.getString("username"); String dept = resultSet.getString("dept"); System.out.println("id:"+id +"username-&gt;"+ username + ",dept-&gt; " + dept ); &#125; &#125;&#125; 参考 https://juejin.im/post/5abb78c8f265da23994e9643]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库：浅谈DML、DDL、DCL的区别]]></title>
    <url>%2F2019%2F08%2F27%2F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9A%E6%B5%85%E8%B0%88DML%E3%80%81DDL%E3%80%81DCL%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[简介SQL是一个标准的数据库语言，是面向集合的描述性非过程化语言。它功能强，效率高，简单易学易维护（迄今为止，我还没见过比它还好学的语言）。然而SQL语言由于以上优点，同时也出现了这样一个问题：它是非过程性语言，即大多数语句都是独立执行的，与上下文无关，而绝大部分应用都是一个完整的过程，显然用SQL完全实现这些功能是很困难的。 所以大多数数据库公司为了解决此问题，作了如下两方面的工作： 扩充SQL，在SQL中引入过程性结构； 把SQL嵌入到高级语言中，以便一起完成一个完整的应用。 SQL语言的分类SQL语言共分为四大类：数据查询语言DQL，数据操纵语言DML，数据定义语言DDL，数据控制语言DCL。 DQL DML（data query language）数据查询语言 数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块：SELECT &lt;字段名表&gt;FROM &lt;表或视图名&gt;WHERE &lt;查询条件&gt; 1SELECT 列名称 FROM 表名称 DML DML（data manipulation language）数据操纵语言 就是我们最经常用到的 UPDATE、INSERT、DELETE。 主要用来对数据库的数据进行一些操作。 123UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值INSERT INTO table_name (列1, 列2,...) VALUES (值1, 值2,....)DELETE FROM 表名称 WHERE 列名称 = 值 DDL DDL（data definition language）数据库定义语言 其实就是我们在创建表的时候用到的一些sql，比如说：CREATE、ALTER、DROP等。DDL主要是用在定义或改变表的结构，数据类型，表之间的链接和约束等初始化工作上 12345678910111213CREATE TABLE 表名称(列名称1 数据类型,列名称2 数据类型,列名称3 数据类型,....)ALTER TABLE table_nameALTER COLUMN column_name datatypeDROP TABLE 表名称DROP DATABASE 数据库名称 DCL DCL（Data Control Language）数据库控制语言 是用来设置或更改数据库用户或角色权限的语句，包括（grant,deny,revoke等）语句。这个比较少用到。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库：drop、truncate、delete的区别]]></title>
    <url>%2F2019%2F08%2F27%2F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9Adrop%E3%80%81truncate%E3%80%81delete%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[近日在删除数据时，发现除了常用的Delete &amp; Drop语句之外，还有Truncate也是与删除数据相关的，针对上述三种有进行简单的比较与整理 用法drop 用法：drop table 表名 drop是DDL，会隐式提交，所以，不能回滚，不会触发触发器。 drop语句删除表结构及所有数据，并将表所占用的空间全部释放。 drop语句将删除表的结构所依赖的约束，触发器，索引，依赖于该表的存储过程/函数将保留,但是变为invalid状态。 truncate 清空表中的数据,用法：truncate table 表名 truncate是DDL，会隐式提交，所以，不能回滚，不会触发触发器。 truncate会删除表中所有记录，并且将重新设置高水线和所有的索引，缺省情况下将空间释放到minextents个extent，除非使用reuse storage，。不会记录日志，所以执行速度很快，但不能通过rollback撤消操作（如果一不小心把一个表truncate掉，也是可以恢复的，只是不能通过rollback来恢复）。 对于外键（foreignkey ）约束引用的表，不能使用 truncate table，而应使用不带 where 子句的 delete 语句。 truncatetable不能用于参与了索引视图的表。 delete delete from 表名 （where 列名 = 值） delete是DML，执行delete操作时，每次从表中删除一行，并且同时将该行的的删除操作记录在redo和undo表空间中以便进行回滚（rollback）和重做操作，但要注意表空间要足够大，需要手动提交（commit）操作才能生效，可以通过rollback撤消操作。 delete可根据条件删除表中满足条件的数据，如果不指定where子句，那么删除表中所有记录。 delete语句不影响表所占用的extent，高水线(high watermark)保持原位置不变。。 区别 TRUNCATE 和DELETE只删除数据， DROP则删除整个表（结构和数据）。 应用范围：TRUNCATE只能对TABLE；DELETE可以是table和view。 delete语句是数据库操作语言(dml)，这个操作会放到 rollback segement 中，事务提交之后才生效；如果有相应的 trigger，执行的时候将被触发。 truncate、drop 是数据库定义语言(ddl)，操作立即生效，原数据不放到 rollback segment 中，不能回滚，操作不触发trigger。 总结 delete语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存，以便进行回滚操作。 执行速度一般来说：drop&gt;truncate&gt;delete 在使用drop和truncate时一定要注意，虽然可以恢复，但为了减少麻烦，还是要慎重。 如果想删除部分数据用delete，注意带上where子句，回滚段要足够大； 如果想删除表，当然用drop； 如果想保留表而将所有数据删除，如果和事务无关，用truncate即可； 如果和事务有关，或者想触发trigger，还是用delete； 如果是整理表内部的碎片，可以用truncate跟上reuse stroage，再重新导入/插入数据。 参考 https://www.cnblogs.com/zhizhao/p/7825469.html https://blog.csdn.net/johnson_mar/article/details/74352515 https://blog.csdn.net/smillest/article/details/52577440]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis中#和$的区别]]></title>
    <url>%2F2019%2F08%2F26%2FMybatis%E4%B8%AD-%E5%92%8C-%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[背景动态 sql 是 mybatis 的主要特性之一，在 mapper 中定义的参数传到 xml 中之后，在查询之前 mybatis 会对其进行动态解析。mybatis 为我们提供了两种支持动态 sql 的语法：#{} 以及 ${} 。 主要区别 #相当于对数据加上双引号 $相当于直接显示数据。 #{} ： 根据参数的类型进行处理，比如传入String类型，则会为参数加上双引号。#{} 传参在进行SQL预编译时，会把参数部分用一个占位符 ? 代替，这样可以防止 SQL注入。 ${} ： 将参数取出不做任何处理，直接放入语句中，就是简单的字符串替换，并且该参数会参加SQL的预编译，需要手动过滤参数防止 SQL注入。 因此mybatis中优先使用 #{}；当需要动态传入 表名或列名时，再考虑使用 ${} ， 比较特殊，他的应用场景是需要动态传入表名或列名时使用，MyBatis排序时使用orderby动态参数时需要注意，用 {} 比较特殊， 他的应用场景是 需要动态传入 表名或列名时使用，MyBatis排序时使用order by 动态参数时需要注意，用比较特殊，他的应用场景是需要动态传入表名或列名时使用，MyBatis排序时使用orderby动态参数时需要注意，用而不是#。 示例 #对传入的参数视为字符串，也就是它会预编译，$不会将传入的值进行预编译. select from user where user_name=${rookie}，比如我传一个rookie，那么传过来就是 select from user where user_name = ‘rookie’； select from user where user_name= ${rookie} ,那么传过来的sql就是：select from user where user_name=rookie; #优势在于它能很大程度防止sql注入，$ 不行。 比如：用户进行一个登录操作，后台sql验证式样的：select from user where user_name=#{name} and password = #{pwd}，如果前台传来的用户名是“rookie”，密码是 “1 or 2”，用#的方式就不会出现sql注入，换成用$ 方式，sql语句就变成了 select from user where user_name=rookie and password = 1 or 2。这样的话就形成了sql注入。]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch-生命周期管理]]></title>
    <url>%2F2019%2F08%2F23%2FElasticSearch-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1月29日，Elastic Stack 迎来 6.6 版本的发布，该版本带来很多新功能，比如： Index Lifecycle Management Frozen Index Geoshape based on Bkd Tree SQL adds support for Date histograms …… 在这些众多功能中，Index Lifecycle Management(索引生命周期管理，后文简称 ILM) 是最受社区欢迎的。今天我们从以下几方面来快速了解下该功能： 为什么索引会有生命？什么是索引生命周期？ ILM 是如何划分索引生命周期的？ ILM 是如何管理索引生命周期的？ 实战 Index Lifecycle 索引生命周期先来看第一个问题： 为什么索引有生命？ 索引(Index)是 Elasticsearch 中数据组织的一个逻辑概念，是具有相同或相似字段的文档组合。它由众多分片(Shard)组成，比如book、people都可以用作索引名称，可以简单类比为关系型数据库的表(table)。 所谓生命，即生与死；索引的生与死便是创建与删除了。 在我们日常使用 Elasticsearch 的时候，索引的创建与删除似乎是很简单的事情，用的时候便创建，不用了删除即可，有什么好管理的呢？ 这就要从 Elasticsearch 的应用场景来看了。 在业务搜索场景，用户会将业务数据存储在 Elasticsearch 中，比如商品数据、订单数据、用户数据等，实现快速的全文检索功能。像这类数据基本都是累加的，不会删除。一般删除的话，要么是业务升级，要么是业务歇菜了。此种场景下，基本只有生，没有死，也就不存在管理一说。 而在日志业务场景中，用户会将各种日志，如系统、防火墙、中间件、数据库、web 服务器、应用日志等全部实时地存入 Elasticsearch 中，进行即时日志查询与分析。这种类型的数据都会有时间维度，也就是时序性的数据。由于日志的数据量过大，用户一般不会存储全量的数据，一般会在 Elasticsearch 中存储热数据，比如最近7天、30天的数据等，而在7天或者30天之前的数据都会被删除。为了便于操作，用户一般会按照日期来建立索引，比如 nginx 的日志索引名可能为 nginx_log-2018.11.12、nginx_log-2018.11.13等，当要查询或删除某一天的日志时，只需要针对对应日期的索引做操作就可以了。那么在该场景下，每天都会上演大量索引的生与死。 一个索引由生到死的过程，即为一个生命周期。举例如下： 生：在 2019年2月5日 创建 nginx_log-2019.02.05的索引，开始处理日志数据的读写请求生：在 2019年2月6日 nginx_log-2019.02.05 索引便不再处理写请求，只处理读请求死：在 2019年3月5日 删除 nginx_log-2018.02.05的索引其他的索引，如 nginx_log-2019.02.06 等也会经过相同的一个生命周期。 ILM 是如何划分索引生命周期的？我们现在已经了解何为生命周期了，而最简单的生命周期只需要生与死两个阶段即可。但在实际使用中生命周期是有多个阶段的，我们来看下 ILM 是如何划分生命周期的。 ILM 一共将索引生命周期分为四个阶段(Phase)： Hot 阶段 Warm 阶段 Cold 阶段 Delete 阶段 如果我们拿一个人的生命周期来做类比的话，大概如下图所示： Hot 阶段 索引数据正在活跃的更新和查询 Hot 阶段可类比为人类婴儿到青年的阶段，在这个阶段，它会不断地进行知识的输入与输出(数据读写)，不断地长高长大(数据量增加)成有用的青年。 由于该阶段需要进行大量的数据读写，因此需要高配置的节点，一般建议将节点内存与磁盘比控制在 32 左右，比如 64GB 内存搭配 2TB 的 SSD 硬盘。 Warm 阶段 索引数据不再被更新，但是仍被查询 Warm 阶段可类比为人类青年到中年的阶段，在这个阶段，它基本不会再进行知识的输入(数据写入)，主要进行知识输出(数据读取)，为社会贡献价值。 由于该阶段主要负责数据的读取，中等配置的节点即可满足需求，可以将节点内存与磁盘比提高到 64~96 之间，比如 64GB 内存搭配 4~6TB 的 HDD 磁盘。 Cold 阶段 索引已经不被更新且很少查询。但是索引数据的信息还需要被搜索，若被搜索则比较慢 Cold 阶段可类别比为人类中年到老年的阶段，在这个阶段，它退休了，在社会有需要的时候才出来输出下知识(数据读取)，大部分情况都是静静地待着。 由于该阶段只负责少量的数据读取工作，低等配置的节点即可满足要求，可以将节点内存与磁盘比进一步提高到 96 以上，比如128，即 64GB 内存搭配 8 TB 的 HDD 磁盘。 Delete 阶段 索引不再被需要可以安全的删除 Delete 阶段可类比为人类寿终正寝的阶段，在发光发热之后，静静地逝去，Rest in Peace~ ILM 对于索引的生命周期进行了非常详细的划分，但它并不强制要求必须有这个4个阶段，用户可以根据自己的需求组合成自己的生命周期。 ILM 是如何管理索引生命周期的？所谓生命周期的管理就是控制 4 个生命阶段转换，何时由 Hot 转为 Warm，何时由 Warm 转为 Cold，何时 Delete 等。 阶段的转换必然是需要时机的，而对于时序数据来说，时间必然是最好的维度，而 ILM 也是以时间为转换的衡量单位。比如下面这张转换的示意图，即默认是 Hot 阶段，在索引创建 3 天后转为 Warm 阶段，7 天后转为 Cold 阶段，30 天后删除。这个设置的相关字段为 min_age，后文会详细讲解。 ILM 将不同的生命周期管理策略称为 Policy，而所谓的 Policy 是由不同阶段(Phase)的不同动作(Action)组成的。 Action是一系列操作索引的动作，比如 Rollover、Shrink、Force Merge等，不同阶段可用的 Action 不同，详情如下： Hot Phase Rollover滚动索引操作，可用在索引大小或者文档数达到某设定值时，创建新的索引用于数据读写，从而控制单个索引的大小。这里要注意的一点是，如果启用了 Rollover，那么所有阶段的时间不再以索引创建时间为准，而是以该索引 Rollover 的时间为准。 Warm Phase Allocate 设定副本数、修改分片分配规则(如将分片迁移到中等配置的节点上)等 Read-Onlly 设定当前索引为只读状态 Force Merge 合并 segment 操作 Shrink 缩小 shard 分片数 Cold Phase Allocate 同上 Delete Phase Delete 删除从上面看下来整体操作还是很简单的，Kibana 也提供了一套 UI 界面来设置这些策略，如下所示：从上图看下来 ILM 的设置是不是一目了然呢？ 当然，ILM 是有自己的 api 的，比如上面图片对应的 api 请求如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748PUT /_ilm/policy/test_ilm2&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "rollover": &#123; "max_age": "30d", "max_size": "50gb" &#125; &#125; &#125;, "warm": &#123; "min_age": "3d", "actions": &#123; "allocate": &#123; "require": &#123; "box_type": "warm" &#125;, "number_of_replicas": 0 &#125;, "forcemerge": &#123; "max_num_segments": 1 &#125;, "shrink": &#123; "number_of_shards": 1 &#125; &#125; &#125;, "cold": &#123; "min_age": "7d", "actions": &#123; "allocate": &#123; "require": &#123; "box_type": "cold" &#125; &#125; &#125; &#125;, "delete": &#123; "min_age": "30d", "actions": &#123; "delete": &#123;&#125; &#125; &#125; &#125; &#125;&#125; 这里不展开讲了，感兴趣的同学可以自行查看官方手册。 现在管理策略(Policy)已经有了，那么如何应用到索引(Index)上面呢？ 方法为设定如下的索引配置： index.lifecycle.name 设定 Policy 名称，比如上面的 test_ilm2index.lifecycle.rollover_alias 如果使用了 Rollover，那么还需要指定该别名修改索引配置可以直接修改(PUT index_name/_settings)或者通过索引模板(Index Template)来实现。 我们这里不展开讲了，大家参考下面的实战就明白了。 实战目标现在需要收集 nginx 日志，只需保留最近30天的日志，但要保证最近7天的日志有良好的查询性能，搜索响应时间在 100ms 以内。 为了让大家可以快速看到效果，下面实际操作的时候会将 30天、7天 替换为 40秒、20秒。 ES 集群架构这里我们简单介绍下这次实战所用 ES 集群的构成。该 ES 集群一共有 3个节点组成，每个节点都有名为 box_type 的属性，如下所示：1234GET _cat/nodeattrs?s=attres01_hot 172.24.0.5 172.24.0.5 box_type hotes02_warm 172.24.0.4 172.24.0.4 box_type warmes03_cold 172.24.0.3 172.24.0.3 box_type cold 由上可见我们有 1 个 hot 节点、1 个 warm 节点、1 个 cold 节点，分别用于对应 ILM 的阶段，即 Hot 阶段的索引都位于 hot 上，Warm 阶段的索引都位于 warm 上，Cold 阶段的索引都位于 cold 上。 创建 ILM Policy根据要求，我们的 Policy 设定如下： 索引名以 nginx_logs 为前缀，且以每10个文档做一次 Rollover Rollover 后 5 秒转为 Warm 阶段 Rollover 后 20 秒转为 Cold 阶段 Rollover 后 40 秒删除API 请求如下： 12345678910111213141516171819202122232425262728293031323334353637383940PUT /_ilm/policy/nginx_ilm_policy&#123; "policy": &#123; "phases": &#123; "hot": &#123; "actions": &#123; "rollover": &#123; "max_docs": "10" &#125; &#125; &#125;, "warm": &#123; "min_age": "5s", "actions": &#123; "allocate": &#123; "include": &#123; "box_type": "warm" &#125; &#125; &#125; &#125;, "cold": &#123; "min_age": "20s", "actions": &#123; "allocate": &#123; "include": &#123; "box_type": "cold" &#125; &#125; &#125; &#125;, "delete": &#123; "min_age": "40s", "actions": &#123; "delete": &#123;&#125; &#125; &#125; &#125; &#125;&#125; 创建 Index Template我们基于索引模板来创建所需的索引，如下所示： 1234567891011PUT /_template/nginx_ilm_template&#123; "index_patterns": ["nginx_logs-*"], "settings": &#123; "number_of_shards": 1, "number_of_replicas": 0, "index.lifecycle.name": "nginx_ilm_policy", "index.lifecycle.rollover_alias": "nginx_logs", "index.routing.allocation.include.box_type": "hot" &#125;&#125; 上述配置解释如下： index.lifecycle.name 指明该索引应用的 ILM Policy index.lifecycle.rollover_alias 指明在 Rollover 的时候使用的 alias index.routing.allocation.include.box_type 指明新建的索引都分配在 hot 节点上 创建初始索引 IndexILM 的第一个索引需要我们手动来创建，另外启动 Rollover 必须以数值类型结尾，比如 nginx_logs-000001。索引创建的 api 如下： 12345678PUT nginx_logs-000001&#123; "aliases": &#123; "nginx_logs": &#123; "is_write_index":true &#125; &#125;&#125; 此时索引分布如下所示： 修改 ILM Polling IntervalILM Service 会在后台轮询执行 Policy，默认间隔时间为 10 分钟，为了更快地看到效果，我们将其修改为 1 秒。 123456PUT _cluster/settings&#123; "persistent": &#123; "indices.lifecycle.poll_interval":"1s" &#125;&#125; 开始吧一切准备就绪，我们开始吧！ 首先执行下面的新建文档操作 10 次。1234POST nginx_logs/_doc&#123; "name":"abbc"&#125; 之后 Rollover 执行，新的索引创建，如下所示: 5 秒后，nginx_logs-000001 转到 Warm 阶段 15 秒后(20 秒是指距离 Rollover 的时间，因为上面已经过去5秒了，所以这里只需要15秒)，nginx_logs-00001转到 Cold 阶段 25 秒后，nginx_logs-00001删除 至此，一个完整的 ILM Policy 执行的流程就结束了，而后续 nginx_logs-000002 也会按照这个设定进行流转。 总结ILM 是 Elastic 团队将多年 Elasticsearch 在日志场景领域的最佳实践进行的一次总结归纳和落地实施，极大地降低了用好 Elasticsearch 的门槛。掌握了 ILM 的核心概念，也就意味着掌握了 Elasticsearch 的最佳实践。希望本文能对大家入门 ILM 有所帮助。 参考 Elastic 中文社区：【最新】Elasticsearch 6.6 Index Lifecycle Management 尝鲜 官网文档：Managing the index lifecycle 官网文档：Policy phases and actions]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux nohup命令详解]]></title>
    <url>%2F2019%2F07%2F22%2FLinux-nohup%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[场景 今天在linux上部署wdt程序，在SSH客户端执行./start-dishi.sh,启动成功,在关闭SSH客户端后，运行的程序也同时终止了，怎样才能保证在推出SSH客户端后程序能一直执行呢？通过网上查找资料，发现需要使用nohup命令。 完美解决方案：nohup ./start-dishi.sh &gt;output 2&gt;&amp;1 &amp; 命令用途：不挂断地运行命令。语法：nohup Command [ Arg … ] [ &amp; ]描述：nohup 命令运行由 Command 参数和任何相关的 Arg 参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加 &amp; （ 表示“and”的符号）到命令的尾部。 nohup 是 no hang up 的缩写，就是不挂断的意思。 nohup命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。 在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中。 操作系统中有三个常用的流： 0：标准输入流 stdin 1：标准输出流 stdout 2：标准错误流 stderr 一般当我们用 &gt; console.txt，实际是 1&gt;console.txt的省略用法；&lt; console.txt ，实际是 0 &lt; console.txt的省略用法。 示例 nohup ./command &gt; myout.file 2&gt;&amp;1 &amp; 在上面的例子中，0 – stdin (standard input)，1 – stdout (standard output)，2 – stderr (standard error) ； 2&gt;&amp;1是将标准错误（2）重定向到标准输出（&amp;1），标准输出（&amp;1）再被重定向输入到myout.file文件中。 nohup ./command.sh &gt;/dev/null 2&gt;&amp;1 &amp; /dev/null文件的作用，这是一个无底洞，任何东西都可以定向到这里，但是却无法打开。 所以一般很大的stdou和stderr当你不关心的时候可以利用stdout和stderr定向到这里 nohup和&amp;的区别&amp; ： 指在后台运行,但当用户推出(挂起)的时候，命令自动也跟着退出 nohup ： 不挂断的运行，注意并没有后台运行的功能，，就是指，用nohup运行命令可以使命令永久的执行下去，和用户终端没有关系，例如我们断开SSH连接都不会影响他的运行，注意了nohup没有后台运行的意思；&amp;才是后台运行 那么，我们可以巧妙的吧他们结合起来用就是nohup COMMAND &amp;这样就能使命令永久的在后台执行 例如： sh test.sh &amp; 将sh test.sh任务放到后台 ，关闭xshell，对应的任务也跟着停止。 nohup sh test.sh将sh test.sh任务放到后台，关闭标准输入，终端不再能够接收任何输入（标准输入），重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。 nohup sh test.sh &amp; 将sh test.sh任务放到后台，但是依然可以使用标准输入，终端能够接收任何输入，重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。 参考： https://www.cnblogs.com/zq-inlook/p/3577003.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring中的scope配置和@scope注解]]></title>
    <url>%2F2019%2F06%2F27%2FSpring%E4%B8%AD%E7%9A%84scope%E9%85%8D%E7%BD%AE%E5%92%8C-scope%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介Scope，也称作用域，在Spring IOC容器是指其创建的Bean对象相对于其他Bean 对象的请求可见范围。在Spring IOC容器中具有以下几种作用域：基本作用域（singleton、prototype），Web作用域（reqeust、session、globalsession），自定义作用域。 配置 1、Spring 的作用域在装配 Bean 时就必须在配置文件中指明，配置方式如下（以 xml 配置文件为例）：12&lt;!-- 具体的作用域需要在 scope 属性中定义 --&gt;&lt;bean id="XXX" class="com.XXX.XXXXX" scope="XXXX" /&gt; 2、基于注解开发时，@scope完成bean的作用域配置默认是单例模式（singleton）如果需要设置的话可以修改对应值与以上提到的一致例如：1@scope(“prototype”) 分类 singleton：单例模式，在整个Spring IoC容器中，使用singleton定义的Bean将只有一个实例 prototype：原型模式，每次通过容器的getBean方法获取prototype定义的Bean时，都将产生一个新的Bean实例 request：对于每次HTTP请求，使用request定义的Bean都将产生一个新实例，即每次HTTP请求将会产生不同的Bean实例。只有在Web应用中使用Spring时，该作用域才有效 session：对于每次HTTP Session，使用session定义的Bean都将产生一个新实例。同样只有在Web应用中使用Spring时，该作用域才有效 globalsession：每个全局的HTTP Session，使用session定义的Bean都将产生一个新实例。典型情况下，仅在使用portlet context的时候有效。同样只有在Web应用中使用Spring时，该作用域才有效 其中比较常用的是singleton和prototype两种作用域。对于singleton作用域的Bean，每次请求该Bean都将获得相同的实例。容器负责跟踪Bean实例的状态，负责维护Bean实例的生命周期行为；如果一个Bean被设置成prototype作用域，程序每次请求该id的Bean，Spring都会新建一个Bean实例，然后返回给程序。在这种情况下，Spring容器仅仅使用new 关键字创建Bean实例，一旦创建成功，容器不在跟踪实例，也不会维护Bean实例的状态。如果不指定Bean的作用域，Spring默认使用singleton作用域。Java在创建Java实例时，需要进行内存申请；销毁实例时，需要完成垃圾回收，这些工作都会导致系统开销的增加。因此，prototype作用域Bean的创建、销毁代价比较大。而singleton作用域的Bean实例一旦创建成功，可以重复使用。因此，除非必要，否则尽量避免将Bean被设置成prototype作用域。 示例Spring_@Scope(“prototype”) 这里新建两个bean实例，Order和MainController，其中MainController引用了Order，主要演示了这两个类分别为单例和非单例下结果。 model和Controller都是单例 model 12345678910111213141516171819@Componentpublic class Order &#123; private String orderNum = "test"; public String getOrderNum() &#123; return orderNum; &#125; public void setOrderNum(String orderNum) &#123; this.orderNum = orderNum; &#125; @Override public String toString() &#123; return "Order&#123;" + "orderNum='" + orderNum + '\'' + '&#125;'; &#125;&#125; MainController 1234567891011121314151617181920212223@Controller@RequestMapping(value = "/api")public class MainController &#123; @Autowired private Order order; private String name; @RequestMapping(value = "/users/&#123;username&#125;",method = RequestMethod.GET) @ResponseBody public String userProfile(@PathVariable("username") String username) &#123; name = username; order.setOrderNum( name); try &#123; for(int i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getId() + "name:" + name + "--order:" + order.getOrderNum()); Thread.sleep(2000); &#125; &#125; catch (Exception e) &#123; &#125; return order.toString(); &#125;&#125; 启动之后，我们分别发送两个请求： http://localhost:8080/api/users/aaa http://localhost:8080/api/users/bbb 结果如下：1234567891031name:aaa--order:aaa31name:aaa--order:aaa32name:bbb--order:bbb31name:bbb--order:bbb32name:bbb--order:bbb31name:bbb--order:bbb32name:bbb--order:bbb31name:bbb--order:bbb32name:bbb--order:bbb31name:bbb--order:bbb 可以看到当第二个请求发送后，31线程里的name和order都发生了改变，因为当前的Order和Controller都是单例 model是单例,Controller不是单例 model 12345678910111213141516171819@Componentpublic class Order &#123; private String orderNum = "test"; public String getOrderNum() &#123; return orderNum; &#125; public void setOrderNum(String orderNum) &#123; this.orderNum = orderNum; &#125; @Override public String toString() &#123; return "Order&#123;" + "orderNum='" + orderNum + '\'' + '&#125;'; &#125;&#125; Controller,注意@Scope(“prototype”)加的位置 123456789101112131415161718192021222324@Controller@RequestMapping(value = "/api")@Scope("prototype")public class MainController &#123; @Autowired private Order order; private String name; @RequestMapping(value = "/users/&#123;username&#125;",method = RequestMethod.GET) @ResponseBody public String userProfile(@PathVariable("username") String username) &#123; name = username; order.setOrderNum( name); try &#123; for(int i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getId() + "name:" + name + "--order:" + order.getOrderNum()); Thread.sleep(2000); &#125; &#125; catch (Exception e) &#123; &#125; return order.toString(); &#125;&#125; 启动之后，我们分别发送两个请求： http://localhost:8080/api/users/aaa http://localhost:8080/api/users/bbb 结果如下：12345678923name:aaa--order:aaa23name:aaa--order:aaa24name:bbb--order:bbb23name:aaa--order:bbb24name:bbb--order:bbb23name:aaa--order:bbb24name:bbb--order:bbb23name:aaa--order:bbb24name:bbb--order:bbb 可以看到第二个请求发送后，23线程中的name还是aaa没有发生变化，而order受到了影响，发生了变化，因为Controller是一个新的，所以name也是一个新的变量，但是order是单例的。 model不是单例,Controller是单例 model 1234567891011121314151617181920@Component@Scope("prototype")public class Order &#123; private String orderNum = "test"; public String getOrderNum() &#123; return orderNum; &#125; public void setOrderNum(String orderNum) &#123; this.orderNum = orderNum; &#125; @Override public String toString() &#123; return "Order&#123;" + "orderNum='" + orderNum + '\'' + '&#125;'; &#125;&#125; Controller 1234567891011121314151617181920212223@Controller@RequestMapping(value = "/api")public class MainController &#123; @Autowired private Order order; private String name; @RequestMapping(value = "/users/&#123;username&#125;",method = RequestMethod.GET) @ResponseBody public String userProfile(@PathVariable("username") String username) &#123; name = username; order.setOrderNum( name); try &#123; for(int i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getId() + "name:" + name + "--order:" + order.getOrderNum()); Thread.sleep(2000); &#125; &#125; catch (Exception e) &#123; &#125; return order.toString(); &#125;&#125; 启动之后，我们分别发送两个请求： http://localhost:8080/api/users/aaa http://localhost:8080/api/users/bbb 结果如下：123456723name:aaa--order:aaa23name:aaa--order:aaa24name:bbb--order:bbb23name:bbb--order:bbb24name:bbb--order:bbb23name:bbb--order:bbb24name:bbb--order:bbb 可以看到name和order都变了，可见只设置Model是没有用的 model和Controller都不是单例 model 1234567891011121314151617181920@Component@Scope("prototype")public class Order &#123; private String orderNum = "test"; public String getOrderNum() &#123; return orderNum; &#125; public void setOrderNum(String orderNum) &#123; this.orderNum = orderNum; &#125; @Override public String toString() &#123; return "Order&#123;" + "orderNum='" + orderNum + '\'' + '&#125;'; &#125;&#125; Controller 123456789101112131415161718192021222324@Controller@RequestMapping(value = "/api")@Scope("prototype")public class MainController &#123; @Autowired private Order order; private String name; @RequestMapping(value = "/users/&#123;username&#125;",method = RequestMethod.GET) @ResponseBody public String userProfile(@PathVariable("username") String username) &#123; name = username; order.setOrderNum( name); try &#123; for(int i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getId() + "name:" + name + "--order:" + order.getOrderNum()); Thread.sleep(2000); &#125; &#125; catch (Exception e) &#123; &#125; return order.toString(); &#125;&#125; 启动之后，我们分别发送两个请求： http://localhost:8080/api/users/aaa http://localhost:8080/api/users/bbb 结果如下：1234567830name:aaa--order:aaa30name:aaa--order:aaa30name:aaa--order:aaa31name:bbb--order:bbb30name:aaa--order:aaa31name:bbb--order:bbb30name:aaa--order:aaa31name:bbb--order:bbb 这样30请求的和31请求的就完全分开了 参考 https://blog.csdn.net/Tracycater/article/details/54019223 https://www.jianshu.com/p/ce9415465ee4]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之-代理模式]]></title>
    <url>%2F2019%2F06%2F26%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基本概念代理模式也称为委托模式，是一种结构性设计模式。说到代理，可能大部分人都会有一种陌生又熟悉的感觉，日常生活中好像都能遇到，比如代理上网，招商代理，商务代理等；但又说不出个具体的一二三来；代理这个事情如果我们换个角度，从委托者的角色出发，我们找代理上网，是因为我们在访问某些网站时存在困难，需要有个角色来间接的帮我们实现这个功能；我们找商务代理，可能是因为许多事我们不在行或者其他原因，需要找专业的中间人来帮我们做事。因此，日常生活中我们更多扮演的是委托人的角色，代理以一种中间人的角色，帮我们是处理我们无能为力的事情。如果从写代码的角度出发，当我们遇到以下场景： 无法直接访问某个对象 不想直接访问某个对象 访问某个对象存在困难 的时候，我们就可以通过一个代理，通过它来间接访问真正的对象。 应用代理模式非常简洁，总共就三个角色，包括抽象主题，委托者和代理者123456789101112package com.github.feifuzeng.designpattern.proxy;/** * @author feifz * @version 1.0.0 * @Description 抽象主题 * @createTime 2019年06月25日 15:11:00 */public interface Subject &#123; void doSomething();&#125; 1234567891011121314package com.github.feifuzeng.designpattern.proxy;/** * @author feifz * @version 1.0.0 * @Description 真正实现的主题类 * @createTime 2019年06月25日 15:11:00 */public class RealSubject implements Subject&#123; @Override public void doSomething() &#123; System.out.println("This is real doSomeThing"); &#125;&#125; 123456789101112131415161718192021package com.github.feifuzeng.designpattern.proxy;/** * @author feifz * @version 1.0.0 * @Description 代理主题 * @createTime 2019年06月25日 15:12:00 */public class ProxySubject implements Subject &#123; private Subject mSubject; /** 代理类持有委托类的引用*/ public ProxySubject(Subject realSubject) &#123; mSubject = realSubject; &#125; @Override public void doSomething() &#123; mSubject.doSomething(); &#125;&#125; 12345678910111213141516171819package com.github.feifuzeng.designpattern.proxy;/** * @author feifz * @version 1.0.0 * @Description 客户端 * @createTime 2019年06月25日 15:13:00 */public class Client &#123; public static void main(String[] args) &#123; /** 创建委托类*/ Subject mRealSubject=new RealSubject(); /** 创建代理类*/ ProxySubject mProxy = new ProxySubject(mRealSubject); /** 由代理类去做具体的操作*/ mProxy.doSomething(); &#125;&#125; 可以看到RealSubject和ProxySubject都实现了接口Subject。在客户端使用ProxySubject的实例调用doSomething方法，而不是使用RealSubject的实例来实现。你可能会好奇，这么做的意义是什么呢？直接用RealSubject的实例来调用doSomething方法不也可以吗？何必多此一举。试想，如果现在有很多个委托类，他们各自的实现都不同，客户端只关心doSomething 的调用，而不关心具体的实现，这样代理类就可以在其内部屏蔽委托类之间的差异了，这也是客户端不想关注的事情。 参考 https://juejin.im/post/5a4e4725f265da3e2c37e36e https://www.runoob.com/design-pattern/proxy-pattern.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>代理模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见Linux命令]]></title>
    <url>%2F2019%2F06%2F25%2F%E5%B8%B8%E8%A7%81Linux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[简介常见Linux命令汇总 删除Linux快速删除已输入的命令 ctrl + w —往回删除一个单词，光标放在最末尾 ctrl + k —往前删除到末尾，光标放在最前面（可以使用ctrl+a） ctrl + u 删除光标以前的字符 ctrl + k 删除光标以后的字符 ctrl + a 移动光标至的字符头 ctrl + e 移动光标至的字符尾 ctrl + l 清屏Vim 删除 dd:删除游标所在的一整行(常用) ndd:n为数字。删除光标所在的向下n行，例如20dd则是删除光标所在的向下20行 d1G:删除光标所在到第一行的所有数据 dG:删除光标所在到最后一行的所有数据 d$:删除光标所在处，到该行的最后一个字符 d0:那个是数字0,删除光标所在到该行的最前面的一个字符 x,X:x向后删除一个字符(相当于[del]按键),X向前删除一个字符(相当于[backspace]即退格键) nx:n为数字，连续向后删除n个字符 查看Linux 查看磁盘空间df 以磁盘分区为单位查看文件系统，可以获取硬盘被占用了多少空间，目前还剩下多少空间等信息。例如，我们使用df -h命令来查看磁盘信息， -h 选项为根据大小适当显示：显示内容参数说明： Filesystem：文件系统Size： 分区大小Used： 已使用容量Avail： 还可以使用的容量Use%： 已用百分比Mounted on： 挂载点 操作 ll -h Linux显示当前文件夹下所有文件大小 du -sh 或du -h -d 0 查看当前目录所有文件和文件夹的大小 参考：https://www.cnblogs.com/jiu0821/p/8527950.html wget -b url 后台下载 后台任务启动后，会返回两段话，第一段返回一个pid，代表这个后台任务的进程，并且我们可以kill掉这个id来终止此次下载，第二段返回了一句话，意思是会将输出（持续）写入到wget-log这个文件。 远程操作 linux scp远程拷贝文件及文件夹(参考) 拷贝文件夹 示例：拷贝本机/home/admin/test整个目录至远程主机192.168.1.100的/home/test目录下 1scp -r /home/admin/test/ root@192.168.1.100:/home/test 注：其中root为目标主机账号，执行命令后会提示输入root对应的密码。 拷贝单个文件至远程主机 示例：拷贝本机/home/test.txt文件至远程主机192.168.1.100的/home/test目录下 1scp /home/admin/test.txt root@192.168.1.100:/home/test 注：其实上传文件和文件夹区别就在参数 -r， 跟cp, rm的参数使用差不多， 文加价多个 -r 远程文件/文件夹下载 示例：把192.168.62.10上面的/home/test/文件夹，下载到本地的/home/admin/download/下，使用远程端的root登陆 1scp -r root@192.168.62.10:/home/test/ /home/admin/download/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见spring或第三方内置静态类与工具]]></title>
    <url>%2F2019%2F06%2F24%2F%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3-%E5%87%BA%E5%8F%82%E5%A4%84%E7%90%86-BusiResponseBody%2F</url>
    <content type="text"><![CDATA[简介在日常开发中，可以会用到各种各样的静态变量或工具类，但其实spring源生就提供了很多优秀的静态变量类和工具类，这里简单记录一下。 spring org.springframework.http.MediaType http返回和请求头等信息 org.springframework.http.HttpStatus http各种状态码与对应关系 org.dozer org.dozer.Mapper 对象之间拷贝属性，类似于org.apache.commons.beanutils.copyProperties(Object dest, Object orig) 源码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package org.dozer;/** * Public root interface for performing Dozer mappings from application code. * * @author tierney.matt * @author garsombke.franz */public interface Mapper &#123; /** * Constructs new instance of destinationClass and performs mapping between from source * * @param source * @param destinationClass * @param &lt;T&gt; * @return * @throws MappingException */ &lt;T&gt; T map(Object source, Class&lt;T&gt; destinationClass) throws MappingException; /** * Performs mapping between source and destination objects * * @param source * @param destination * @throws MappingException */ void map(Object source, Object destination) throws MappingException; /** * Constructs new instance of destinationClass and performs mapping between from source * * @param source * @param destinationClass * @param mapId * @param &lt;T&gt; * @return * @throws MappingException */ &lt;T&gt; T map(Object source, Class&lt;T&gt; destinationClass, String mapId) throws MappingException; /** * Performs mapping between source and destination objects * * @param source * @param destination * @param mapId * @throws MappingException */ void map(Object source, Object destination, String mapId) throws MappingException;&#125;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>常见</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列之-RocketMQ入门]]></title>
    <url>%2F2019%2F06%2F24%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8B-RocketMQ%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[简介 RocketMQ是阿里开源的消息中间件，目前已经捐献个Apache基金会，它是由Java语言开发的，具备高吞吐量、高可用性、适合大规模分布式系统应用等特点，经历过双11的洗礼，实力不容小觑。 官网：https://rocketmq.apache.org/ 快速入门：https://rocketmq.apache.org/docs/quick-start/ 阿里云帮助文档：https://help.aliyun.com/document_detail/29532.html RocketMQ中文文档本文分三篇,分别从概念原理，集群搭建，Java接入实战，顺序消息和事务消息讲解本篇内容参照官方文档(Alibaba,apache),原文讲的很简白,本篇引用,不做赘述. 基本概念及优势rocketmq是一个基于发布订阅队列模型的消息中间件,具有高可用,高性能,高实时,天然分布式等特点,(支撑过数次Alibaba双十一),能保证消息严格有序,亿级消息堆积等,设计模型和传统的消息中间件一样,如下(不包含协议) 物理部署结构由上图结构,rocketmq由四大部分组成(集群模式):NameServer Cluster(名称服务器),Broker Cluster(代理),Producer Cluster(生产者),Consumer Cluster(消费者);它们中的每一个都可以水平扩展，而不会出现单点故障。如上截图所示 NameServer名称服务器提供轻量级服务发现和路由。每个名称服务器记录完整的路由信息，提供相应的读写服务，支持快速存储扩展 NameServer是一个几乎无状态的功能齐全的服务器，主要包括两个功能：1.broker 管理，nameserver 接受来自broker集群的注册信息并提供心跳来检测他们是否可用。2.路由管理 每一个nameserver都持有关于broker集群和队列的全部路由信息，用来向客户端提供查询。我们知道 ，rocketMQ客户端（生产者/消费者）会从nameserver查询队列的路由信息.有四种方式能够让客户端湖区到nameserver的地址：(1).通过程序，像这样producer.setNamesrvAddr(“ip:port”)(最实用)(2).java 配置项，这么用rocketmq.namesrv.addr(3).环境变量 NAMESRV_ADDR(4).HTTP 端点其中优先级:程序&gt;java配置&gt;环境变量&gt;Http端点BrokerBroker通过提供轻量级主题和队列机制来处理消息存储。它们支持Push和Pull模型，包含容错机制(2个副本或3个副本)，提供了极强的峰值处理里能力和按照时间顺序存储数以百万记的消息存储能力，此外，代理提供了灾难恢复、丰富的度量统计和警报机制，这些都是在传统的消息传递系统中缺乏的 broker server负责消息的存储传递，消息查询，保证高可用等等。像下图所示，broker server有一些非常重要的子模块：(1)remoting（远程） 模块，broker的入口，处理从客户端发起的请求。(2)client manager（客户端管理） 管理各个客户端（生产者/消费者）还有维护消费者主题订阅。(3)store（存储服务），提供简单的api来在磁盘保持或者查询消息。(4)HA 高可用服务 提供主从broker的数据同步。(5)index(索引服务)为消息建立索引提供消息快速查询。Producerproduce支持分布式部署，分布式的produce通过broker集群提供的各种负载均衡策略将消息发送到broker集群中。发送过程支持快速失败是低延迟的 Consumer消费者也支持在推送和者拉取模式下分布式部署，支持集群消费和消息广播。提供实时的消息订阅机制，能够满足大多数消费者的需求. 名词解释Topic 消息主题，一级消息类型，通过 Topic 对消息进行分类。 Tag 消息标签，二级消息类型，用来进一步区分某个 Topic 下的消息分类。 Producer 消息生产者，也称为消息发布者，负责生产并发送消息。 Producer ID 一类 Producer 的标识，这类 Producer 通常生产并发送一类消息，且发送逻辑一致。 Producer 实例 Producer 的一个对象实例，不同的 Producer 实例可以运行在不同进程内或者不同机器上。Producer 实例线程安全，可在同一进程内多线程之间共享。 Consumer 消息消费者，也称为消息订阅者，负责接收并消费消息。 Consumer ID 一类 Consumer 的标识，这类 Consumer 通常接收并消费一类消息，且消费逻辑一致。 Consumer 实例 Consumer 的一个对象实例，不同的 Consumer 实例可以运行在不同进程内或者不同机器上。一个 Consumer 实例内配置线程池消费消息。 集群消费 一个 Consumer ID 所标识的所有 Consumer 平均分摊消费消息。例如某个 Topic 有 9 条消息，一个 Consumer ID 有 3 个 Consumer 实例，那么在集群消费模式下每个实例平均分摊，只消费其中的 3 条消息。 广播消费 一个 Consumer ID 所标识的所有 Consumer 都会各自消费某条消息一次。例如某个 Topic 有 9 条消息，一个 Consumer ID 有 3 个 Consumer 实例，那么在广播消费模式下每个实例都会各自消费 9 条消息。 定时消息 Producer 将消息发送到 MQ 服务端，但并不期望这条消息立马投递，而是推迟到在当前时间点之后的某一个时间投递到 Consumer 进行消费，该消息即定时消息。 延时消息 Producer 将消息发送到 MQ 服务端，但并不期望这条消息立马投递，而是延迟一定时间后才投递到 Consumer 进行消费，该消息即延时消息。 事务消息 MQ 提供类似 X/Open XA 的分布事务功能，通过 MQ 事务消息能达到分布式事务的最终一致。 顺序消息 MQ 提供的一种按照顺序进行发布和消费的消息类型, 分为全局顺序消息和分区顺序消息。 顺序发布 对于指定的一个 Topic，客户端将按照一定的先后顺序进行发送消息。 顺序消费 对于指定的一个 Topic，按照一定的先后顺序进行接收消息，即先发送的消息一定会先被客户端接收到。 全局顺序消息 对于指定的一个 Topic，所有消息按照严格的先入先出（FIFO）的顺序进行发布和消费。 分区顺序消息 对于指定的一个 Topic，所有消息根据 sharding key 进行区块分区。同一个分区内的消息按照严格的 FIFO 顺序进行发布和消费。Sharding key 是顺序消息中用来区分不同分区的关键字段，和普通消息的 key 是完全不同的概念。 消息堆积 Producer 已经将消息发送到 MQ 服务端，但由于 Consumer 消费能力有限，未能在短时间内将所有消息正确消费掉，此时在 MQ 服务端保存着未被消费的消息，该状态即消息堆积。 消息过滤 订阅者可以根据消息标签（Tag）对消息进行过滤，确保订阅者最终只接收被过滤后的消息类型。消息过滤在 MQ 服务端完成。 消息轨迹 在一条消息从发布者发出到订阅者消费处理过程中，由各个相关节点的时间、地点等数据汇聚而成的完整链路信息。通过消息轨迹，用户能清晰定位消息从发布者发出，经由 MQ 服务端，投递给消息订阅者的完整链路，方便定位排查问题。 重置消费位点 以时间轴为坐标，在消息持久化存储的时间范围内（默认3天），重新设置消息订阅者对其订阅 Topic 的消费进度，设置完成后订阅者将接收设定时间点之后由消息发布者发送到 MQ 服务端的消息。 消息类型普通消息指没特性的消息,仅仅是个消息,区别于有特性的定时和延时消息、顺序消息和事务消息.发送普通消息有三种方式: 可靠同步发送原理：同步发送是指消息发送方发出数据后，会在收到接收方发回响应之后才发下一个数据包的通讯方式。应用场景：此种方式应用场景非常广泛，例如重要通知邮件、报名短信通知、营销短信系统等。 可靠异步发送原理：异步发送是指发送方发出数据后，不等接收方发回响应，接着发送下个数据包的通讯方式。 MQ 的异步发送，需要用户实现异步发送回调接口（SendCallback）。消息发送方在发送了一条消息后，不需要等待服务器响应即可返回，进行第二条消息发送。发送方通过回调接口接收服务器响应，并对响应结果进行处理。应用场景：异步发送一般用于链路耗时较长，对 RT 响应时间较为敏感的业务场景，例如用户视频上传后通知启动转码服务，转码完成后通知推送转码结果等。 单向（Oneway）发送原理：单向（Oneway）发送特点为发送方只负责发送消息，不等待服务器回应且没有回调函数触发，即只发送请求不等待应答。 此方式发送消息的过程耗时非常短，一般在微秒级别。应用场景：适用于某些耗时非常短，但对可靠性要求并不高的场景，例如日志收集。 定时消息和延时消息定时消息：Producer 将消息发送到 MQ 服务端，但并不期望这条消息立马投递，而是推迟到在当前时间点之后的某一个时间投递到 Consumer 进行消费，该消息即定时消息。延时消息：Producer 将消息发送到 MQ 服务端，但并不期望这条消息立马投递，而是延迟一定时间后才投递到 Consumer 进行消费，该消息即延时消息。适用场景消息生产和消费有时间窗口要求：比如在电商交易中超时未支付关闭订单的场景，在订单创建时会发送一条 MQ 延时消息。这条消息将会在30分钟以后投递给消费者，消费者收到此消息后需要判断对应的订单是否已完成支付。 如支付未完成，则关闭订单。如已完成支付则忽略。通过消息触发一些定时任务，比如在某一固定时间点向用户发送提醒消息。使用方式定时消息和延时消息的使用在代码编写上存在略微的区别：发送定时消息需要明确指定消息发送时间点之后的某一时间点作为消息投递的时间点。发送延时消息时需要设定一个延时时间长度，消息将从当前发送时间点开始延迟固定时间之后才开始投递。 顺序消息顺序消息（FIFO 消息）是 MQ 提供的一种严格按照顺序进行发布和消费的消息类型。 顺序消息指消息发布和消息消费都按顺序进行。顺序发布：对于指定的一个 Topic，客户端将按照一定的先后顺序发送消息。顺序消费：对于指定的一个 Topic，按照一定的先后顺序接收消息，即先发送的消息一定会先被客户端接收到。全局顺序 : 对于指定的一个 Topic，所有消息按照严格的先入先出（FIFO）的顺序进行发布和消费。全局顺序适用场景性能要求不高，所有的消息严格按照 FIFO 原则进行消息发布和消费的场景分区顺序对于指定的一个 Topic，所有消息根据 sharding key 进行区块分区。 同一个分区内的消息按照严格的 FIFO 顺序进行发布和消费。 Sharding key 是顺序消息中用来区分不同分区的关键字段，和普通消息的 Key 是完全不同的概念。适用场景性能要求高，以 sharding key 作为分区字段，在同一个区块中严格的按照 FIFO 原则进行消息发布和消费的场景。 示例【例一】用户注册需要发送发验证码，以用户 ID 作为 sharding key， 那么同一个用户发送的消息都会按照先后顺序来发布和订阅。 【例二】电商的订单创建，以订单 ID 作为 sharding key，那么同一个订单相关的创建订单消息、订单支付消息、订单退款消息、订单物流消息都会按照先后顺序来发布和订阅。 阿里巴巴集团内部电商系统均使用分区顺序消息，既保证业务的顺序，同时又能保证业务的高性能。 2.4 事务消息常见的分布式事务解决方案有：最终一致性，两阶段／三界阶段提交，TCC，本地消息表等。这些解决方案中，最终一致性的性能最好。可以通过RocketMQ实现最终一致性。RocketMQ事务消息实现方式:事务消息：MQ 提供类似 X/Open XA 的分布事务功能，通过 MQ 事务消息能达到分布式事务的最终一致。半消息：暂不能投递的消息，发送方已经将消息成功发送到了 MQ 服务端，但是服务端未收到生产者对该消息的二次确认，此时该消息被标记成“暂不能投递”状态，处于该种状态下的消息即半消息。消息回查：由于网络闪断、生产者应用重启等原因，导致某条事务消息的二次确认丢失，MQ 服务端通过扫描发现某条消息长期处于“半消息”时，需要主动向消息生产者询问该消息的最终状态（Commit 或是 Rollback），该过程即消息回查。 这篇讲的挺细致:分布式开放消息系统(RocketMQ)的原理与实践 消息模型集群消费集群： MQ 约定使用相同 Consumer ID 的订阅者属于同一个集群。同一个集群下的订阅者消费逻辑必须完全一致（包括 Tag 的使用），这些订阅者在逻辑上可以认为是一个消费节点。 集群消费当使用集群消费模式时，MQ 认为任意一条消息只需要被集群内的任意一个消费者处理即可 适用场景和注意事项消费端集群化部署，每条消息只需要被处理一次。由于消费进度在服务端维护，可靠性更高。集群消费模式下，每一条消息都只会被分发到一台机器上处理集群消费模式下，不保证每一次失败重投的消息路由到同一台机器上，因此处理消息时不应该做任何确定性假设。 广播消费当使用广播消费模式时，MQ 会将每条消息推送给集群内所有注册过的客户端，保证消息至少被每台机器消费一次。适用场景和注意事项广播消费模式下不支持顺序消息。每条消息都需要被相同逻辑的多台机器处理。消费进度在客户端维护，出现重复的概率稍大于集群模式。广播模式下，MQ 保证每条消息至少被每台客户端消费一次，但是并不会对消费失败的消息进行失败重投，因此业务方需要关注消费失败的情况。广播模式下，客户端第一次启动时默认从最新消息消费。客户端的消费进度是被持久化在客户端本地的隐藏文件中，因此不建议删除该隐藏文件，否则会丢失部分消息。广播模式下，每条消息都会被大量的客户端重复处理，因此推荐尽可能使用集群模式。目前仅 Java 客户端支持广播模式。广播模式下服务端不维护消费进度，所以 MQ 控制台不支持消息堆积查询和堆积报警功能。 点对点(P2P)点对点（Point to Point, 简称 P2P），顾名思义，是一对一的消息传递模式，即只有一个消息发送者和一个消息接收者。而发布/订阅（Pub/Sub）通常用于一对多或多对多的消息群发场景，拥有一个或多个消息发送者和多个消息接收者。 在点对点（P2P）模型中，发送者发送消息时已经明确该消息预期的接收方信息，并明确该消息只需要被特定的单个客户端消费。发送者发送消息时通过 Topic 信息直接指定接收者，接收者无需提前进行订阅即可获取该消息。点对点模式可以节省接收方注册订阅关系的成本，而且收发消息的链路有单独的优化，推送延迟更低。P2P和pub/sub的区别:发送消息时，Pub/Sub 模式需要按照和接收端约定好的 Topic 发送消息，而 P2P 模式下无需事先约定传输消息的 Topic，发送端可以直接按照规范发送消息到目标的接收客户端。接收消息时，Pub/Sub 模式需要按照和发送端约定好的 Topic 提前订阅才能收到消息，而 P2P 模式下无需事先订阅，可以简化接收端的程序逻辑，并节省订阅的成本。 消息过滤消息过滤可在broker和consumer端过滤,一般在consumer上过滤,因为单消息量大时,broker压力过重,影响吞吐量做好过滤得明确为什么要产生这样的消息,明确topic和tag设计的侧重.Topic：消息主题，通过 Topic 对不同的业务消息进行分类；Tag：消息标签，用来进一步区分某个 Topic 下的消息分类，MQ 允许消费者按照Tag 对消息进行过滤，确保消费者最终只消费到他关注的消息类型。topic是一级标签,tag是二级标签.使用准则:1.消息类型是否一致,事务消息,顺序消息等2.业务相关性:没有直接关联的话应用多个topic,例如订单,物流,支付,而男装订单,女装订单则可以使用tag3.消息量级是否相当：A消息是万亿量级,B消息是轻量级但要求实时性高,即便A和B符合准则1,2,也挺该使用不同topic,避免B消息被 “拖累” 刷盘策略先看下影响消息可靠性的几种情况： 1.Broker正常关闭2.Broker异常Crash3.OS Crash4.机器掉电，但是能立即恢复供电情况。5.机器无法开机（可能是cpu、主板、内存等关键设备损坏）6.磁盘设备损坏。1,2,3,4四种情况都属于硬件资源可立即恢复情况,异步复制的话,可以保证99%的消息不丢失.5,6属于单点故障,同步刷盘可保证所有消息不丢失 异步复制(异步刷盘)当集群做了主从配置(多主多从),producer向master发送消息,master立马返回,此后根据设置的策略,slave从master上pull消息,进行同步,当master挂了后,slave不会主动提升为master,但仍可订阅 同步双写producer向master发送消息,只有master和slave都写入成功时,才返回.性能较异步略低,适用于对消息严格的业务,比如带money 安装与使用 环境要求：64bit OS, Linux/Unix/Mac is recommended;64bit JDK 1.8+;Maven 3.2.x;Git;4g+ free disk for Broker server 下载编译Click here to download the 4.4.0 source release. 1234&gt; unzip rocketmq-all-4.4.0-source-release.zip&gt; cd rocketmq-all-4.4.0/&gt; mvn -Prelease-all -DskipTests clean install -U&gt; cd distribution/target/apache-rocketmq 启动 Name Server 123&gt; nohup sh bin/mqnamesrv &amp;&gt; tail -f ~/logs/rocketmqlogs/namesrv.logThe Name Server boot success... ⚠️注意事项：若在启动过程中查看日志报错如下：1ERROR: Please set the JAVA_HOME variable in your environment, We need Java(x64)! !! 打开启动脚本runserver.sh以及runbroker.sh文件，发现有如下三行：123[ ! -e "$JAVA_HOME/bin/java" ] &amp;&amp; JAVA_HOME=$HOME/jdk/java[ ! -e "$JAVA_HOME/bin/java" ] &amp;&amp; JAVA_HOME=/usr/java[ ! -e "$JAVA_HOME/bin/java" ] &amp;&amp; error_exit "Please set the JAVA_HOME variable in your environment, We need java(x64)!" 这里默认设置java安装路径为$HOME/jdk/java,其中第二行是阿里巴巴集团内部服务器上的java目录，若报以上错误，将这一行注释掉。然后第一行的JAVA_HOME的值改为自己机器的Java安装目录。然后再次起送mqnameserver以及mqbroker，观察日志发现启动成功：特别是MAC OS tips:MAC OS reference :MAC OS Start 启动 Broker123&gt; nohup sh bin/mqbroker -n localhost:9876 &amp;&gt; tail -f ~/logs/rocketmqlogs/broker.log The broker[%s, 172.30.30.233:10911] boot success... 启动自动创建topic1nohup sh mqbroker -n localhost:9876 autoCreateTopicEnable=true &amp; 发送消费消息 关闭 Servers 1234567&gt; sh bin/mqshutdown brokerThe mqbroker(36695) is running...Send shutdown request to mqbroker(36695) OK&gt; sh bin/mqshutdown namesrvThe mqnamesrv(36664) is running...Send shutdown request to mqnamesrv(36664) OK RocketMQ插件RocketMQ-Console RocketMQ-Console是RocketMQ项目的扩展插件，是一个图形化管理控制台，提供Broker集群状态查看，Topic管理，Producer、Consumer状态展示，消息查询等常用功能，这个功能在安装好RocketMQ后需要额外单独安装、运行。 进入rocketmq-externals项目GitHub地址，如下图，可看到RocketMQ项目的诸多扩展项目，其中就包含我们需要下载的rocketmq-console。 使用git命令下载项目源码，由于我们仅需要rocketmq-console，故下载此项目对应分支即可。 1$ git clone -b release-rocketmq-console-1.0.0 https://github.com/apache/rocketmq-externals.git 进入项目文件夹并按照实际情况修改对应配置文件 12345678910111213141516server.contextPath=server.port=8080#spring.application.index=truespring.application.name=rocketmq-consolespring.http.encoding.charset=UTF-8spring.http.encoding.enabled=truespring.http.encoding.force=truelogging.config=classpath:logback.xml#if this value is empty,use env value rocketmq.config.namesrvAddr NAMESRV_ADDR | now, you can set it in ops page.default localhost:9876rocketmq.config.namesrvAddr=localhost:9876#if you use rocketmq version &lt; 3.5.8, rocketmq.config.isVIPChannel should be false.default truerocketmq.config.isVIPChannel=#rocketmq-console&apos;s data path:dashboard/monitorrocketmq.config.dataPath=/tmp/rocketmq-console/data#set it false if you don&apos;t want use dashboard.default truerocketmq.config.enableDashBoardCollect=true Name Server地址默认为空，注释说可以在启动项目后在后台配置，经测试，后台配置切换失败，有报错，所以打包前需修改配置文件明确给出Name Server地址，或者启动服务的时候给出rocketmq.config.namesrvAddr参数值。 将项目打成jar包，并运行jar文件。 1234$ mvn clean package -Dmaven.test.skip=true$ java -jar target/rocketmq-console-ng-1.0.0.jar#如果配置文件没有填写Name Server$ java -jar target/rocketmq-console-ng-1.0.0.jar --rocketmq.config.namesrvAddr='127.0.0.1:9876' 启动成功后，访问地址http://localhost:8080/rocketmq, 即可进入管理后台操作。 RocketMQ命令行管理工具(CLI Admin Tool) 命令行管理工具（CLI Admin Tool）对RocketMQ集群的管理提供了更多精细化的管理命令，命令行的方式对操作人员的要求稍高一些，当然，掌握了使用方法，就会简单高效很多。命令行管理工具无需额外安装，已经包含在${RocketMQ_HOME}/bin文件夹下面。 上面已经讲过命令行管理工具已经包含在RocketMQ项目中，我们进入项目下的bin文件夹，并执行命令bash mqadmin:1234567891011121314151617181920212223242526272829303132333435363738394041424344The most commonly used mqadmin commands are: updateTopic Update or create topic deleteTopic Delete topic from broker and NameServer. updateSubGroup Update or create subscription group deleteSubGroup Delete subscription group from broker. updateBrokerConfig Update broker's config updateTopicPerm Update topic perm topicRoute Examine topic route info topicStatus Examine topic Status info topicClusterList get cluster info for topic brokerStatus Fetch broker runtime status data queryMsgById Query Message by Id queryMsgByKey Query Message by Key queryMsgByUniqueKey Query Message by Unique key queryMsgByOffset Query Message by offset printMsg Print Message Detail printMsgByQueue Print Message Detail sendMsgStatus send msg to broker. brokerConsumeStats Fetch broker consume stats data producerConnection Query producer's socket connection and client version consumerConnection Query consumer's socket connection, client version and subscription consumerProgress Query consumers's progress, speed consumerStatus Query consumer's internal data structure cloneGroupOffset clone offset from other group. clusterList List all of clusters topicList Fetch all topic list from name server updateKvConfig Create or update KV config. deleteKvConfig Delete KV config. wipeWritePerm Wipe write perm of broker in all name server resetOffsetByTime Reset consumer offset by timestamp(without client restart). updateOrderConf Create or update or delete order conf cleanExpiredCQ Clean expired ConsumeQueue on broker. cleanUnusedTopic Clean unused topic on broker. startMonitoring Start Monitoring statsAll Topic and Consumer tps stats allocateMQ Allocate MQ checkMsgSendRT check message send response time clusterRT List All clusters Message Send RT getNamesrvConfig Get configs of name server. updateNamesrvConfig Update configs of name server. getBrokerConfig Get broker config by cluster or special broker! queryCq Query cq command. sendMessage Send a message consumeMessage Consume message 上面清单中左边为命令名称，右边为命令含义的解释，可以看到，大部分我们常用的功能已包含其中，具体如何使用这些命令，可以通过执行bash mqadmin help 来了解细节，我们以常用命令updateTopic为例，执行bash mqadmin help updateTopic,打印如下信息：12345678910111213usage: mqadmin updateTopic [-b &lt;arg&gt;] [-c &lt;arg&gt;] [-h] [-n &lt;arg&gt;] [-o &lt;arg&gt;] [-p &lt;arg&gt;] [-r &lt;arg&gt;] [-s &lt;arg&gt;] -t &lt;arg&gt; [-u &lt;arg&gt;] [-w &lt;arg&gt;] -b,--brokerAddr &lt;arg&gt; create topic to which broker -c,--clusterName &lt;arg&gt; create topic to which cluster -h,--help Print help -n,--namesrvAddr &lt;arg&gt; Name server address list, eg: 192.168.0.1:9876;192.168.0.2:9876 -o,--order &lt;arg&gt; set topic's order(true|false) -p,--perm &lt;arg&gt; set topic's permission(2|4|6), intro[2:W 4:R; 6:RW] -r,--readQueueNums &lt;arg&gt; set read queue nums -s,--hasUnitSub &lt;arg&gt; has unit sub (true|false) -t,--topic &lt;arg&gt; topic name -u,--unit &lt;arg&gt; is unit topic (true|false) -w,--writeQueueNums &lt;arg&gt; set write queue nums 可以看见，刚才新建的TopicTest以及一些系统默认的topic。如果想学习了解这些命令的源码实现可以点击查看这里。 参考 apache:http://rocketmq.apache.org/docs/motivation/ alibaba:https://help.aliyun.com/document_detail/29532.html https://my.oschina.net/buru1kan/blog/1814356 http://xiajunhust.github.io/2016/11/12/RocketMQ%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%80-MAC%E7%B3%BB%E7%BB%9F%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义注解-方法重试@RetryProcess]]></title>
    <url>%2F2019%2F06%2F20%2F%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3-%E6%96%B9%E6%B3%95%E9%87%8D%E8%AF%95%2F</url>
    <content type="text"><![CDATA[背景在项目开发中，有时候会出现接口调用失败，本身调用又是异步的，如果是因为一些网络问题请求超时，总想可以重试几次把任务处理掉。 一些RPC框架，比如dubbo都是有重试机制的，但是并不是每一个项目多会使用dubbo框架，常规的小项目有时候直接使用http进行不同项目之间的交互。 思路使用spring aop和自定义注解来，建立一套重试机制。根据切入点和自定义注解，来完成重试工作。 自定义注解定义注解12345678910111213141516171819202122232425package com.github.feifuzeng.study.annotation;import org.springframework.stereotype.Component;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/** * @author feifz * @version 1.0.0 * @Description 重试机制自定义注解 * @createTime 2019年06月20日 14:05:00 */@Target(&#123;ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface RetryProcess &#123; int value() default 1; int sleep() default 1000;&#125; 注解中value参数为重试次数，默认为1，sleep为重试间隔，默认为1000毫秒。 定义切面12345678910111213141516171819202122232425262728293031323334353637383940414243package com.github.feifuzeng.study.annotation;/** * @author feifz * @version 1.0.0 * @Description 重试注解切面 * @createTime 2019年06月20日 14:07:00 */import lombok.extern.log4j.Log4j2;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.reflect.MethodSignature;import org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint;import org.springframework.stereotype.Component;import java.util.concurrent.atomic.AtomicInteger;@Aspect@Component@Log4j2public class RetryProcessAspect &#123; private AtomicInteger atomicInteger = new AtomicInteger(0); @AfterThrowing(pointcut=("@annotation(com.github.feifuzeng.study.annotation.RetryProcess)")) public void tryAgain(JoinPoint point) &#123; try &#123; MethodSignature methodSignature = (MethodSignature) point.getSignature(); RetryProcess retryProcess = methodSignature.getMethod().getAnnotation(RetryProcess.class); if (atomicInteger.intValue() &lt; retryProcess.value()) &#123; int i = atomicInteger.incrementAndGet(); Thread.sleep(retryProcess.sleep() * i); log.debug("开始重试第" + i + "次"); MethodInvocationProceedingJoinPoint methodPoint = ((MethodInvocationProceedingJoinPoint) point); methodPoint.proceed(); &#125; &#125; catch (Throwable throwable) &#123; tryAgain(point); &#125; &#125;&#125; 使用示例1234@RetryProcess(value = 2,sleep = 2000) public List&lt;User&gt; findList() &#123; return userMapper.queryUserList(); &#125; 该自定义注解用于方法上，当改方法在执行过程中存在异常，不管是系统异常还是手动抛出的业务异常，均可实现重试。 知识课堂JoinPoint 对象 JoinPoint对象封装了SpringAop中切面方法的信息,在切面方法中添加JoinPoint参数,就可以获取到封装了该方法信息的JoinPoint对象.常用api: 方法名 功能 Signature getSignature(); 获取封装了署名信息的对象,在该对象中可以获取到目标方法名,所属类的Class等信息 Object[] getArgs(); 获取传入目标方法的参数对象 Object getTarget(); 获取被代理的对象 Object getThis(); 获取代理对象 ProceedingJoinPoint对象 ProceedingJoinPoint对象是JoinPoint的子接口,该对象只用在@Around的切面方法中,添加了 Object proceed() throws Throwable //执行目标方法Object proceed(Object[] var1) throws Throwable //传入的新的参数去执行目标方法 两个方法.MethodInvocationProceedingJoinPoint 对象 查看源码得知MethodInvocationProceedingJoinPoint是ProceedingJoinPoint和StaticPart的实现，只要使用的还是ProceedingJoinPoint对象的两个实现方法。 参考 https://www.cnblogs.com/tom-plus/p/7844228.html]]></content>
      <categories>
        <category>注解</category>
      </categories>
      <tags>
        <tag>自定义注解</tag>
        <tag>方法重试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前后端API交互如何保证数据安全性？]]></title>
    <url>%2F2019%2F06%2F17%2F%E5%89%8D%E5%90%8E%E7%AB%AFAPI%E4%BA%A4%E4%BA%92%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8%E6%80%A7%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言前后端分离的开发方式，我们以接口为标准来进行推动，定义好接口，各自开发自己的功能，最后进行联调整合。无论是开发原生的APP还是webapp还是PC端的软件,只要是前后端分离的模式，就避免不了调用后端提供的接口来进行业务交互。网页或者app，只要抓下包就可以清楚的知道这个请求获取到的数据，这样的接口对爬虫工程师来说是一种福音，要抓你的数据简直轻而易举。数据的安全性非常重要，特别是用户相关的信息，稍有不慎就会被不法分子盗用，所以我们对这块要非常重视，容不得马虎。 如何保证API调用时数据的安全性？ 通信使用https 请求签名，防止参数被篡改 身份确认机制，每次请求都要验证是否合法 APP中使用ssl pinning防止抓包操作 对所有请求和响应都进行加解密操作 等等方案……. 对所有请求和响应都进行加解密操作方案有很多种，当你做的越多，也就意味着安全性更高，今天我跟大家来介绍一下对所有请求和响应都进行加解密操作的方案，即使能抓包，即使能调用我的接口，但是我返回的数据是加密的，只要加密算法够安全，你得到了我的加密内容也对我没什么影响。像这种工作最好做成统一处理的，你不能让每个开发都去关注这件事情，如果让每个开发去关注这件事情就很麻烦了，返回数据时还得手动调用下加密的方法，接收数据后还得调用下解密的方法。为此，我基于Spring Boot封装了一个Starter, 内置了AES加密算法。GitHub地址如下：https://github.com/feifuzeng/spring-boot-starter-encrypt 入门使用 下载源码并在项目工程中引入 在启动类上增加加解密注解在启动类上增加@EnableEncrypt注解开启加解密操作： 1234567@EnableEncrypt@SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 配置文件增加配置 12spring.encrypt.key=abcdef0123456789spring.encrypt.debug=false spring.encrypt.key：加密key，必须是16位spring.encrypt.debug：是否开启调试模式,默认为false,如果为true则不启用 加解密操作为了考虑通用性，不会对所有请求都执行加解密，基于注解来做控制响应数据需要加密的话，就在Controller的方法上加@Encrypt注解即可。123456789101112@Encrypt@RequestMapping("/list")@ResponseBodypublic List&lt;User&gt; query()&#123; List&lt;User&gt; list = new ArrayList&lt;&gt;(); User user = new User(); user.setGender(1); user.setId("1"); user.setName("11111"); list.add(user); return list;&#125; 当我们访问/list接口时，返回的数据就是加密之后base64编码的格式。还有一种操作就是前端提交的数据，分为2种情况，一种是get请求，这种暂时没处理，后面再考虑，目前只处理的post请求，基于json格式提交的方式，也就是说后台需要用@RequestBody接收数据才行, 需要解密的操作我们加上@Decrypt注解即可。123456789@Encrypt@Decrypt@RequestMapping("/queryList")@ResponseBodypublic List&lt;User&gt; queryList(@RequestBody User user)&#123; List&lt;User&gt; list = new ArrayList&lt;&gt;(); list.add(user); return list;&#125; 加了@Decrypt注解后，前端提交的数据需要按照AES加密算法，进行加密，然后提交到后端，后端这边会自动解密，然后再映射到参数对象中。上面讲解的都是后端的代码，前端使用的话我们以js来讲解，当然你也能用别的语言来做。前端需要做的就2件事情： 统一处理数据的响应，在渲染到页面之前进行解密操作 当有POST请求的数据发出时，统一加密js加密文件请参考我GitHub中示例工程中引入的js文件示例代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;数据传输解密示例&lt;/title&gt; &lt;/head&gt; &lt;script type="text/javascript" src = "js/aes.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src = "js/crypto-js.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src = "js/pad-zeropadding.js"&gt;&lt;/script&gt; &lt;script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"&gt;&lt;/script&gt;&lt;body&gt; &lt;input type="button" value="获取数据" onclick="getData()"/&gt; &lt;hr&gt; &lt;br&gt; 姓名：&lt;input type="text" id="name" /&gt;&lt;br&gt; 性别：&lt;input type="text" id="gender" /&gt;&lt;br&gt; &lt;input type="button" value="发送数据" onclick="sendData()"/&gt; &lt;script&gt; var backUrl =''; //var backUrl ='http://localhost:8080'; function getData() &#123; $.ajax(&#123; type: "GET", url:backUrl+"/user/list", success: function(resData) &#123; alert("返回的数据："+resData); alert("解密之后的数据："+Decrypt(resData)); &#125; &#125;); &#125; function sendData() &#123; var name=document.getElementById("name").value; var gender=document.getElementById("gender").value; alert("发送的数据："+JSON.stringify(&#123;"name":name,"gender":gender&#125;)); $.ajax(&#123; type: "POST", url:backUrl+"/user/queryList", data:JSON.stringify(&#123;"name":name,"gender":gender&#125;), dataType:'json', contentType: "application/json", success: function(resData) &#123; alert("返回的数据："+resData); alert("解密之后："+Decrypt(resData)); &#125; &#125;); &#125; var key = CryptoJS.enc.Utf8.parse("abcdef0123456789"); function Encrypt(word) &#123; var srcs = CryptoJS.enc.Utf8.parse(word); var encrypted = CryptoJS.AES.encrypt(srcs, key, &#123; mode : CryptoJS.mode.ECB, padding : CryptoJS.pad.Pkcs7 &#125;); return encrypted.toString(); &#125; function Decrypt(word) &#123; var decrypt = CryptoJS.AES.decrypt(word, key, &#123; mode : CryptoJS.mode.ECB, padding : CryptoJS.pad.Pkcs7 &#125;); return CryptoJS.enc.Utf8.stringify(decrypt).toString(); &#125; &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 到此为止，我们就为整个前后端交互的通信做了一个加密的操作，只要加密的key不泄露，别人得到你的数据也没用，问题是如何保证key不泄露呢？服务端的安全性较高，可以存储在数据库中或者配置文件中，毕竟在我们自己的服务器上，最危险的其实就时前端了，app还好，可以打包，但是要防止反编译等等问题。如果是webapp则可以依赖于js加密来实现，下面我给大家介绍一种动态获取加密key的方式，只不过实现起来比较复杂，我们不上代码，只讲思路：加密算法有对称加密和非对称加密，AES是对称加密，RSA是非对称加密。之所以用AES加密数据是因为效率高，RSA运行速度慢,可以用于签名操作。我们可以用这2种算法互补，来保证安全性，用RSA来加密传输AES的秘钥，用AES来加密数据，两者相互结合，优势互补。其实大家理解了HTTPS的原理的话对于下面的内容应该是一看就懂的，HTTPS比HTTP慢的原因都是因为需要让客户端与服务器端安全地协商出一个对称加密算法。剩下的就是通信时双方使用这个对称加密算法进行加密解密。客户端启动，发送请求到服务端，服务端用RSA算法生成一对公钥和私钥，我们简称为pubkey1,prikey1，将公钥pubkey1返回给客户端。客户端拿到服务端返回的公钥pubkey1后，自己用RSA算法生成一对公钥和私钥，我们简称为pubkey2,prikey2，并将公钥pubkey2通过公钥pubkey1加密，加密之后传输给服务端。此时服务端收到客户端传输的密文，用私钥prikey1进行解密，因为数据是用公钥pubkey1加密的，通过解密就可以得到客户端生成的公钥pubkey2然后自己在生成对称加密，也就是我们的AES,其实也就是相对于我们配置中的那个16的长度的加密key,生成了这个key之后我们就用公钥pubkey2进行加密，返回给客户端，因为只有客户端有pubkey2对应的私钥prikey2，只有客户端才能解密，客户端得到数据之后，用prikey2进行解密操作，得到AES的加密key,最后就用加密key进行数据传输的加密，至此整个流程结束。 spring-boot-starter-encrypt原理最后我们来简单的介绍下spring-boot-starter-encrypt的原理吧，也让大家能够理解为什么Spring Boot这么方便，只需要简单的配置一下就可以实现很多功能。 启动类上的@EnableEncrypt注解是用来开启功能的,通过@Import导入自动配置类12345678@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import(&#123;EncryptAutoConfiguration.class&#125;)public @interface EnableEncrypt &#123;&#125; EncryptAutoConfiguration中配置请求和响应的处理类，用的是Spring中的RequestBodyAdvice和ResponseBodyAdvice，在Spring中对请求进行统计处理比较方便。如果还要更底层去封装那就要从servlet那块去处理了。123456789101112131415161718192021222324252627282930/** * 加解密自动配置 * */@Configuration@Component@EnableAutoConfiguration@EnableConfigurationProperties(EncryptProperties.class)public class EncryptAutoConfiguration &#123; /** * 配置请求解密 * @return */ @Bean public EncryptResponseBodyAdvice encryptResponseBodyAdvice() &#123; return new EncryptResponseBodyAdvice(); &#125; /** * 配置请求加密 * @return */ @Bean public EncryptRequestBodyAdvice encryptRequestBodyAdvice() &#123; return new EncryptRequestBodyAdvice(); &#125;&#125; 通过RequestBodyAdvice和ResponseBodyAdvice就可以对请求响应做处理了，大概的原理就是这么多了。 参考 http://cxytiandi.com/blog/detail/20235]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>前后端</tag>
        <tag>加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之-工厂模式]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B-%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基本概念工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。 介绍意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。 主要解决：主要解决接口选择的问题。 何时使用：我们明确地计划不同条件下创建不同实例时。 如何解决：让其子类实现工厂接口，返回的也是一个抽象的产品。 关键代码：创建过程在其子类执行。 应用实例： 1、您需要一辆汽车，可以直接从工厂里面提货，而不用去管这辆汽车是怎么做出来的，以及这个汽车里面的具体实现。 2、Hibernate 换数据库只需换方言和驱动就可以。 优点： 1、一个调用者想创建一个对象，只要知道其名称就可以了。 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 3、屏蔽产品的具体实现，调用者只关心产品的接口。 缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。 使用场景： 1、日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方。 2、数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时。 3、设计一个连接服务器的框架，需要三个协议，”POP3”、”IMAP”、”HTTP”，可以把这三个作为产品类，共同实现一个接口。 注意事项：作为一种创建类模式，在任何需要生成复杂对象的地方，都可以使用工厂方法模式。有一点需要注意的地方就是复杂对象适合使用工厂模式，而简单对象，特别是只需要通过 new 就可以完成创建的对象，无需使用工厂模式。如果使用工厂模式，就需要引入一个工厂类，会增加系统的复杂度。 实现我们将创建一个 Shape 接口和实现 Shape 接口的实体类。下一步是定义工厂类 ShapeFactory。FactoryPatternDemo，我们的演示类使用 ShapeFactory 来获取 Shape 对象。它将向 ShapeFactory 传递信息（CIRCLE / RECTANGLE / SQUARE），以便获取它所需对象的类型。 步骤 1创建一个接口: Shape.java 123public interface Shape &#123; void draw();&#125; 步骤 2创建实现接口的实体类。 Rectangle.java 1234567public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Rectangle::draw() method."); &#125;&#125; Square.java1234567public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println("Inside Square::draw() method."); &#125;&#125; Circle.java1234567public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Circle::draw() method.&quot;); &#125;&#125; 步骤 3创建一个工厂，生成基于给定信息的实体类的对象。 ShapeFactory.java1234567891011121314151617public class ShapeFactory &#123; //使用 getShape 方法获取形状类型的对象 public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase("CIRCLE"))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase("RECTANGLE"))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase("SQUARE"))&#123; return new Square(); &#125; return null; &#125;&#125; 步骤 4使用该工厂，通过传递类型信息来获取实体类的对象。 FactoryPatternDemo.java123456789101112131415161718192021222324public class FactoryPatternDemo &#123; public static void main(String[] args) &#123; ShapeFactory shapeFactory = new ShapeFactory(); //获取 Circle 的对象，并调用它的 draw 方法 Shape shape1 = shapeFactory.getShape("CIRCLE"); //调用 Circle 的 draw 方法 shape1.draw(); //获取 Rectangle 的对象，并调用它的 draw 方法 Shape shape2 = shapeFactory.getShape("RECTANGLE"); //调用 Rectangle 的 draw 方法 shape2.draw(); //获取 Square 的对象，并调用它的 draw 方法 Shape shape3 = shapeFactory.getShape("SQUARE"); //调用 Square 的 draw 方法 shape3.draw(); &#125;&#125; 步骤 5执行程序，输出结果：123Inside Circle::draw() method.Inside Rectangle::draw() method.Inside Square::draw() method. 参考 http://www.runoob.com/design-pattern/factory-pattern.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>工厂模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之-装饰器模式]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B-%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基本概念 装饰器模式，顾名思义起的是装饰的作用，就是在一个类上增加功能。如果通过继承来增加功能，在不修改代码的情况下，如果增加功能多的话，会使类的数量爆炸式增长，为管理带来巨大的麻烦。装饰器模式就比较好地解决了这一点。 介绍以下为装饰器模式的通用类图: Component，一般是接口或者抽象类，定义了最简单的方法，装饰器类和被装饰类都要实现该接口。 ConcreteComponent，被装饰类，实现了Component。 Decorator，装饰器类，通过该类为ConcreteComponent动态添加额外的方法，实现了Component接口，并且该对象中持有一个Component的成员变量。 ConcreteDecoratorA，ConcreteDecoratorB，具体的装饰类，该类中的方法就是要为ConcreteComponent动态添加的方法。 实现我们以生产一件衣服为例，生产一件衣服本身是个很简单的过程，一块布料裁剪好了之后做出衣服的样子就可以了，但是这样的衣服是卖不出去的，因为毫无美感，我们需要通过一些装饰来使衣服变得好看。但是时代在变化，人们的审美也在变化，装饰总是不断在变的，所以我们就要有一个灵活机动的模式来修改装饰。 Clothes.java123public interface Clothes &#123; public void makeClothes();&#125; MakeClothes.java12345678public class MakeClothes implements Clothes &#123; @Override public void makeClothes() &#123; System.out.println("制作一件衣服"); &#125;&#125; 步骤 3创建装饰器。 OperationSubstract.java 123456public class OperationSubstract implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 - num2; &#125;&#125; 话不多说，先来个衣服的最初成品，就是毫无美感的那种，那么如果现在要增加装饰，可以用一个类继承MakeClothes，然后增加里面makeClothes()方法，但是如果过几天装饰就变了，那么又要改动代码，而且如果装饰过多，这个类就显得很庞杂，不好维护，这个时候装饰器模式就来大显身手了。 Decorator.java1234567891011public class Decorator implements Clothes &#123; private Clothes clothes; public Decorator(Clothes _clothes) &#123; this.clothes = _clothes; &#125; @Override public void makeClothes() &#123; clothes.makeClothes(); &#125;&#125; 这就是一个装饰器，它有一个构造函数，参数是一个衣服类，同时它重载了makeClothes()方法，以便它的子类对其进行修改。下面是两个子类，分别对衣服进行了绣花和镂空 Embroidery.java1234567891011121314public class Embroidery extends Decorator &#123; public Embroidery(Clothes _clothes) &#123; super(_clothes); &#125; public void embroidery() &#123; System.out.println("给衣服绣花"); &#125; @Override public void makeClothes() &#123; super.makeClothes(); this.embroidery(); &#125;&#125; Hollow.java1234567891011121314public class Hollow extends Decorator &#123; public Hollow(Clothes _clothes) &#123; super(_clothes); &#125; public void hollow() &#123; System.out.println("关键位置镂空"); &#125; @Override public void makeClothes() &#123; super.makeClothes(); this.hollow(); &#125;&#125; 这两个子类的构造器都传入一个衣服模型，而且两个子类分别有各自的方法——绣花和镂空，但是他们均重写了makeClothes()方法，在制作衣服的过程中加入了绣花和镂空的操作，这样一来，我们只需要增删改这几个装饰器的子类，就可以完成各种不同的装饰，简洁明了，一目了然。下面测试一下： DecoratorDemo.java12345678910public class DecoratorDemo &#123; public static void main(String[] args) &#123; Clothes clothes = new MakeClothes(); clothes = new Embroidery(clothes); clothes = new Hollow(clothes); clothes.makeClothes(); System.out.println("衣服做好了"); &#125;&#125; 执行程序，输出结果：1234制作一件衣服给衣服绣花关键位置镂空衣服做好了 参考 https://www.cnblogs.com/fengshenjingjun/p/8343655.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>装饰器模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之-策略模式]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基本概念 在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 介绍意图：定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。主要解决：在有多种算法相似的情况下，使用 if…else 所带来的复杂和难以维护。何时使用：一个系统有许多许多类，而区分它们的只是他们直接的行为。如何解决：将这些算法封装成一个一个的类，任意地替换。关键代码：实现同一个接口。应用实例： 1、诸葛亮的锦囊妙计，每一个锦囊就是一个策略。 2、旅行的出游方式，选择骑自行车、坐汽车，每一种旅行方式都是一个策略。 3、JAVA AWT 中的 LayoutManager。优点： 1、算法可以自由切换。 2、避免使用多重条件判断。 3、扩展性良好。缺点： 1、策略类会增多。 2、所有策略类都需要对外暴露。使用场景： 1、如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 2、一个系统需要动态地在几种算法中选择一种。 3、如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。注意事项：如果一个系统的策略多于四个，就需要考虑使用混合模式，解决策略类膨胀的问题。 实现我们将创建一个定义活动的 Strategy 接口和实现了 Strategy 接口的实体策略类。Context 是一个使用了某种策略的类。StrategyPatternDemo，我们的演示类使用 Context 和策略对象来演示 Context 在它所配置或使用的策略改变时的行为变化。 步骤 1创建一个接口。 Strategy.java123public interface Strategy &#123; public int doOperation(int num1, int num2);&#125; 步骤 2创建实现接口的实体类。 OperationAdd.java 123456public class OperationAdd implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 + num2; &#125;&#125; OperationSubstract.java123456public class OperationSubstract implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 - num2; &#125;&#125; OperationMultiply.java123456public class OperationMultiply implements Strategy&#123; @Override public int doOperation(int num1, int num2) &#123; return num1 * num2; &#125;&#125; 步骤 3创建 Context 类。 Context.java 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeStrategy(int num1, int num2)&#123; return strategy.doOperation(num1, num2); &#125;&#125; 步骤 4使用 Context 来查看当它改变策略 Strategy 时的行为变化。 StrategyPatternDemo.java 123456789101112public class StrategyPatternDemo &#123; public static void main(String[] args) &#123; Context context = new Context(new OperationAdd()); System.out.println("10 + 5 = " + context.executeStrategy(10, 5)); context = new Context(new OperationSubstract()); System.out.println("10 - 5 = " + context.executeStrategy(10, 5)); context = new Context(new OperationMultiply()); System.out.println("10 * 5 = " + context.executeStrategy(10, 5)); &#125;&#125; 步骤 5执行程序，输出结果：12310 + 5 = 1510 - 5 = 510 * 5 = 50 参考 http://www.runoob.com/design-pattern/strategy-pattern.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>策略模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之-观察者模式]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基本概念 观察者模式中，一个被观察者管理所有相依于它的观察者物件，并且在本身的状态改变时主动发出通知。这通常通过呼叫各观察者所提供的方法来实现。此种模式通常被用来实现事件处理系统。 角色 抽象被观察者角色：把所有对观察者对象的引用保存在一个集合中，每个被观察者角色都可以有任意数量的观察者。被观察者提供一个接口，可以增加和删除观察者角色。一般用一个抽象类和接口来实现。 抽象观察者角色：为所有具体的观察者定义一个接口，在得到主题的通知时更新自己。 具体被观察者角色：在被观察者内部状态改变时，给所有登记过的观察者发出通知。具体被观察者角色通常用一个子类实现。 具体观察者角色：该角色实现抽象观察者角色所要求的更新接口，以便使本身的状态与主题的状态相协调。通常用一个子类实现。如果需要，具体观察者角色可以保存一个指向具体主题角色的引用。 适用场景 1) 当一个抽象模型有两个方面, 其中一个方面依赖于另一方面。将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。 2) 当对一个对象的改变需要同时改变其它对象, 而不知道具体有多少对象有待改变。 3) 当一个对象必须通知其它对象，而它又不能假定其它对象是谁。换言之, 你不希望这些对象是紧密耦合的。 应用珠宝商运送一批钻石，有黄金强盗准备抢劫，珠宝商雇佣了私人保镖，警察局也派人护送，于是当运输车上路的时候，强盗保镖警察都要观察运输车一举一动， 抽象的观察者 1234public interface Watcher&#123; public void update();&#125; 抽象的被观察者，在其中声明方法（添加、移除观察者，通知观察者）： 12345678public interface Watched&#123; public void addWatcher(Watcher watcher); public void removeWatcher(Watcher watcher); public void notifyWatchers();&#125; 具体的观察者-保镖12345678public class Security implements Watcher&#123; @Override public void update() &#123; System.out.println(“运输车有行动，保安贴身保护"); &#125;&#125; 具体的观察者-强盗 12345678public class Thief implements Watcher&#123; @Override public void update() &#123; System.out.println(“运输车有行动，强盗准备动手"); &#125;&#125; 具体的观察者-警察 12345678public class Police implements Watcher&#123; @Override public void update() &#123; System.out.println(“运输车有行动，警察护航"); &#125;&#125; 具体的被观察者1234567891011121314151617181920212223242526public class Transporter implements Watched&#123; private List&lt;Watcher&gt; list = new ArrayList&lt;Watcher&gt;(); @Override public void addWatcher(Watcher watcher) &#123; list.add(watcher); &#125; @Override public void removeWatcher(Watcher watcher) &#123; list.remove(watcher); &#125; @Override public void notifyWatchers(String str) &#123; for (Watcher watcher : list) &#123; watcher.update(); &#125; &#125; &#125; 客户端1234567891011121314151617public class Test&#123; public static void main(String[] args) &#123; Transporter transporter = new Transporter(); Police police = new Police(); Security security = new Security(); Thief thief = new Thief(); transporter.addWatcher(police); transporter.addWatcher(security); transporter.addWatcher(security); transporter.notifyWatchers(); &#125;&#125; 小结 我推你拉 例子中没有关于数据和状态的变化通知，只是简单通知到各个观察者，告诉他们被观察者有行动。 观察者模式在关于目标角色、观察者角色通信的具体实现中，有两个版本。 一种情况便是目标角色在发生变化后，仅仅告诉观察者角色“我变化了”，观察者角色如果想要知道具体的变化细节，则就要自己从目标角色的接口中得到。这种模式被很形象的称为：拉模式——就是说变化的信息是观察者角色主动从目标角色中“拉”出来的。 还有一种方法，那就是我目标角色“服务一条龙”，通知你发生变化的同时，通过一个参数将变化的细节传递到观察者角色中去。这就是“推模式”——管你要不要，先给你啦。 这两种模式的使用，取决于系统设计时的需要。如果目标角色比较复杂，并且观察者角色进行更新时必须得到一些具体变化的信息，则“推模式”比较合适。如果目标角色比较简单，则“拉模式”就很合适啦。 参考 https://blog.csdn.net/jason0539/article/details/45055233]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>观察者模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之-职责链模式]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B-%E8%81%8C%E8%B4%A3%E9%93%BE%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[基本概念 什么是链 链是一系列节点的集合。 链的各节点可灵活拆分再重组。 职责链模式 使多个对象都有机会处理请求，从而避免请求的发送者和接受者之间的耦合关系， 将这个对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理他为止。 角色 抽象处理者角色(Handler)：定义出一个处理请求的接口。如果需要，接口可以定义 出一个方法以设定和返回对下家的引用。这个角色通常由一个Java抽象类或者Java接口实现。 具体处理者角色(ConcreteHandler)：具体处理者接到请求后，可以选择将请求处理掉，或者将请求传给下家。由于具体处理者持有对下家的引用，因此，如果需要，具体处理者可以访问下家。 职责链灵活在哪 改变内部的传递规则在内部，项目经理完全可以跳过人事部到那一关直接找到总经理。每个人都可以去动态地指定他的继任者。 可以从职责链任何一关开始。如果项目经理不在，可以直接去找部门经理，责任链还会继续，没有影响。 用与不用的区别不用职责链的结构，我们需要和公司中的每一个层级都发生耦合关系。如果反映在代码上即使我们需要在一个类中去写上很多丑陋的if….else语句。如果用了职责链，相当于我们面对的是一个黑箱，我们只需要认识其中的一个部门，然后让黑箱内部去负责传递就好了 纯的与不纯的责任链模式一个纯的责任链模式要求一个具体的处理者对象只能在两个行为中选择一个：一是承担责任，而是把责任推给下家。不允许出现某一个具体处理者对象在承担了一部分责任后又 把责任向下传的情况。在一个纯的责任链模式里面，一个请求必须被某一个处理者对象所接收；在一个不纯的责任链模式里面，一个请求可以最终不被任何接收端对象所接收。纯的责任链模式的实际例子很难找到，一般看到的例子均是不纯的责任链模式的实现。 示例 代码1.抽象处理者角色123456789101112131415161718192021222324public abstract class Handler &#123; /** * 持有后继的责任对象 */ protected Handler successor; /** * 示意处理请求的方法，虽然这个示意方法是没有传入参数的 * 但实际是可以传入参数的，根据具体需要来选择是否传递参数 */ public abstract void handleRequest(); /** * 取值方法 */ public Handler getSuccessor() &#123; return successor; &#125; /** * 赋值方法，设置后继的责任对象 */ public void setSuccessor(Handler successor) &#123; this.successor = successor; &#125; &#125; 2.具体处理者角色12345678910111213141516171819202122public class ConcreteHandler extends Handler &#123; /** * 处理方法，调用此方法处理请求 */ @Override public void handleRequest() &#123; /** * 判断是否有后继的责任对象 * 如果有，就转发请求给后继的责任对象 * 如果没有，则处理请求 */ if(getSuccessor() != null) &#123; System.out.println("放过请求"); getSuccessor().handleRequest(); &#125;else &#123; System.out.println("处理请求"); &#125; &#125; &#125; 3.客户端类1234567891011public class Client &#123; public static void main(String[] args) &#123; //组装责任链 Handler handler1 = new ConcreteHandler(); Handler handler2 = new ConcreteHandler(); handler1.setSuccessor(handler2); //提交请求 handler1.handleRequest(); &#125;&#125; 说明可以看出，客户端创建了两个处理者对象，并指定第一个处理者对象的下家是第二个处理者对象，而第二个处理者对象没有下家。然后客户端将请求传递给第一个处理者对象。由于本示例的传递逻辑非常简单：只要有下家，就传给下家处理；如果没有下家，就自行处理。因此，第一个处理者对象接到请求后，会将请求传递给第二个处理者对象。由于第二个处理者对象没有下家，于是自行处理请求。活动时序图如下所示。 参考 https://blog.csdn.net/jason0539/article/details/45091639#]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>职责链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话设计模式]]></title>
    <url>%2F2019%2F06%2F11%2F%E5%A4%A7%E8%AF%9D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[介绍分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。其实还有两类：并发型模式和线程池模式。 中英文对照 中文 英文 1.工厂方法模式 Factory Method Pattern 2.抽象工厂模式 Abstract Factory Pattern 3.建造者模式 Builder Pattern 4.原型模式 Prototype Pattern 5.单例模式 Singleton Pattern 6.适配器模式 Adapter Pattern 7.桥梁模式/桥接模式 Bridge Pattern 8.组合模式 Composite Pattern 9.装饰模式 Decorator Pattern 10.门面模式/外观模式 Facade Pattern 11.享元模式 Flyweight Pattern 12.代理模式 Proxy pattern 13.责任链模式 Chain of Responsibility Pattern 14.命令模式 Command Pattern 15.解释器模式 Interpreter Pattern 16.迭代器模式 Iterator Pattern 17.中介者模式 Mediator Pattern 18.备忘录模式 Memento Pattern 19.观察者模式 Observer Pattern 20状态模式 State Pattern 21.策略模式 Strategy Pattern 22.模板方法模式 Template Method Pattern 23.访问者模式 Visitor Pattern 参考 http://www.runoob.com/design-pattern/design-pattern-intro.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具类🔧-Fastjson进阶使用]]></title>
    <url>%2F2019%2F06%2F10%2F%E5%B7%A5%E5%85%B7%E7%B1%BB%F0%9F%94%A7-Fastjson%E9%AB%98%E7%BA%A7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[fastjson用于将Java Bean序列化为JSON字符串，也可以从JSON字符串反序列化到JavaBean。主要就以下几点做个介绍 SerializerFeature特性的使用 JSONField与JSONType注解的使用 SerializeFilter 泛型反序列化 fastjson各种概念 简单使用 通过maven引入相应的json包 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.58&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 定义一个需要转换所实体类User，代码如下： 12345678910111213141516171819package com.ohaotian.feifz.style.study.utils.fastjson;/** * @author feifz * @version 1.0.0 * @Description 研究fastjson用到的对象 * @createTime 2019年06月10日 16:36:00 */import com.alibaba.fastjson.annotation.JSONField;import lombok.Data;import java.util.Date;@Datapublic class User &#123; private Long id; private String name; @JSONField(format = "yyyy-MM-dd HH:mm:ss") private Date createTime;&#125; 写个简单的测试类用于测试fastjson的序列化与反序列化，代码如下： 123456789101112131415161718192021222324252627282930313233343536package com.ohaotian.feifz.style.study.utils.fastjson;/** * @author feifz * @version 1.0.0 * @Description fastjson简单使用 * @createTime 2019年06月10日 16:37:00 */import com.alibaba.fastjson.JSON;import java.util.Date;public class SimpleTest &#123; public static void main(String[] args) &#123; serialize(); deserialize(); &#125; public static void serialize() &#123; User user = new User(); user.setId(11L); user.setName("西安"); user.setCreateTime(new Date()); String jsonString = JSON.toJSONString(user, true); System.out.println(jsonString); &#125; public static void deserialize() &#123; String jsonString = "&#123;\"createTime\":\"2018-08-17 14:38:38\",\"id\":11,\"name\":\"西安\"&#125;"; User user = JSON.parseObject(jsonString, User.class); System.out.println(user.getName()); System.out.println(user.getCreateTime()); &#125;&#125; SerializerFeature特性的使用 fastjson通过SerializerFeature对生成的json格式的数据进行一些定制，比如可以输入的格式更好看，使用单引号而非双引号等。例子程序如下：123456789101112131415161718192021222324252627package com.ohaotian.feifz.style.study.utils.fastjson;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.serializer.SerializerFeature;import java.util.Date;/** * @author feifz * @version 1.0.0 * @Description SerializerFeature特性的使用 * @createTime 2019年06月10日 16:40:00 */public class SerializerFeatureTest &#123; public static void main(String[] args) &#123; User user = new User(); user.setId(11L); user.setCreateTime(new Date()); String jsonString = JSON.toJSONString(user, SerializerFeature.PrettyFormat, SerializerFeature.WriteNullStringAsEmpty, SerializerFeature.UseSingleQuotes); System.out.println(jsonString); &#125;&#125; 输出的结果如下：12345&#123; 'createTime':'2019-06-10 17:25:34', 'id':11, 'name':''&#125; SerializerFeature常用属性 名称 含义 QuoteFieldNames 输出key时是否使用双引号,默认为true UseSingleQuotes 使用单引号而不是双引号,默认为false WriteMapNullValue 是否输出值为null的字段,默认为false WriteEnumUsingToString Enum输出name()或者original,默认为false UseISO8601DateFormat Date使用ISO8601格式输出，默认为false WriteNullListAsEmpty List字段如果为null,输出为[],而非null WriteNullStringAsEmpty 字符类型字段如果为null,输出为”“,而非null WriteNullNumberAsZero 数值字段如果为null,输出为0,而非null WriteNullBooleanAsFalse Boolean字段如果为null,输出为false,而非null SkipTransientField 如果是true，类中的Get方法对应的Field是transient，序列化时将会被忽略。默认为true SortField 按字段名称排序后输出。默认为false WriteTabAsSpecial 把\t做转义输出，默认为false不推荐设为true PrettyFormat 结果是否格式化,默认为false WriteClassName 序列化时写入类型信息，默认为false。反序列化是需用到 DisableCircularReferenceDetect 消除对同一对象循环引用的问题，默认为false WriteSlashAsSpecial 对斜杠’/’进行转义 BrowserCompatible 将中文都会序列化为\uXXXX格式，字节数会多一些，但是能兼容IE 6，默认为false WriteDateUseDateFormat 全局修改日期格式,默认为false。 DisableCheckSpecialChar 一个对象的字符串属性中如果有特殊字符如双引号，将会在转成json时带有反斜杠转移符。如果不需要转义，可以使用这个属性。默认为false BeanToArray 将对象转为array输出 JSONField与JSONType注解的使用 注意和@JSONField不同的是,@JSONType是配置在类上的，而@JSONField是配置在字段和方法上的。 JSONField注解的使用 fastjson提供了JSONField对序列化与反序列化进行定制，比如可以指定字段的名称，序列化的顺序。JSONField用于属性，方法方法参数上。JSONField的源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940package com.alibaba.fastjson.annotation;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import com.alibaba.fastjson.parser.Feature;import com.alibaba.fastjson.serializer.SerializerFeature;@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.METHOD, ElementType.FIELD, ElementType.PARAMETER &#125;)public @interface JSONField &#123;// 配置序列化和反序列化的顺序 int ordinal() default 0;// 指定字段的名称 String name() default "";// 指定字段的格式，对日期格式有用 String format() default ""; // 是否序列化 boolean serialize() default true;// 是否反序列化 boolean deserialize() default true;//字段级别的SerializerFeature SerializerFeature[] serialzeFeatures() default &#123;&#125;;// Feature[] parseFeatures() default &#123;&#125;; //给属性打上标签， 相当于给属性进行了分组 String label() default ""; boolean jsonDirect() default false; //制定属性的序列化类 Class&lt;?&gt; serializeUsing() default Void.class; //制定属性的反序列化类 Class&lt;?&gt; deserializeUsing() default Void.class; String[] alternateNames() default &#123;&#125;; boolean unwrapped() default false;&#125; 属性name通过在注解@JSONField指定属性name可以指定该属性在序列化和反序列化过程中输出的key。可作用于Field上，也可作用于setter和setter方法上。一、作用Field （相当于get set的总和）@JSONField作用在Field时，其name不仅定义了输入key的名称，同时也定义了输出的名称。二、作用在setter和getter方法上顾名思义，当作用在setter方法上时，就相当于根据name到json中寻找对应的值，并调用该setter对象赋值。当作用在getter上时，在bean转换为json时，其key值为name定义的值。示例如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.ohaotian.feifz.style.study.utils.fastjson;/** * @author feifz * @version 1.0.0 * @Description JSONField注解 name属性用法 * @createTime 2019年06月11日 14:00:00 */import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.annotation.JSONField;public class JsonFieldNameDemo &#123; @JSONField(name = "jsonName") private String name; private String age; private String email; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @JSONField(name = "jsonAge") public String getAge() &#123; return age; &#125; @JSONField(name = "jsonAge") public void setAge(String age) &#123; this.age = age; &#125; public String getEmail() &#123; return email; &#125; @JSONField(name = "jsonEmail") public void setEmail(String email) &#123; this.email = email; &#125; @Override public String toString() &#123; return "JsonFieldNameDemo [name=" + name + ", age=" + age + ", email=" + email + "]"; &#125; public static void main(String[] args) &#123; JsonFieldNameDemo jsonFieldNameDemo = new JsonFieldNameDemo(); jsonFieldNameDemo.setAge("21岁"); jsonFieldNameDemo.setEmail("111@11.com"); jsonFieldNameDemo.setName("hhh"); System.out.println(JSON.toJSONString(jsonFieldNameDemo)); //输出了：&#123;"jsonName":"hhh","jsonAge":"21岁","email":"111@11.com"&#125; String json = "&#123;\"email\":\"111@11.com\",\"jsonAge\":\"21岁\",\"jsonName\":\"hhh\"&#125;"; JsonFieldNameDemo jsonTest = JSON.parseObject(json, JsonFieldNameDemo.class); System.out.println(jsonTest.toString()); &#125;&#125; 输出结果如下：12&#123;"email":"111@11.com","jsonAge":"21岁","jsonName":"hhh"&#125;JsonFieldNameDemo [name=hhh, age=21岁, email=null] serialize，deserializeserialize：是否序列化，默认值都为true，指定为false时序列化过程中不序列化该字段。deserialize：是否反序列化，默认值都为true，指定为false时反序列化过程中不反序列化该字段示例代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.ohaotian.feifz.style.study.utils.fastjson;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.annotation.JSONField;/** * @author feifz * @version 1.0.0 * @Description @JSONField name属性示例 * @createTime 2019年06月11日 14:09:00 */public class JsonFieldNameDemo &#123; private String name; @JSONField(deserialize = false) private String age; @JSONField(serialize = false) private String email; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAge() &#123; return age; &#125; public void setAge(String age) &#123; this.age = age; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; @Override public String toString() &#123; return "JsonFieldNameDemo&#123;" + "name='" + name + '\'' + ", age='" + age + '\'' + ", email='" + email + '\'' + '&#125;'; &#125; public static void main(String[] args) &#123; JsonFieldNameDemo jsonFieldNameTest1 = new JsonFieldNameDemo(); jsonFieldNameTest1.setAge("18"); jsonFieldNameTest1.setEmail("123@163.com"); jsonFieldNameTest1.setName("Bob"); System.out.println(JSON.toJSONString(jsonFieldNameTest1)); String json = "&#123;\"age\":\"18\",\"email\":\"123@163.com\",\"name\":\"Bob\"&#125;"; JsonFieldNameDemo jsonFieldNameTest2 = JSON.parseObject(json, JsonFieldNameDemo.class); System.out.println(jsonFieldNameTest2.toString()); &#125;&#125; 输出结果如下：12&#123;"age":"18","name":"Bob"&#125;JsonFieldNameDemo&#123;name='Bob', age='null', email='123@163.com'&#125; serializeUsing，deserializeUsing其中serializeUsing与deserializeUsing可以用于对字段的序列化与反序列化进行定制化。比如我们在User实体上加上个sex属性，类型为boolean。下面分别定义了序列化类与反序列化类。序列化类代码如下： 123456789101112131415161718192021222324252627282930313233package com.ohaotian.feifz.style.study.utils.fastjson;/** * @author feifz * @version 1.0.0 * @Description TODO * @createTime 2019年06月11日 10:23:00 */import com.alibaba.fastjson.serializer.JSONSerializer;import com.alibaba.fastjson.serializer.ObjectSerializer;import java.io.IOException;import java.lang.reflect.Type;public class SexSerializer implements ObjectSerializer &#123; @Override public void write(JSONSerializer serializer, Object object, Object fieldName, Type fieldType, int features) throws IOException &#123; Boolean value = (Boolean) object; String text = "女"; if (value != null &amp;&amp; value == true) &#123; text = "男"; &#125; serializer.write(text); &#125;&#125; 反序列化类代码如下：12345678910111213141516171819202122232425262728293031323334353637package com.ohaotian.feifz.style.study.utils.fastjson;/** * @author feifz * @version 1.0.0 * @Description TODO * @createTime 2019年06月11日 10:24:00 */import com.alibaba.fastjson.parser.DefaultJSONParser;import com.alibaba.fastjson.parser.JSONToken;import com.alibaba.fastjson.parser.deserializer.ObjectDeserializer;import java.lang.reflect.Type;public class SexDeserialize implements ObjectDeserializer &#123; @Override public &lt;T&gt; T deserialze(DefaultJSONParser parser, Type type, Object fieldName) &#123; String sex = parser.parseObject(String.class); if ("男".equals(sex)) &#123; return (T) Boolean.TRUE; &#125; else &#123; return (T) Boolean.FALSE; &#125; &#125; @Override public int getFastMatchToken() &#123; return JSONToken.UNDEFINED; &#125;&#125; 在定义bean对象时在字段sex上加上注解如下：12345678910111213141516171819202122232425262728package com.ohaotian.feifz.style.study.utils.fastjson;/** * @author feifz * @version 1.0.0 * @Description 研究fastjson用到的对象 * @createTime 2019年06月10日 16:36:00 */import com.alibaba.fastjson.annotation.JSONField;import lombok.Data;import java.util.Date;@Datapublic class User &#123; private Long id; private String name; @JSONField(format = "yyyy-MM-dd HH:mm:ss") private Date createTime; @JSONField(serializeUsing = SexSerializer.class, deserializeUsing = SexDeserialize.class) private Boolean sex;&#125; 这样在执行下面代码之后就会发现对sex字段的值定制化了1234567891011121314151617181920212223242526272829303132333435363738package com.ohaotian.feifz.style.study.utils.fastjson;/** * @author feifz * @version 1.0.0 * @Description fastjson简单使用 * @createTime 2019年06月10日 16:37:00 */import com.alibaba.fastjson.JSON;import java.util.Date;public class SimpleTest &#123; public static void main(String[] args) &#123; serialize(); deserialize(); &#125; public static void serialize() &#123; User user = new User(); user.setId(11L); user.setName("西安"); user.setSex(false); user.setCreateTime(new Date()); String jsonString = JSON.toJSONString(user, true); System.out.println(jsonString); &#125; public static void deserialize() &#123; String jsonString = "&#123;\"createTime\":\"2018-08-17 14:38:38\",\"id\":11,\"name\":\"西安\",\"sex\":\"男\"&#125;"; User user = JSON.parseObject(jsonString, User.class); System.out.println(user.getSex()); System.out.println(user.getName()); System.out.println(user.getCreateTime()); &#125;&#125; 输出的结果如下：123456789&#123; "createTime":"2019-06-11 11:20:08", "id":11, "name":"西安", "sex":"女"&#125;true西安Fri Aug 17 14:38:38 CST 2018 JSONType注解的使用 fastjosn提供了JSONType用于类级别的定制化, JSONType的源码如下： 源码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.alibaba.fastjson.annotation;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import com.alibaba.fastjson.PropertyNamingStrategy;import com.alibaba.fastjson.parser.Feature;import com.alibaba.fastjson.serializer.SerializeFilter;import com.alibaba.fastjson.serializer.SerializerFeature;@Retention(RetentionPolicy.RUNTIME)//需要标注在类上@Target(&#123; ElementType.TYPE &#125;)public @interface JSONType &#123; boolean asm() default true;//这里可以定义输出json的字段顺序 String[] orders() default &#123;&#125;;//包含的字段 String[] includes() default &#123;&#125;;//不包含的字段 String[] ignores() default &#123;&#125;;//类级别的序列化特性定义 SerializerFeature[] serialzeFeatures() default &#123;&#125;; Feature[] parseFeatures() default &#123;&#125;; //按字母顺序进行输出 boolean alphabetic() default true; Class&lt;?&gt; mappingTo() default Void.class; Class&lt;?&gt; builder() default Void.class; String typeName() default ""; String typeKey() default ""; Class&lt;?&gt;[] seeAlso() default&#123;&#125;; //序列化类 Class&lt;?&gt; serializer() default Void.class; //反序列化类 Class&lt;?&gt; deserializer() default Void.class; boolean serializeEnumAsJavaBean() default false; PropertyNamingStrategy naming() default PropertyNamingStrategy.CamelCase; Class&lt;? extends SerializeFilter&gt;[] serialzeFilters() default &#123;&#125;;&#125; 属性 includes序列化时，只序列化这些字段使用示例：@JSONType(includes = {“name”,”age”}) ignores序列化时，忽略这些字段使用示例：@JSONType(ignores = {“email”}) orders序列化时，按照指定的顺序输出结果使用示例：@JSONType(orders = {“name”,”age”}) 使用示例：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.ohaotian.feifz.style.study.utils.fastjson;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.annotation.JSONType;/** * @author feifz * @version 1.0.0 * @Description @JSONType 注解使用示例 * @createTime 2019年06月11日 14:41:00 */@JSONType(includes = &#123;"name","age"&#125;,ignores = &#123;"email"&#125;,orders = &#123;"name","age"&#125;)public class JsonTypeDemo &#123; private String name; private String age; private String email; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAge() &#123; return age; &#125; public void setAge(String age) &#123; this.age = age; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public JsonTypeDemo(String name, String age, String email) &#123; this.name = name; this.age = age; this.email = email; &#125; @Override public String toString() &#123; return "JsonTypeDemo&#123;" + "name='" + name + '\'' + ", age='" + age + '\'' + ", email='" + email + '\'' + '&#125;'; &#125; public static void main(String[] args) &#123; JsonTypeDemo demo = new JsonTypeDemo("Mary","17","123@163.com"); System.out.println(JSON.toJSONString(demo)); String json = "&#123;\"age\":\"17\",\"email\":\"123@163.com\",\"name\":\"Mary\"&#125;"; JsonTypeDemo jsonTypeDemo = JSON.parseObject(json,JsonTypeDemo.class); System.out.println(jsonTypeDemo.toString()); &#125;&#125; 输出结果如下：12&#123;"name":"Mary","age":"17"&#125;JsonTypeDemo&#123;name='Mary', age='17', email='123@163.com'&#125; SerializeFilter泛型反序列化12345678910111213141516171819202122232425262728293031323334353637383940414243444546```# fastjson各种概念* JSON：本身是Abstract，提供了一系统的工具方法方便用户使用的API。## 序列化相关的概念* SerializeConfig：内部是个map容器主要功能是配置并记录每种Java类型对应的序列化类。* SerializeWriter 继承自Java的Writer，其实就是个转为FastJSON而生的StringBuilder，完成高性能的字符串拼接。* SerializeFilter: 用于对对象的序列化实现各种定制化的需求。* SerializerFeature：对于对输出的json做各种格式化的需求。* JSONSerializer：相当于一个序列化组合器，集成了*SerializeConfig， SerializeWriter ， SerializeFilter与SerializerFeature。序列化的入口代码如下，上面提到的各种概念都包含了：``` java/** * @since 1.2.9 * @return */ public static String toJSONString(Object object, // SerializeConfig config, // SerializeFilter[] filters, // String dateFormat, // int defaultFeatures, // SerializerFeature... features) &#123; SerializeWriter out = new SerializeWriter(null, defaultFeatures, features); try &#123; JSONSerializer serializer = new JSONSerializer(out, config); if (dateFormat != null &amp;&amp; dateFormat.length() != 0) &#123; serializer.setDateFormat(dateFormat); serializer.config(SerializerFeature.WriteDateUseDateFormat, true); &#125; if (filters != null) &#123; for (SerializeFilter filter : filters) &#123; serializer.addFilter(filter); &#125; &#125; serializer.write(object); return out.toString(); &#125; finally &#123; out.close(); &#125; &#125; 反序列化相关的概念ParserConfig：内部通过一个map保存各种ObjectDeserializer。JSONLexer : 与SerializeWriter相对应，用于解析json字符串。JSONToken：定义了一系统的特殊字符，这些称为token。ParseProcess ：定制反序列化，类似于SerializeFilter。Feature：用于定制各种反序列化的特性。DefaultJSONParser：相当于反序列化组合器，集成了ParserConfig，Feature， JSONLexer 与ParseProcess。反序列化的入口代码如下，上面的概念基本都包含了：12345678910111213141516171819202122232425262728293031323334353637@SuppressWarnings("unchecked") public static &lt;T&gt; T parseObject(String input, Type clazz, ParserConfig config, ParseProcess processor, int featureValues, Feature... features) &#123; if (input == null) &#123; return null; &#125; if (features != null) &#123; for (Feature feature : features) &#123; featureValues |= feature.mask; &#125; &#125; DefaultJSONParser parser = new DefaultJSONParser(input, config, featureValues); if (processor != null) &#123; if (processor instanceof ExtraTypeProvider) &#123; parser.getExtraTypeProviders().add((ExtraTypeProvider) processor); &#125; if (processor instanceof ExtraProcessor) &#123; parser.getExtraProcessors().add((ExtraProcessor) processor); &#125; if (processor instanceof FieldTypeResolver) &#123; parser.setFieldTypeResolver((FieldTypeResolver) processor); &#125; &#125; T value = (T) parser.parseObject(clazz, null); parser.handleResovleTask(value); parser.close(); return (T) value; &#125; 参考 https://www.jianshu.com/p/eaeaa5dce258]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具类🔧-Fastjson入门使用]]></title>
    <url>%2F2019%2F06%2F10%2F%E5%B7%A5%E5%85%B7%E7%B1%BB%F0%9F%94%A7-Fastjson%2F</url>
    <content type="text"><![CDATA[简介什么是Fastjson?fastjson是阿里巴巴的开源JSON解析库，它可以解析JSON格式的字符串，支持将Java Bean序列化为JSON字符串，也可以从JSON字符串反序列化到JavaBean。Fastjson是一个Java语言编写的高性能功能完善的JSON库。它采用一种“假定有序快速匹配”的算法，把JSON Parse的性能提升到极致，是目前Java语言中最快的JSON库。Fastjson接口简单易用，已经被广泛使用在缓存序列化、协议交互、Web输出、Android客户端等多种应用场景。主要特点：快速FAST (比其它任何基于Java的解析器和生成器更快，包括jackson）强大（支持普通JDK类包括任意Java Bean Class、Collection、Map、Date或enum）零依赖（没有依赖其它任何类库除了JDK）开源，使用Apache License 2.0协议开源。源码：https://github.com/alibaba/fastjsonwiki：https://github.com/alibaba/fastjson/wiki/Quick-Start-CN Fastjson使用场景fastjson已经被广泛使用在各种场景，包括cache存储、RPC通讯、MQ通讯、网络协议通讯、Android客户端、Ajax服务器处理程序等等。 Fastjson的优点 速度快fastjson相对其他JSON库的特点是快，从2011年fastjson发布1.1.x版本之后，其性能从未被其他Java实现的JSON库超越。 使用广泛fastjson在阿里巴巴大规模使用，在数万台服务器上部署，fastjson在业界被广泛接受。在2012年被开源中国评选为最受欢迎的国产开源软件之一。 测试完备fastjson有非常多的testcase，在1.2.11版本中，testcase超过3321个。每次发布都会进行回归测试，保证质量稳定。 使用简单fastjson的API十分简洁。 12String text = JSON.toJSONString(obj); //序列化Model vo = JSON.parseObject("&#123;...&#125;", Mpode.class); //反序列化 功能完备支持泛型，支持流处理超大文本，支持枚举，支持序列化和反序列化扩展。 下载和使用下载你可以在maven中央仓库中直接下载：1http://repo1.maven.org/maven2/com/alibaba/fastjson/ 或者配置maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt; 其中x.x.x是版本号，跟进需要使用特定版本，建议使用最新版本,目前最新版本为1.2.58 简单使用 JSON这个类是fastjson API的入口，主要的功能都通过这个类提供。序列化API 123456789101112131415package com.alibaba.fastjson;public abstract class JSON &#123; // 将Java对象序列化为JSON字符串，支持各种各种Java基本类型和JavaBean public static String toJSONString(Object object, SerializerFeature... features); // 将Java对象序列化为JSON字符串，返回JSON字符串的utf-8 bytes public static byte[] toJSONBytes(Object object, SerializerFeature... features); // 将Java对象序列化为JSON字符串，写入到Writer中 public static void writeJSONString(Writer writer, Object object, SerializerFeature... features); // 将Java对象序列化为JSON字符串，按UTF-8编码写入到OutputStream中 public static final int writeJSONString(OutputStream os, // Object object, // SerializerFeature... features);&#125; JSON字符串反序列化API 1234567891011121314151617package com.alibaba.fastjson;public abstract class JSON &#123; // 将JSON字符串反序列化为JavaBean public static &lt;T&gt; T parseObject(String jsonStr, Class&lt;T&gt; clazz, Feature... features); // 将JSON字符串反序列化为JavaBean public static &lt;T&gt; T parseObject(byte[] jsonBytes, // UTF-8格式的JSON字符串 Class&lt;T&gt; clazz, Feature... features); // 将JSON字符串反序列化为泛型类型的JavaBean public static &lt;T&gt; T parseObject(String text, TypeReference&lt;T&gt; type, Feature... features); // 将JSON字符串反序列为JSONObject public static JSONObject parseObject(String text);&#125; 简单示例parse Tree12import com.alibaba.fastjson.*;JSONObject jsonObj = JSON.parseObject(jsonStr); parse POJO12import com.alibaba.fastjson.JSON;Model model = JSON.parseObject(jsonStr, Model.class); parse POJO Generic123import com.alibaba.fastjson.JSON;Type type = new TypeReference&lt;List&lt;Model&gt;&gt;() &#123;&#125;.getType(); List&lt;Model&gt; list = JSON.parseObject(jsonStr, type); convert POJO to json string123import com.alibaba.fastjson.JSON;Model model = ...; String jsonStr = JSON.toJSONString(model); convert POJO to json bytes123import com.alibaba.fastjson.JSON;Model model = ...; byte[] jsonBytes = JSON.toJSONBytes(model); write POJO as json string to OutputStream1234import com.alibaba.fastjson.JSON;Model model = ...; OutputStream os;JSON.writeJSONString(os, model); write POJO as json string to Writer1234import com.alibaba.fastjson.JSON;Model model = ...; Writer writer = ...;JSON.writeJSONString(writer, model); 高级使用Fastjson 定制序列化 简介 fastjson支持多种方式定制序列化。通过@JSONField定制序列化通过@JSONType定制序列化通过SerializeFilter定制序列化通过ParseProcess定制反序列化 使用@JSONField配置 可以把@JSONField配置在字段或者getter/setter方法上。例如：1234public class VO &#123; @JSONField(name="ID") private int id; &#125; 或者123456789public class VO &#123; private int id; @JSONField(name="ID") public int getId() &#123; return id;&#125; @JSONField(name="ID") public void setId(int value) &#123;this.id = id;&#125;&#125; 使用@JSONType配置和JSONField类似，但JSONType配置在类上，而不是field或者getter/setter方法上。 通过SerializeFilter定制序列化通过SerializeFilter可以使用扩展编程的方式实现定制序列化。fastjson提供了多种SerializeFilter： PropertyPreFilter 根据PropertyName判断是否序列化 PropertyFilter 根据PropertyName和PropertyValue来判断是否序列化 NameFilter 修改Key，如果需要修改Key,process返回值则可 ValueFilter 修改Value BeforeFilter 序列化时在最前添加内容*AfterFilter 序列化时在最后添加内容以上的SerializeFilter在JSON.toJSONString中可以使用。12SerializeFilter filter = ...; // 可以是上面5个SerializeFilter的任意一种。JSON.toJSONString(obj, filter); 通过ParseProcess定制反序列化 Fastjson 实例Fastjson 对象或数组转JSONFastjson阿里巴巴工程师开源的一个 json 库：Fastjson，这个库在解析速度和易用性上来说都很不错。在日志解析，前后端数据传输交互中，经常会遇到String与map、json、xml等格式相互转换与解析的场景，其中json基本成为了跨语言、跨前后端的事实上的标准数据交互格式。应该来说各个语言中解析json的库都一大片（具体 json 格式与三方库的介绍请见：http://www.json.org/json-zh.html ），比如python都集成在了内置库中，成为标准API，今天我们要聊的是java中如何方便的使用json格式。下面一个示例是如何使用Fastjson 把对象或数组转JSON1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.ohaotian.feifz.style.study.utils;/** * @author feifz * @version 1.0.0 * @Description alibaba fastjson工具类 * @createTime 2019年06月10日 11:45:00 */import com.alibaba.fastjson.JSON;import lombok.Data;import java.util.ArrayList;import java.util.List;class FastJsonTest &#123; public static void main(String[] args) &#123; // 构建用户geust User guestUser = new User(); guestUser.setName("guest"); guestUser.setAge(28); // 构建用户root User rootUser = new User(); rootUser.setName("root"); guestUser.setAge(35); // 构建用户组对象 UserGroup group = new UserGroup(); group.setName("admin"); group.getUsers().add(guestUser); group.getUsers().add(rootUser); // 用户组对象转JSON串 String jsonString = JSON.toJSONString(group); System.out.println("jsonString:" + jsonString); // JSON串转用户组对象 UserGroup group2 = JSON.parseObject(jsonString, UserGroup.class); System.out.println("group2:" + group2); // 构建用户对象数组 User[] users = new User[2]; users[0] = guestUser; users[1] = rootUser; // 用户对象数组转JSON串 String jsonString2 = JSON.toJSONString(users); System.out.println("jsonString2:" + jsonString2); // JSON串转用户对象列表 List&lt;User&gt; users2 = JSON.parseArray(jsonString2, User.class); System.out.println("users2:" + users2); &#125;&#125;@Dataclass User &#123; private String name; private int age;&#125;@Dataclass UserGroup &#123; private String name; private List&lt;User&gt; users = new ArrayList&lt;&gt;();&#125; 输出结果：1234jsonString:&#123;"name":"admin","users":[&#123;"age":35,"name":"guest"&#125;,&#123;"age":0,"name":"root"&#125;]&#125; group2:UserGroup [name=admin, users=[User [name=guest, age=35], User [name=root, age=0]]] jsonString2:[&#123;"age":35,"name":"guest"&#125;,&#123;"age":0,"name":"root"&#125;] users2:[User [name=guest, age=35], User [name=root, age=0]] fastjson通过各方面测试都很好，功能性能都是No.1，喜欢，它的源代码质量很高，作者也煞费苦心，将性能做到了最好，全面超越其他的json类库。通过fastjson我们可以快速进行开发。 Fastjson Obejct/Map/JSON/String 互转fastjson主要的使用入口Fastjson API入口类是com.alibaba.fastjson.JSON，常用的序列化操作都可以在JSON类上的静态方法直接完成。12345678public static final Object parse(String text); // 把JSON文本parse为JSONObject或者JSONArray public static final JSONObject parseObject(String text)； // 把JSON文本parse成JSONObject public static final &lt;T&gt; T parseObject(String text, Class&lt;T&gt; clazz); // 把JSON文本parse为JavaBean public static final JSONArray parseArray(String text); // 把JSON文本parse成JSONArray public static final &lt;T&gt; List&lt;T&gt; parseArray(String text, Class&lt;T&gt; clazz); //把JSON文本parse成JavaBean集合 public static final String toJSONString(Object object); // 将JavaBean序列化为JSON文本 public static final String toJSONString(Object object, boolean prettyFormat); // 将JavaBean序列化为带格式的JSON文本 public static final Object toJSON(Object javaObject); 将JavaBean转换为JSONObject或者JSONArray。 有关类库的一些说明1234SerializeWriter：相当于StringBufferJSONArray：相当于List&lt;Object&gt;JSONObject：相当于Map&lt;String, Object&gt;JSON反序列化没有真正数组，本质类型都是List&lt;Object&gt; 示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134package com.ohaotian.feifz.style.study.utils;/** * @author feifz * @version 1.0.0 * @Description fastjson 高级应用 * @createTime 2019年06月10日 15:43:00 */import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.serializer.SerializeConfig;import com.alibaba.fastjson.serializer.SimpleDateFormatSerializer;import lombok.Data;import java.util.ArrayList;import java.util.Date;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Random;/** * @author feifz * @version 1.0.0 * @Description alibaba fastjson高级应用 * @createTime 2019年06月10日 11:45:00 */public class FastjsonTest &#123; private static SerializeConfig mapping = new SerializeConfig(); static &#123; mapping.put(Date.class, new SimpleDateFormatSerializer("yyyy-MM-dd HH:mm:ss")); &#125; public static void main(String[] args) &#123; Date date = new Date(); String text = JSON.toJSONString(date, mapping); System.out.println(text); &#125; public static void json2List() &#123; /**List -&gt; JSON array*/ List&lt;Bar&gt; barList = new ArrayList&lt;Bar&gt;(); barList.add(new Bar()); barList.add(new Bar()); barList.add(new Bar()); String json = JSON.toJSONString(barList); System.out.println(json); json = JSON.toJSONString(barList, true); System.out.println(json); /**JSON array -&gt; List*/ List&lt;Bar&gt; barList1 = JSON.parseArray(json, Bar.class); for (Bar bar : barList1) &#123; System.out.println(bar.toString()); &#125; &#125; public static void json2Map() &#123; //Map -&gt; JSON Map&lt;String, Bar&gt; map = new HashMap&lt;String, Bar&gt;(); map.put("a", new Bar()); map.put("b", new Bar()); map.put("c", new Bar()); String json = JSON.toJSONString(map, true); System.out.println(json); //JSON -&gt; Map Map&lt;String, Bar&gt; map1 = (Map&lt;String, Bar&gt;) JSON.parse(json); for (String key : map1.keySet()) &#123; System.out.println(key + ":" + map1.get(key)); &#125; &#125; public static void array2JSON() &#123; String[] arr_String = &#123;"a", "b", "c"&#125;; String json_arr_String = JSON.toJSONString(arr_String, true); System.out.println(json_arr_String); JSONArray jsonArray = JSON.parseArray(json_arr_String); for (Object o : jsonArray) &#123; System.out.println(o); &#125; System.out.println(jsonArray); &#125; public static void array2JSON2() &#123; Bar[] arr_Bar = &#123;new Bar(), new Bar(), new Bar()&#125;; String json_arr_Bar = JSON.toJSONString(arr_Bar, true); System.out.println(json_arr_Bar); JSONArray jsonArray = JSON.parseArray(json_arr_Bar); for (Object o : jsonArray) &#123; System.out.println(o); &#125; System.out.println(jsonArray); &#125; public static void map2JSON() &#123; Map map = new HashMap(); map.put("a", "aaa"); map.put("b", "bbb"); map.put("c", "ccc"); String json = JSON.toJSONString(map); System.out.println(json); Map map1 = JSON.parseObject(json); for (Object o : map.entrySet()) &#123; Map.Entry&lt;String, String&gt; entry = (Map.Entry&lt;String, String&gt;) o; System.out.println(entry.getKey() + "---&gt;" + entry.getValue()); &#125; &#125;&#125;@Dataclass Bar &#123; public static SerializeConfig mapping = new SerializeConfig(); private String barName; private int barAge; private Date barDate = new Date(); static &#123; mapping.put(Date.class, new SimpleDateFormatSerializer("yyyy-MM-dd")); &#125; &#123; Random r = new Random(); barName = "sss_" + String.valueOf(r.nextFloat()); barAge = r.nextInt(); &#125; public static void main(String[] args) &#123; String x1 = JSON.toJSONString(new Bar(), true); System.out.println(x1); String x2 = JSON.toJSONString(new Bar(), mapping); System.out.println(x2); &#125;&#125; Fastjson API Fastjson JSONField Fastjson JSONPath Fastjson toJSONString Fastjson writeJSONString Fastjson parseObject Fastjson Api Compare Fastjson API Stream Fastjson API ParseProcess Fastjson API SerializeFilter Fastjson 常见问题参考 https://www.w3cschool.cn/fastjson https://www.jianshu.com/p/eaeaa5dce258]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http请求-okhttp3]]></title>
    <url>%2F2019%2F06%2F06%2FHttp%E8%AF%B7%E6%B1%82-okhttp3%2F</url>
    <content type="text"><![CDATA[简介 HTTP是现代应用常用的一种交换数据和媒体的网络方式，高效地使用HTTP能让资源加载更快，节省带宽。OkHttp是一个高效的HTTP客户端，它有以下默认特性： 支持HTTP/2，允许所有同一个主机地址的请求共享同一个socket连接 连接池减少请求延时 透明的GZIP压缩减少响应数据的大小 缓存响应内容，避免一些完全重复的请求 源码：https://github.com/square/okhttp说明：OkHttp支持Android 2.3及以上版本Android平台,对于Java, JDK1.7及以上。 当网络出现问题的时候OkHttp依然坚守自己的职责，它会自动恢复一般的连接问题，如果你的服务有多个IP地址，当第一个IP请求失败时，OkHttp会交替尝试你配置的其他IP，OkHttp使用现代TLS技术(SNI, ALPN)初始化新的连接，当握手失败时会回退到TLS 1.0。 简单使用引入maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;4.0.0-RC1&lt;/version&gt;&lt;/dependency&gt; 请求方法同步请求 就是执行请求的操作是阻塞式，直到 HTTP 响应返回。它对应 OKHTTP 中的 execute 方法。 GET请求1234567891011121314151617181920/** * 同步get方式请求 * * @param url * @return * @throws IOException */public static String doGet(String url) throws IOException &#123; OkHttpClient client = new OkHttpClient(); Request request = new Request.Builder() .url(url) .build(); try (Response response = client.newCall(request).execute()) &#123; if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125; &#125;&#125; POST请求Json提交参数123456789101112131415161718192021222324/** * 同步post方式请求-json提交参数 * * @param url * @param json * @return * @throws IOException */ public static String doPost(String url, final String json) throws IOException &#123; OkHttpClient client = new OkHttpClient(); RequestBody body = RequestBody.create(JSON, json); Request request = new Request.Builder() .url(url) .post(body) .build(); try (Response response = client.newCall(request).execute()) &#123; if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125; &#125; &#125; form表单提交参数12345678910111213141516171819202122232425262728/** * 同步post方式请求-form表单提交参数 * * @param url * @param paramsMap * @return * @throws IOException */public static String doPost(String url, Map&lt;String, String&gt; paramsMap) throws IOException &#123; OkHttpClient client = new OkHttpClient(); FormBody.Builder builder = new FormBody.Builder(); for (String key : paramsMap.keySet()) &#123; builder.add(key, paramsMap.get(key)); &#125; RequestBody formBody = builder.build(); Request request = new Request.Builder() .url(url) .post(formBody) .build(); try (Response response = client.newCall(request).execute()) &#123; if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125; &#125;&#125; 异步请求 就是类似于非阻塞式的请求，它的执行结果一般都是通过接口回调的方式告知调用者。它对应 OKHTTP 中的 enqueue 方法。 这是异步请求，所以调用enqueue则无需再开启子线程，enqueue方法会自动将网络请求部分放入子线程中执行。enqueue回调方法onResponse与onFailure都执行在子线程中。注意事项： 回调接口的onFailure方法和onResponse执行在子线程。 Response.code是http响应行中的code，如果访问成功则返回200.这个不是服务器设置的，而是http协议中自带的。res中的code才是服务器设置的。注意二者的区别。 response.body().string()本质是输入流的读操作，所以它还是网络请求的一部分，所以这行代码必须放在子线程。 response.body().string()只能调用一次，在第一次时有返回值，第二次再调用时将会返回null。原因是：response.body().string()的本质是输入流的读操作，必须有服务器的输出流的写操作时客户端的读操作才能得到数据。而服务器的写操作只执行一次，所以客户端的读操作也只能执行一次，第二次将返回null。 再次强调，response.body().string()方法必须放在子线程中。当执行这行代码得到结果后，再跳转到UI线程修改UI。 异步请求自定义回调函数123456789101112131415161718/** * okhttp 异步调用回调函数 */ static class OkHttpCallback implements Callback &#123; @Override public void onFailure(@NotNull Call call, @NotNull IOException e) &#123; log.error(e); &#125; @Override public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException &#123; if (response.isSuccessful()) &#123; log.info("Successful data acquisition . . . "); log.info("response.code()==" + response.code()); log.info("response.body().string()==" + response.body().string()); &#125; &#125; &#125; GET请求12345678910111213141516/** * 异步get方式请求 * * @param url * @return * @throws IOException */public static void doSyncGet(String url) &#123; OkHttpClient okHttpClient = new OkHttpClient(); final Request request = new Request.Builder() .url(url) .get() .build(); Call call = okHttpClient.newCall(request); call.enqueue(new OkHttpCallback());&#125; POST请求Json提交参数123456789101112131415161718/** * 异步post方式请求-json提交参数 * * @param url * @param json * @return * @throws IOException */public static void doSyncPost(String url, final String json) &#123; OkHttpClient client = new OkHttpClient(); RequestBody body = RequestBody.create(JSON, json); Request request = new Request.Builder() .url(url) .post(body) .build(); client.newCall(request).enqueue(new OkHttpCallback());&#125; form表单提交参数12345678910111213141516171819202122/** * 异步post方式请求-form表单提交参数 * * @param url * @param paramsMap * @return * @throws IOException */ public static void doSyncPost(String url, Map&lt;String, String&gt; paramsMap) &#123; OkHttpClient client = new OkHttpClient(); FormBody.Builder builder = new FormBody.Builder(); for (String key : paramsMap.keySet()) &#123; builder.add(key, paramsMap.get(key)); &#125; RequestBody formBody = builder.build(); Request request = new Request.Builder() .url(url) .post(formBody) .build(); client.newCall(request).enqueue(new OkHttpCallback()); &#125; 参考 https://www.jianshu.com/p/da4a806e599b https://blog.csdn.net/m0_38143863/article/details/80220247 示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183package com.ohaotian.feifz.style.study.utils;import lombok.extern.log4j.Log4j2;import okhttp3.Call;import okhttp3.Callback;import okhttp3.FormBody;import okhttp3.MediaType;import okhttp3.OkHttpClient;import okhttp3.Request;import okhttp3.RequestBody;import okhttp3.Response;import org.jetbrains.annotations.NotNull;import java.io.IOException;import java.util.Map;/** * @author feifz * @version 1.0.0 * @Description http工具类，基于okhttp3 * @createTime 2019年06月06日 09:30:00 */@Log4j2public class HttpUtil &#123; public static final MediaType JSON = MediaType.get("application/json; charset=utf-8"); /** * 同步get方式请求 * * @param url * @return * @throws IOException */ public static String doGet(String url) throws IOException &#123; OkHttpClient client = new OkHttpClient(); Request request = new Request.Builder() .url(url) .build(); try (Response response = client.newCall(request).execute()) &#123; if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125; &#125; &#125; /** * 异步get方式请求 * * @param url * @return * @throws IOException */ public static void doSyncGet(String url) &#123; OkHttpClient okHttpClient = new OkHttpClient(); final Request request = new Request.Builder() .url(url) .get() .build(); Call call = okHttpClient.newCall(request); call.enqueue(new OkHttpCallback()); &#125; /** * 同步post方式请求-json提交参数 * * @param url * @param json * @return * @throws IOException */ public static String doPost(String url, final String json) throws IOException &#123; OkHttpClient client = new OkHttpClient(); RequestBody body = RequestBody.create(JSON, json); Request request = new Request.Builder() .url(url) .post(body) .build(); try (Response response = client.newCall(request).execute()) &#123; if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125; &#125; &#125; /** * 异步post方式请求-json提交参数 * * @param url * @param json * @return * @throws IOException */ public static void doSyncPost(String url, final String json) &#123; OkHttpClient client = new OkHttpClient(); RequestBody body = RequestBody.create(JSON, json); Request request = new Request.Builder() .url(url) .post(body) .build(); client.newCall(request).enqueue(new OkHttpCallback()); &#125; /** * 同步post方式请求-form表单提交参数 * * @param url * @param paramsMap * @return * @throws IOException */ public static String doPost(String url, Map&lt;String, String&gt; paramsMap) throws IOException &#123; OkHttpClient client = new OkHttpClient(); FormBody.Builder builder = new FormBody.Builder(); for (String key : paramsMap.keySet()) &#123; builder.add(key, paramsMap.get(key)); &#125; RequestBody formBody = builder.build(); Request request = new Request.Builder() .url(url) .post(formBody) .build(); try (Response response = client.newCall(request).execute()) &#123; if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125; &#125; &#125; /** * 异步post方式请求-form表单提交参数 * * @param url * @param paramsMap * @return * @throws IOException */ public static void doSyncPost(String url, Map&lt;String, String&gt; paramsMap) &#123; OkHttpClient client = new OkHttpClient(); FormBody.Builder builder = new FormBody.Builder(); for (String key : paramsMap.keySet()) &#123; builder.add(key, paramsMap.get(key)); &#125; RequestBody formBody = builder.build(); Request request = new Request.Builder() .url(url) .post(formBody) .build(); client.newCall(request).enqueue(new OkHttpCallback()); &#125; /** * okhttp 异步调用回调函数 */ static class OkHttpCallback implements Callback &#123; @Override public void onFailure(@NotNull Call call, @NotNull IOException e) &#123; log.error(e); &#125; @Override public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException &#123; if (response.isSuccessful()) &#123; log.info("Successful data acquisition . . . "); log.info("response.code()==" + response.code()); log.info("response.body().string()==" + response.body().string()); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>okhttp3</tag>
        <tag>开源项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka生产者发送消息的三种方式]]></title>
    <url>%2F2019%2F05%2F31%2FKafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[介绍Kafka是一种分布式的基于发布/订阅的消息系统，它的高吞吐量、灵活的offset是其它消息系统所没有的。 三种方式Kafka发送消息主要有三种方式： 发送并忘记 同步发送 异步发送+回调函数 发送并忘记发送并忘记(不关心消息是否正常到达，对返回结果不做任何判断处理)发送并忘记的方式本质上也是一种异步的方式，只是它不会获取消息发送的返回结果，这种方式的吞吐量是最高的，但是无法保证消息的可靠性. 123456789101112131415161718public static void wayOne() &#123; String brokeList = "127.0.0.1:9092"; String topic = "testTopic"; String key = "testKey"; Producer&lt;String, String&gt; producer = initProducer(brokeList); long before = System.currentTimeMillis(); System.out.println("发送前--&gt;"+before); for(int i=1 ;i&lt;10000;i++)&#123; producer.send(new ProducerRecord&lt;&gt;(topic, 0,key, String.valueOf(i))); &#125; producer.flush(); producer.close(); long after = System.currentTimeMillis(); System.out.println("发送后--&gt;"+after); long temp = after-before; System.out.println("时间间隔--&gt;"+temp); &#125; 同步发送同步发送(通过get方法等待Kafka的响应，判断消息是否发送成功)以同步的方式发送消息时，一条一条的发送，对每条消息返回的结果判断， 可以明确地知道每条消息的发送情况，但是由于同步的方式会阻塞，只有当消息通过get返回future对象时，才会继续下一条消息的发送： 1234567891011121314151617181920212223public static void wayTwo() &#123; String brokeList = "39.96.117.232:9092"; String topic = "testTopic"; String key = "testKey"; Producer&lt;String, String&gt; producer = initProducer(brokeList); long before = System.currentTimeMillis(); System.out.println("发送前--&gt;"+before); for(int i=1 ;i&lt;10000;i++)&#123; Future&lt;RecordMetadata&gt; recordMetadataFuture = producer.send(new ProducerRecord&lt;&gt;(topic,key, String.valueOf(i))); try &#123; RecordMetadata record = recordMetadataFuture.get(10, TimeUnit.MICROSECONDS); System.out.println(record.toString()); &#125;catch (Exception e)&#123; System.out.println(e); &#125; &#125; long after = System.currentTimeMillis(); System.out.println("发送后--&gt;"+after); long temp = after-before; System.out.println("时间间隔--&gt;"+temp); &#125; 异步发送+回调函数异步发送+回调函数(消息以异步的方式发送，通过回调函数返回消息发送成功/失败)在调用send方法发送消息的同时，指定一个回调函数，服务器在返回响应时会调用该回调函数，通过回调函数能够对异常情况进行处理，当调用了回调函数时，只有回调函数执行完毕生产者才会结束，否则一直会阻塞。 123456789101112131415161718192021222324public static void wayThree() &#123; String brokeList = "39.96.117.232:9092"; String topic = "testTopic"; String key = "testKey"; Producer&lt;String, String&gt; producer = initProducer(brokeList); long before = System.currentTimeMillis(); System.out.println("发送前--&gt;"+before); for(int i=1 ;i&lt;10000;i++)&#123; producer.send(new ProducerRecord&lt;&gt;(topic, key, String.valueOf(i)), new Callback() &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if(e==null)&#123; System.out.println("## 发送消息成功-&gt;"); &#125;else &#123; System.out.println("## 发送消息失败-&gt;&#123;&#125;"+e.getMessage()); &#125; &#125; &#125;); &#125; long after = System.currentTimeMillis(); System.out.println("发送后--&gt;"+after); long temp = after-before; System.out.println("时间间隔--&gt;"+temp); &#125; 示例代码 基于java实现(待完善) 依赖jar包： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.12&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package com.ohaotian.feifz.mq.kafka;import org.apache.kafka.clients.producer.Callback;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import java.util.Properties;import java.util.concurrent.Future;import java.util.concurrent.TimeUnit;/** * @Author: feifz * @Date: 2019-05-31 15:34 * @Version: 1.0 * @Description: kafka 发送消息三种方式发送消息示例代码 * @Refer https://www.cnblogs.com/FG123/p/10091478.html */public class KafkaSendMsgDemo &#123; public static void main(String[] args) &#123; /** 三种方式虽然在时间上有所差别，但并不是说时间越快的越好，具体要看业务的应用场景： 场景1：如果业务要求消息必须是按顺序发送的，那么可以使用同步的方式，并且只能在一个partation上或指定同一个key，结合参数设置retries的值让发送失败时重试，设置max_in_flight_requests_per_connection=1，可以控制生产者在收到服务器晌应之前只能发送1个消息，从而控制消息顺序发送； 场景2：如果业务只关心消息的吞吐量，容许少量消息发送失败，也不关注消息的发送顺序，那么可以使用发送并忘记的方式，并配合参数acks=0，这样生产者不需要等待服务器的响应，以网络能支持的最大速度发送消息； 场景3：如果业务需要知道消息发送是否成功，并且对消息的顺序不关心，那么可以用异步+回调的方式来发送消息，配合参数retries=0，并将发送失败的消息记录到日志文件中； * */ wayThree(); &#125; /** * 发送并忘记(不关心消息是否正常到达，对返回结果不做任何判断处理) */ public static void wayOne() &#123; String brokeList = "127.0.0.1:9092"; String topic = "testTopic"; String key = "testKey"; Producer&lt;String, String&gt; producer = initProducer(brokeList); long before = System.currentTimeMillis(); System.out.println("发送前--&gt;"+before); for(int i=1 ;i&lt;10000;i++)&#123; producer.send(new ProducerRecord&lt;&gt;(topic, 0,key, String.valueOf(i))); &#125; producer.flush(); producer.close(); long after = System.currentTimeMillis(); System.out.println("发送后--&gt;"+after); long temp = after-before; System.out.println("时间间隔--&gt;"+temp); &#125; /** * 同步发送-(通过get方法等待Kafka的响应，判断消息是否发送成功) */ public static void wayTwo() &#123; String brokeList = "39.96.117.232:9092"; String topic = "testTopic"; String key = "testKey"; Producer&lt;String, String&gt; producer = initProducer(brokeList); long before = System.currentTimeMillis(); System.out.println("发送前--&gt;"+before); for(int i=1 ;i&lt;10000;i++)&#123; Future&lt;RecordMetadata&gt; recordMetadataFuture = producer.send(new ProducerRecord&lt;&gt;(topic,key, String.valueOf(i))); try &#123; RecordMetadata record = recordMetadataFuture.get(10, TimeUnit.MICROSECONDS); System.out.println(record.toString()); &#125;catch (Exception e)&#123; System.out.println(e); &#125; &#125; long after = System.currentTimeMillis(); System.out.println("发送后--&gt;"+after); long temp = after-before; System.out.println("时间间隔--&gt;"+temp); &#125; /** * 异步发送+回调函数(消息以异步的方式发送，通过回调函数返回消息发送成功/失败) */ public static void wayThree() &#123; String brokeList = "39.96.117.232:9092"; String topic = "testTopic"; String key = "testKey"; Producer&lt;String, String&gt; producer = initProducer(brokeList); long before = System.currentTimeMillis(); System.out.println("发送前--&gt;"+before); for(int i=1 ;i&lt;10000;i++)&#123; producer.send(new ProducerRecord&lt;&gt;(topic, key, String.valueOf(i)), new Callback() &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if(e==null)&#123; System.out.println("## 发送消息成功-&gt;"); &#125;else &#123; System.out.println("## 发送消息失败-&gt;&#123;&#125;"+e.getMessage()); &#125; &#125; &#125;); &#125; long after = System.currentTimeMillis(); System.out.println("发送后--&gt;"+after); long temp = after-before; System.out.println("时间间隔--&gt;"+temp); &#125; /** * 初始化producer * @param brokeList * @return */ private static Producer&lt;String, String&gt; initProducer(String brokeList) &#123; Properties props = new Properties(); props.put("bootstrap.servers", brokeList); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("retries", 0); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); return producer; &#125;&#125;]]></content>
      <categories>
        <category>组件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka安装与入门]]></title>
    <url>%2F2019%2F05%2F31%2FKafka%E5%AE%89%E8%A3%85%E4%B8%8E%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[介绍Kafka 是一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量 ：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 支持通过Kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。 Kafka通过官网发布了最新版本2.0.0 安装 基于Linux和macos操作系统 参考 http://kafka.apache.org/quickstart http://orchome.com/6 Step 1: 下载代码 下载2.2.0版本并解压缩 12$ tar -xzf kafka_2.12-2.2.0.tgz$ cd kafka_2.12-2.2.0 Step 2: 启动服务 运行kafka需要使用Zookeeper，所以你需要先启动Zookeeper，如果你没有Zookeeper，你可以使用kafka自带打包和配置好的Zookeeper。1$ bin/zookeeper-server-start.sh config/zookeeper.properties 现在启动kafka服务1$ nohup bin/kafka-server-start.sh config/server.properties &amp; 入门 主要介绍发送Kafka消息，消费kafka消息等简单示例代码，以及使用过程中遇到的问题和解决方案 发送kafka消息示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.ohaotian.datatransmission.core.writer.kafka;import com.alibaba.fastjson.JSONObject;import lombok.extern.log4j.Log4j2;import org.apache.kafka.clients.producer.Callback;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import java.util.Properties;/** * @Author: feifz * @Date: 2019-05-31 14:58 * @Version: 1.0 * @Description: kafka 发送消息示例 */@Log4j2public class KafkaProducerDemo &#123; public static void main(String[] args) &#123; String brokeList = "127.0.0.1:9092"; String topic = "testTopic"; String key = "testKey"; String message = "this is a test kafka message!"; Producer&lt;String, String&gt; producer = initProducer(brokeList); producer.send(new ProducerRecord&lt;&gt;(topic, key, JSONObject.toJSONString(message)), new Callback() &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (e == null) &#123; log.info("## 发送消息成功-&gt;&#123;&#125;", JSONObject.toJSONString(message)); &#125; else &#123; log.error("## 发送消息失败-&gt;&#123;&#125;", e.getMessage()); &#125; &#125; &#125;); producer.close(); &#125; /** * 初始化producer * @param brokeList * @return */ private static Producer&lt;String, String&gt; initProducer(String brokeList) &#123; Properties props = new Properties(); props.put("bootstrap.servers", brokeList); props.put("acks", "all"); props.put("retries", 0); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432L); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); return producer; &#125;&#125; 监控 参考：https://www.orchome.com/55 Kafka Manager简介为了简化开发者和服务工程师维护Kafka集群的工作，构建了一个叫做Kafka管理器的基于Web工具，叫做 Kafka Manager。这个管理工具可以很容易地发现分布在集群中的哪些topic分布不均匀，或者是分区在整个集群分布不均匀的的情况。它支持管理多个集群、选择副本、副本重新分配以及创建Topic。同时，这个管理工具也是一个非常好的可以快速浏览这个集群的工具。 该软件是用Scala语言编写的。目前(2015年02月03日)雅虎已经开源了Kafka Manager工具。这款Kafka集群管理工具主要支持以下几个功能： 管理几个不同的集群；很容易地检查集群的状态(topics, brokers, 副本的分布, 分区的分布)；选择副本；产生分区分配(Generate partition assignments)基于集群的当前状态；重新分配分区。 安装要求Kafka 0.8.. or 0.9.. or 0.10.. or 0.11..Java 8+sbt 0.13.x 配置系统至少需要配置zookeeper集群的地址，可以在kafka-manager安装包的conf目录下面的application.conf文件中进行配置。例如：1kafka-manager.zkhosts="my.zookeeper.host.com:2181" 你可以指定多个zookeeper地址，用逗号分隔：1kafka-manager.zkhosts="my.zookeeper.host.com:2181,other.zookeeper.host.com:2181" 另外, 如果你不想硬编码，可以使用环境变量ZK_HOSTS。1kafka-ZK_HOSTS="my.zookeeper.host.com:2181" 你可以启用/禁止以下的功能，通过修改application.config:1application.features=["KMClusterManagerFeature","KMTopicManagerFeature","KMPreferredReplicaElectionFeature","KMReassignPartitionsFeature"] KMClusterManagerFeature - 允许从Kafka Manager添加，更新，删除集群。KMTopicManagerFeature - 允许从Kafka集群中增加，更新，删除topicKMPreferredReplicaElectionFeature - 允许为Kafka集群运行首选副本KMReassignPartitionsFeature - 允许生成分区分配和重新分配分区考虑为启用了jmx的大群集设置这些参数： kafka-manager.broker-view-thread-pool-size=&lt; 3 * number_of_brokers&gt; kafka-manager.broker-view-max-queue-size=&lt; 3 * total # of partitions across all topics&gt; kafka-manager.broker-view-update-seconds=&lt; kafka-manager.broker-view-max-queue-size / (10 * number_of_brokers) &gt;下面是一个包含10个broker，100个topic的kafka集群示例，每个topic有10个分区，相当于1000个总分区，并启用JMX： kafka-manager.broker-view-thread-pool-size=30 kafka-manager.broker-view-max-queue-size=3000 kafka-manager.broker-view-update-seconds=30控制消费者偏offset缓存的线程池和队列： kafka-manager.offset-cache-thread-pool-size=&lt; default is # of processors&gt; kafka-manager.offset-cache-max-queue-size=&lt; default is 1000&gt; kafka-manager.kafka-admin-client-thread-pool-size=&lt; default is # of processors&gt; kafka-manager.kafka-admin-client-max-queue-size=&lt; default is 1000&gt;您应该在启用了消费者轮询的情况下为大量#消费者增加以上内容。虽然它主要影响基于ZK的消费者轮询。 Kafka管理的消费者offset现在由“__consumer_offsets”topic中的KafkaManagedOffsetCache消费。请注意，这尚未经过跟踪大量offset的测试。每个集群都有一个单独的线程消费这个topic，所以它可能无法跟上被推送到topic的大量offset。 部署下面的命令创建一个可部署应用的zip文件。1sbt clean dist 如果你不想拉源码，在编译，我已经编译好，放在百度云盘上了。 链接:https://pan.baidu.com/s/1AWQihB3CkF0g2Ao7lizTWw 密码:82eq 启动服务解压刚刚的zip文件,然后启动它:1$ bin/kafka-manager 默认情况下，端口为9000。可覆盖，例如：1$ bin/kafka-manager -Dconfig.file=/path/to/application.conf -Dhttp.port=8080 再如果java不在你的路径中，或你需要针对不同的版本，增加-java-home选项：1$ bin/kafka-manager -java-home /usr/local/oracle-java-8 用安全启动服务为SASL添加JAAS配置，添加配置文件位置：1$ bin/kafka-manager -Djava.security.auth.login.config=/path/to/my-jaas.conf 注意：确保运行kafka manager的用户有读取jaas配置文件的权限。 打包如果你想创建一个Debian或者RPM包，你可以使用下面命令打包：12sbt debian:packageBinsbt rpm:packageBin 常见问题1. 如何实现批量发送Kafka消息？生产者发送多个消息到同一个分区的时候，为了减少网络带来的系能开销，kafka会对消息进行批量发送。batch.size通过这个参数来设置批量提交的数据大小，默认是16k,当积压的消息达到这个值的时候就会统一发送（发往同一分区的消息）linger.ms这个设置是为发送设置一定是延迟来收集更多的消息，默认大小是0ms（就是有消息就立即发送）当这两个参数同时设置的时候，只要两个条件中满足一个就会发送。比如说batch.size设置16kb，linger.ms设置50ms，那么当消息积压达到16kb就会发送，如果没有到达16kb，那么在第一个消息到来之后的50ms之后消息将会发送。 2. Kafka如何保证消息的可靠性传输？这块比较常见的一个场景，就是Kafka某个broker宕机，然后重新选举partition 的leader。大家想想，要是此时其他的follower刚好还有些数据没有同步，结果此时leader 挂了，然后选举某个follower成leader之后，不就少了一些数据？这就丢了一些数据啊。生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将follower切换为 leader 之后，就会发现说这个数据就丢了。所以此时一般是要求起码设置如下 4 个参数：给topic设置replication.factor参数：这个值必须大于1，要求每个 partition必须有至少2个副本。在Kafka服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧。在producer端设置 acks=all：这个是要求每条数据，必须是写入所有replica 之后，才能认为是写成功了。在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。我们生产环境就是按照上述要求配置的，这样配置之后，至少在Kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失。生产者会不会弄丢数据？如果按照上述的思路设置了acks=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。]]></content>
      <categories>
        <category>组件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch-安装与入门]]></title>
    <url>%2F2019%2F05%2F31%2FElasticSearch%E5%AE%89%E8%A3%85%E4%B8%8E%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[介绍 ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 安装与配置(以Linux环境下 6.3.2版本为例)ElasticSearch安装-单机版 进入官网下载页选择想要安装的版本下载 解压缩至本地某地址:tar zxvf elasticsearch-6.3.2.tar.gz 修改配置文件：vim config/elasticsearch.yml 12345# 即可启用该物理机器所有网卡网络访问network.host: 0.0.0.0# 设置当前节点为master节点node.name: master 执行启动命令:./elasticsearch后台启动:./elasticsearch -d 启动成功 ElasticSearch-head插件安装 依赖环境：node和npm git clone https://github.com/mobz/elasticsearch-head.git npm install npm run start 修改ElasticSearch配置 在elasticsearch.yml添加如下配置 12http.cors.enabled: truehttp.cors.allow-origin: "* 重启ES 页面访问：http://localhost:9100/ 启动完成 验证 访问：http://localhost:9100/ 周边 Release Note :https://www.elastic.co/guide/en/elasticsearch/reference/index.html Support Matrix:https://www.elastic.co/cn/support/matrix 参考 https://blog.csdn.net/mingleizhen/article/details/76084874]]></content>
      <categories>
        <category>组件</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发-线程池的使用]]></title>
    <url>%2F2019%2F05%2F29%2FJava%E5%B9%B6%E5%8F%91-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[说明 参考：https://www.cnblogs.com/dolphin0520/p/3932921.html 如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，因为频繁创建线程和销毁线程需要时间，这样频繁创建线程就会大大降低系统的效率。 那么有没有一种办法使得线程可以复用，就是执行完一个任务，并不被销毁，而是可以继续执行其他的任务？ 在Java中可以通过线程池来达到这样的效果。今天我们就来详细讲解一下Java的线程池，首先我们从最核心的ThreadPoolExecutor类中的方法讲起，然后再讲述它的实现原理，接着给出了它的使用示例，最后讨论了一下如何合理配置线程池的大小。 Java中的ThreadPoolExecutor类 java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类，因此如果要透彻地了解Java中的线程池，必须先了解这个类。下面我们来看一下ThreadPoolExecutor类的具体实现源码。 在ThreadPoolExecutor类中提供了四个构造方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// Public constructors and methods public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); &#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler); &#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler); &#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; 从上面的代码可以得知，ThreadPoolExecutor继承了AbstractExecutorService类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。 下面解释下一下构造器中各个参数的含义： corePoolSize：核心池的大小，这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性： 1234567TimeUnit.DAYS; //天TimeUnit.HOURS; //小时TimeUnit.MINUTES; //分钟TimeUnit.SECONDS; //秒TimeUnit.MILLISECONDS; //毫秒TimeUnit.MICROSECONDS; //微妙TimeUnit.NANOSECONDS; //纳秒 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择： ArrayBlockingQueue和PriorityBlockingQueue使用较少，一般使用LinkedBlockingQueue和Synchronous。线程池的排队策略与BlockingQueue有关。 1234ArrayBlockingQueue;LinkedBlockingQueue;SynchronousQueue;PriorityBlockingQueue threadFactory：线程工厂，主要用来创建线程； handler：表示当拒绝处理任务时的策略，有以下四种取值： 1234ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 具体参数的配置与线程池的关系将在下一节讲述。 从上面给出的ThreadPoolExecutor类的代码可以知道，ThreadPoolExecutor继承了AbstractExecutorService，我们来看一下AbstractExecutorService的实现： 123456789101112131415161718192021222324252627public abstract class AbstractExecutorService implements ExecutorService &#123; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; &#125;; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; &#125;; public Future&lt;?&gt; submit(Runnable task) &#123;&#125;; public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; &#125;; public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; &#125;; private &lt;T&gt; T doInvokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException &#123; &#125;; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException &#123; &#125;; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; &#125;; public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException &#123; &#125;; public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException &#123; &#125;;&#125; AbstractExecutorService是一个抽象类，它实现了ExecutorService接口。 我们接着看ExecutorService接口的实现： 12345678910111213141516171819202122public interface ExecutorService extends Executor &#123; void shutdown(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 而ExecutorService又是继承了Executor接口，我们看一下Executor接口的实现： 123public interface Executor &#123; void execute(Runnable command);&#125; 到这里，大家应该明白了ThreadPoolExecutor、AbstractExecutorService、ExecutorService和Executor几个之间的关系了。 Executor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)，返回值为void，参数为Runnable类型，从字面意思可以理解，就是用来执行传进去的任务的； 然后ExecutorService接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等； 抽象类AbstractExecutorService实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法； 然后ThreadPoolExecutor继承了类AbstractExecutorService。 在ThreadPoolExecutor类中有几个非常重要的方法： 1234execute()submit()shutdown()shutdownNow() execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit()方法是在ExecutorService中声明的方法，在AbstractExecutorService就已经有了具体的实现，在ThreadPoolExecutor中并没有对其进行重写，这个方法也是用来向线程池提交任务的，但是它和execute()方法不同，它能够返回任务执行的结果，去看submit()方法的实现，会发现它实际上还是调用的execute()方法，只不过它利用了Future来获取任务执行结果（Future相关内容将在下一篇讲述）。 shutdown()和shutdownNow()是用来关闭线程池的。 还有很多其他的方法： 比如：getQueue() 、getPoolSize() 、getActiveCount()、getCompletedTaskCount()等获取与线程池相关属性的方法，有兴趣的朋友可以自行查阅API。 深入剖析线程池实现原理在上一节我们从宏观上介绍了ThreadPoolExecutor，下面我们来深入解析一下线程池的具体实现原理，将从下面几个方面讲解： 1.线程池状态 2.任务的执行 3.线程池中的线程初始化 4.任务缓存队列及排队策略 5.任务拒绝策略 6.线程池的关闭 7.线程池容量的动态调整 1.线程池状态 在ThreadPoolExecutor中定义了一个volatile变量，另外定义了几个static final变量表示线程池的各个状态： 12345volatile int runState;static final int RUNNING = 0;static final int SHUTDOWN = 1;static final int STOP = 2;static final int TERMINATED = 3; runState表示当前线程池的状态，它是一个volatile变量用来保证线程之间的可见性； 下面的几个static final变量表示runState可能的几个取值。 当创建线程池后，初始时，线程池处于RUNNING状态； 如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕； 如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务； 当线程池处于SHUTDOWN或STOP状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为TERMINATED状态。 2.任务的执行 在了解将任务提交给线程池到任务执行完毕整个过程之前，我们先来看一下ThreadPoolExecutor类中其他的一些比较重要成员变量： 12345678910111213141516171819private final BlockingQueue&lt;Runnable&gt; workQueue; //任务缓存队列，用来存放等待执行的任务private final ReentrantLock mainLock = new ReentrantLock(); //线程池的主要状态锁，对线程池状态（比如线程池大小 //、runState等）的改变都要使用这个锁private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); //用来存放工作集 private volatile long keepAliveTime; //线程存货时间 private volatile boolean allowCoreThreadTimeOut; //是否允许为核心线程设置存活时间private volatile int corePoolSize; //核心池的大小（即线程池中的线程数目大于这个参数时，提交的任务会被放进任务缓存队列）private volatile int maximumPoolSize; //线程池最大能容忍的线程数 private volatile int poolSize; //线程池中当前的线程数 private volatile RejectedExecutionHandler handler; //任务拒绝策略 private volatile ThreadFactory threadFactory; //线程工厂，用来创建线程 private int largestPoolSize; //用来记录线程池中曾经出现过的最大线程数 private long completedTaskCount; //用来记录已经执行完毕的任务个数 每个变量的作用都已经标明出来了，这里要重点解释一下corePoolSize、maximumPoolSize、largestPoolSize三个变量。 corePoolSize在很多地方被翻译成核心池大小，其实我的理解这个就是线程池的大小。举个简单的例子： 假如有一个工厂，工厂里面有10个工人，每个工人同时只能做一件任务。 因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做； 当10个工人都有任务在做时，如果还来了任务，就把任务进行排队等待； 如果说新任务数目增长的速度远远大于工人做任务的速度，那么此时工厂主管可能会想补救措施，比如重新招4个临时工人进来； 然后就将任务也分配给这4个临时工人做； 如果说着14个工人做任务的速度还是不够，此时工厂主管可能就要考虑不再接收新的任务或者抛弃前面的一些任务了。 当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管可能就考虑辞掉4个临时工了，只保持原来的10个工人，毕竟请额外的工人是要花钱的。 这个例子中的corePoolSize就是10，而maximumPoolSize就是14（10+4）。 也就是说corePoolSize就是线程池大小，maximumPoolSize在我看来是线程池的一种补救措施，即任务量突然过大时的一种补救措施。 不过为了方便理解，在本文后面还是将corePoolSize翻译成核心池大小。 largestPoolSize只是一个用来起记录作用的变量，用来记录线程池中曾经有过的最大线程数目，跟线程池的容量没有任何关系。 下面我们进入正题，看一下任务从提交到最终执行完毕经历了哪些过程。 在ThreadPoolExecutor类中，最核心的任务提交方法是execute()方法，虽然通过submit也可以提交任务，但是实际上submit方法里面最终调用的还是execute()方法，所以我们只需要研究execute()方法的实现原理即可： 123456789101112public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) &#123; if (runState == RUNNING &amp;&amp; workQueue.offer(command)) &#123; if (runState != RUNNING || poolSize == 0) ensureQueuedTaskHandled(command); &#125; else if (!addIfUnderMaximumPoolSize(command)) reject(command); // is shutdown or saturated &#125;&#125; 上面的代码可能看起来不是那么容易理解，下面我们一句一句解释： 首先，判断提交的任务command是否为null，若是null，则抛出空指针异常； 接着是这句，这句要好好理解一下： 1`if` `(poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command))` 由于是或条件运算符，所以先计算前半部分的值，如果线程池中当前线程数不小于核心池大小，那么就会直接进入下面的if语句块了。 如果线程池中当前线程数小于核心池大小，则接着执行后半部分，也就是执行 1`addIfUnderCorePoolSize(command)` 如果执行完addIfUnderCorePoolSize这个方法返回false，则继续执行下面的if语句块，否则整个方法就直接执行完毕了。 如果执行完addIfUnderCorePoolSize这个方法返回true，然后接着判断： 1`if` `(runState == RUNNING &amp;&amp; workQueue.offer(command))` 如果当前线程池处于RUNNING状态，则将任务放入任务缓存队列；如果当前线程池不处于RUNNING状态或者任务放入缓存队列失败，则执行： 1`addIfUnderMaximumPoolSize(command)` 如果执行addIfUnderMaximumPoolSize方法失败，则执行reject()方法进行任务拒绝处理。 回到前面： 1`if` `(runState == RUNNING &amp;&amp; workQueue.offer(command))` 这句的执行，如果说当前线程池处于RUNNING状态且将任务放入任务缓存队列成功，则继续进行判断： 1`if` `(runState != RUNNING || poolSize == ``0``)` 这句判断是为了防止在将此任务添加进任务缓存队列的同时其他线程突然调用shutdown或者shutdownNow方法关闭了线程池的一种应急措施。如果是这样就执行： 1`ensureQueuedTaskHandled(command)` 进行应急处理，从名字可以看出是保证 添加到任务缓存队列中的任务得到处理。 我们接着看2个关键方法的实现：addIfUnderCorePoolSize和addIfUnderMaximumPoolSize： 1`private` `boolean` `addIfUnderCorePoolSize(Runnable firstTask) &#123;`` ``Thread t = ``null``;`` ``final` `ReentrantLock mainLock = ``this``.mainLock;`` ``mainLock.lock();`` ``try` `&#123;`` ``if` `(poolSize &lt; corePoolSize &amp;&amp; runState == RUNNING)`` ``t = addThread(firstTask); ``//创建线程去执行firstTask任务 `` ``&#125; ``finally` `&#123;`` ``mainLock.unlock();`` ``&#125;`` ``if` `(t == ``null``)`` ``return` `false``;`` ``t.start();`` ``return` `true``;``&#125;` 这个是addIfUnderCorePoolSize方法的具体实现，从名字可以看出它的意图就是当低于核心池大小时执行的方法。下面看其具体实现，首先获取到锁，因为这地方涉及到线程池状态的变化，先通过if语句判断当前线程池中的线程数目是否小于核心池大小，有朋友也许会有疑问：前面在execute()方法中不是已经判断过了吗，只有线程池当前线程数目小于核心池大小才会执行addIfUnderCorePoolSize方法的，为何这地方还要继续判断？原因很简单，前面的判断过程中并没有加锁，因此可能在execute方法判断的时候poolSize小于corePoolSize，而判断完之后，在其他线程中又向线程池提交了任务，就可能导致poolSize不小于corePoolSize了，所以需要在这个地方继续判断。然后接着判断线程池的状态是否为RUNNING，原因也很简单，因为有可能在其他线程中调用了shutdown或者shutdownNow方法。然后就是执行 1`t = addThread(firstTask);` 这个方法也非常关键，传进去的参数为提交的任务，返回值为Thread类型。然后接着在下面判断t是否为空，为空则表明创建线程失败（即poolSize&gt;=corePoolSize或者runState不等于RUNNING），否则调用t.start()方法启动线程。 我们来看一下addThread方法的实现： 1`private` `Thread addThread(Runnable firstTask) &#123;`` ``Worker w = ``new` `Worker(firstTask);`` ``Thread t = threadFactory.newThread(w); ``//创建一个线程，执行任务 `` ``if` `(t != ``null``) &#123;`` ``w.thread = t; ``//将创建的线程的引用赋值为w的成员变量 `` ``workers.add(w);`` ``int` `nt = ++poolSize; ``//当前线程数加1 `` ``if` `(nt &gt; largestPoolSize)`` ``largestPoolSize = nt;`` ``&#125;`` ``return` `t;``&#125;` 在addThread方法中，首先用提交的任务创建了一个Worker对象，然后调用线程工厂threadFactory创建了一个新的线程t，然后将线程t的引用赋值给了Worker对象的成员变量thread，接着通过workers.add(w)将Worker对象添加到工作集当中。 下面我们看一下Worker类的实现： 1`private` `final` `class` `Worker ``implements` `Runnable &#123;`` ``private` `final` `ReentrantLock runLock = ``new` `ReentrantLock();`` ``private` `Runnable firstTask;`` ``volatile` `long` `completedTasks;`` ``Thread thread;`` ``Worker(Runnable firstTask) &#123;`` ``this``.firstTask = firstTask;`` ``&#125;`` ``boolean` `isActive() &#123;`` ``return` `runLock.isLocked();`` ``&#125;`` ``void` `interruptIfIdle() &#123;`` ``final` `ReentrantLock runLock = ``this``.runLock;`` ``if` `(runLock.tryLock()) &#123;`` ``try` `&#123;`` ``if` `(thread != Thread.currentThread())`` ``thread.interrupt();`` ``&#125; ``finally` `&#123;`` ``runLock.unlock();`` ``&#125;`` ``&#125;`` ``&#125;`` ``void` `interruptNow() &#123;`` ``thread.interrupt();`` ``&#125;` ` ``private` `void` `runTask(Runnable task) &#123;`` ``final` `ReentrantLock runLock = ``this``.runLock;`` ``runLock.lock();`` ``try` `&#123;`` ``if` `(runState &lt; STOP &amp;&amp;`` ``Thread.interrupted() &amp;&amp;`` ``runState &gt;= STOP)`` ``boolean` `ran = ``false``;`` ``beforeExecute(thread, task); ``//beforeExecute方法是ThreadPoolExecutor类的一个方法，没有具体实现，用户可以根据`` ``//自己需要重载这个方法和后面的afterExecute方法来进行一些统计信息，比如某个任务的执行时间等 `` ``try` `&#123;`` ``task.run();`` ``ran = ``true``;`` ``afterExecute(task, ``null``);`` ``++completedTasks;`` ``&#125; ``catch` `(RuntimeException ex) &#123;`` ``if` `(!ran)`` ``afterExecute(task, ex);`` ``throw` `ex;`` ``&#125;`` ``&#125; ``finally` `&#123;`` ``runLock.unlock();`` ``&#125;`` ``&#125;` ` ``public` `void` `run() &#123;`` ``try` `&#123;`` ``Runnable task = firstTask;`` ``firstTask = ``null``;`` ``while` `(task != ``null` `|| (task = getTask()) != ``null``) &#123;`` ``runTask(task);`` ``task = ``null``;`` ``&#125;`` ``&#125; ``finally` `&#123;`` ``workerDone(``this``); ``//当任务队列中没有任务时，进行清理工作 `` ``&#125;`` ``&#125;``&#125;` 它实际上实现了Runnable接口，因此上面的Thread t = threadFactory.newThread(w);效果跟下面这句的效果基本一样： 1`Thread t = ``new` `Thread(w);` 相当于传进去了一个Runnable任务，在线程t中执行这个Runnable。 既然Worker实现了Runnable接口，那么自然最核心的方法便是run()方法了： 1`public` `void` `run() &#123;`` ``try` `&#123;`` ``Runnable task = firstTask;`` ``firstTask = ``null``;`` ``while` `(task != ``null` `|| (task = getTask()) != ``null``) &#123;`` ``runTask(task);`` ``task = ``null``;`` ``&#125;`` ``&#125; ``finally` `&#123;`` ``workerDone(``this``);`` ``&#125;``&#125;` 从run方法的实现可以看出，它首先执行的是通过构造器传进来的任务firstTask，在调用runTask()执行完firstTask之后，在while循环里面不断通过getTask()去取新的任务来执行，那么去哪里取呢？自然是从任务缓存队列里面去取，getTask是ThreadPoolExecutor类中的方法，并不是Worker类中的方法，下面是getTask方法的实现： 1`Runnable getTask() &#123;`` ``for` `(;;) &#123;`` ``try` `&#123;`` ``int` `state = runState;`` ``if` `(state &gt; SHUTDOWN)`` ``return` `null``;`` ``Runnable r;`` ``if` `(state == SHUTDOWN) ``// Help drain queue`` ``r = workQueue.poll();`` ``else` `if` `(poolSize &gt; corePoolSize || allowCoreThreadTimeOut) ``//如果线程数大于核心池大小或者允许为核心池线程设置空闲时间，`` ``//则通过poll取任务，若等待一定的时间取不到任务，则返回null`` ``r = workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS);`` ``else`` ``r = workQueue.take();`` ``if` `(r != ``null``)`` ``return` `r;`` ``if` `(workerCanExit()) &#123; ``//如果没取到任务，即r为null，则判断当前的worker是否可以退出`` ``if` `(runState &gt;= SHUTDOWN) ``// Wake up others`` ``interruptIdleWorkers(); ``//中断处于空闲状态的worker`` ``return` `null``;`` ``&#125;`` ``// Else retry`` ``&#125; ``catch` `(InterruptedException ie) &#123;`` ``// On interruption, re-check runState`` ``&#125;`` ``&#125;``&#125;` 在getTask中，先判断当前线程池状态，如果runState大于SHUTDOWN（即为STOP或者TERMINATED），则直接返回null。 如果runState为SHUTDOWN或者RUNNING，则从任务缓存队列取任务。 如果当前线程池的线程数大于核心池大小corePoolSize或者允许为核心池中的线程设置空闲存活时间，则调用poll(time,timeUnit)来取任务，这个方法会等待一定的时间，如果取不到任务就返回null。 然后判断取到的任务r是否为null，为null则通过调用workerCanExit()方法来判断当前worker是否可以退出，我们看一下workerCanExit()的实现： 1`private` `boolean` `workerCanExit() &#123;`` ``final` `ReentrantLock mainLock = ``this``.mainLock;`` ``mainLock.lock();`` ``boolean` `canExit;`` ``//如果runState大于等于STOP，或者任务缓存队列为空了`` ``//或者 允许为核心池线程设置空闲存活时间并且线程池中的线程数目大于1`` ``try` `&#123;`` ``canExit = runState &gt;= STOP ||`` ``workQueue.isEmpty() ||`` ``(allowCoreThreadTimeOut &amp;&amp;`` ``poolSize &gt; Math.max(``1``, corePoolSize));`` ``&#125; ``finally` `&#123;`` ``mainLock.unlock();`` ``&#125;`` ``return` `canExit;``&#125;` 也就是说如果线程池处于STOP状态、或者任务队列已为空或者允许为核心池线程设置空闲存活时间并且线程数大于1时，允许worker退出。如果允许worker退出，则调用interruptIdleWorkers()中断处于空闲状态的worker，我们看一下interruptIdleWorkers()的实现： 1`void` `interruptIdleWorkers() &#123;`` ``final` `ReentrantLock mainLock = ``this``.mainLock;`` ``mainLock.lock();`` ``try` `&#123;`` ``for` `(Worker w : workers) ``//实际上调用的是worker的interruptIfIdle()方法`` ``w.interruptIfIdle();`` ``&#125; ``finally` `&#123;`` ``mainLock.unlock();`` ``&#125;``&#125;` 从实现可以看出，它实际上调用的是worker的interruptIfIdle()方法，在worker的interruptIfIdle()方法中： 1`void` `interruptIfIdle() &#123;`` ``final` `ReentrantLock runLock = ``this``.runLock;`` ``if` `(runLock.tryLock()) &#123; ``//注意这里，是调用tryLock()来获取锁的，因为如果当前worker正在执行任务，锁已经被获取了，是无法获取到锁的`` ``//如果成功获取了锁，说明当前worker处于空闲状态`` ``try` `&#123;`` ``if` `(thread != Thread.currentThread()) `` ``thread.interrupt();`` ``&#125; ``finally` `&#123;`` ``runLock.unlock();`` ``&#125;`` ``&#125;``&#125;` 这里有一个非常巧妙的设计方式，假如我们来设计线程池，可能会有一个任务分派线程，当发现有线程空闲时，就从任务缓存队列中取一个任务交给空闲线程执行。但是在这里，并没有采用这样的方式，因为这样会要额外地对任务分派线程进行管理，无形地会增加难度和复杂度，这里直接让执行完任务的线程去任务缓存队列里面取任务来执行。 我们再看addIfUnderMaximumPoolSize方法的实现，这个方法的实现思想和addIfUnderCorePoolSize方法的实现思想非常相似，唯一的区别在于addIfUnderMaximumPoolSize方法是在线程池中的线程数达到了核心池大小并且往任务队列中添加任务失败的情况下执行的： 1`private` `boolean` `addIfUnderMaximumPoolSize(Runnable firstTask) &#123;`` ``Thread t = ``null``;`` ``final` `ReentrantLock mainLock = ``this``.mainLock;`` ``mainLock.lock();`` ``try` `&#123;`` ``if` `(poolSize &lt; maximumPoolSize &amp;&amp; runState == RUNNING)`` ``t = addThread(firstTask);`` ``&#125; ``finally` `&#123;`` ``mainLock.unlock();`` ``&#125;`` ``if` `(t == ``null``)`` ``return` `false``;`` ``t.start();`` ``return` `true``;``&#125;` 看到没有，其实它和addIfUnderCorePoolSize方法的实现基本一模一样，只是if语句判断条件中的poolSize &lt; maximumPoolSize不同而已。 到这里，大部分朋友应该对任务提交给线程池之后到被执行的整个过程有了一个基本的了解，下面总结一下： 1）首先，要清楚corePoolSize和maximumPoolSize的含义； 2）其次，要知道Worker是用来起到什么作用的； 3）要知道任务提交给线程池之后的处理策略，这里总结一下主要有4点： 如果当前线程池中的线程数目小于corePoolSize，则每来一个任务，就会创建一个线程去执行这个任务； 如果当前线程池中的线程数目&gt;=corePoolSize，则每来一个任务，会尝试将其添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务； 如果当前线程池中的线程数目达到maximumPoolSize，则会采取任务拒绝策略进行处理； 如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止。 3.线程池中的线程初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。 在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 下面是这2个方法的实现： 1`public` `boolean` `prestartCoreThread() &#123;`` ``return` `addIfUnderCorePoolSize(``null``); ``//注意传进去的参数是null``&#125;` `public` `int` `prestartAllCoreThreads() &#123;`` ``int` `n = ``0``;`` ``while` `(addIfUnderCorePoolSize(``null``))``//注意传进去的参数是null`` ``++n;`` ``return` `n;``&#125;` 注意上面传进去的参数是null，根据第2小节的分析可知如果传进去的参数为null，则最后执行线程会阻塞在getTask方法中的 1`r = workQueue.take();` 即等待任务队列中有任务。 4.任务缓存队列及排队策略 在前面我们多次提到了任务缓存队列，即workQueue，它用来存放等待执行的任务。 workQueue的类型为BlockingQueue，通常可以取下面三种类型： 1）ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小； 2）LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE； 3）synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 5.任务拒绝策略 当线程池的任务缓存队列已满并且线程池中的线程数目达到maximumPoolSize，如果还有任务到来就会采取任务拒绝策略，通常有以下四种策略： 1`ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。``ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。``ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）``ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务` 6.线程池的关闭 ThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()，其中： shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 7.线程池容量的动态调整 ThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize()， setCorePoolSize：设置核心池大小 setMaximumPoolSize：设置线程池最大能创建的线程数目大小 当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。 使用示例前面我们讨论了关于线程池的实现原理，这一节我们来看一下它的具体使用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.ohaotian.feifz.style.study.thread;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * @author feifz * @version 1.0.0 * @Description 线程池示例 * @createTime 2019年05月28日 16:31:00 */public class ThreadDemoTest &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); for (int i = 0; i &lt; 15; i++) &#123; MyTask myTask = new MyTask(i); executor.execute(myTask); System.out.println("线程池中线程数目：" + executor.getPoolSize() + "，队列中等待执行的任务数目：" + executor.getQueue().size() + "，已执行玩别的任务数目：" + executor.getCompletedTaskCount()); &#125; executor.shutdown(); &#125;&#125;class MyTask implements Runnable &#123; private int taskNum; public MyTask(int num) &#123; this.taskNum = num; &#125; @Override public void run() &#123; System.out.println("正在执行task " + taskNum); try &#123; Thread.currentThread().sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("task " + taskNum + "执行完毕"); &#125;&#125; 从执行结果可以看出，当线程池中线程的数目大于5时，便将任务放入任务缓存队列里面，当任务缓存队列满了之后，便创建新的线程。如果上面程序中，将for循环中改成执行20个任务，就会抛出任务拒绝异常了。 不过在java doc中，并不提倡我们直接使用ThreadPoolExecutor，而是使用Executors类中提供的几个静态方法来创建线程池： 123Executors.newCachedThreadPool(); //创建一个缓冲池，缓冲池容量大小为Integer.MAX_VALUEExecutors.newSingleThreadExecutor(); //创建容量为1的缓冲池Executors.newFixedThreadPool(int); //创建固定容量大小的缓冲池 下面是这三个静态方法的具体实现; 12345678910111213141516public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 从它们的具体实现来看，它们实际上也是调用了ThreadPoolExecutor，只不过参数都已配置好了。 newFixedThreadPool创建的线程池corePoolSize和maximumPoolSize值是相等的，它使用的LinkedBlockingQueue； newSingleThreadExecutor将corePoolSize和maximumPoolSize都设置为1，也使用的LinkedBlockingQueue； newCachedThreadPool将corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE，使用的SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过60秒，就销毁线程。 实际中，如果Executors提供的三个静态方法能满足要求，就尽量使用它提供的三个方法，因为自己去手动配置ThreadPoolExecutor的参数有点麻烦，要根据实际任务的类型和数量来进行配置。 另外，如果ThreadPoolExecutor达不到要求，可以自己继承ThreadPoolExecutor类进行重写。 如何合理配置线程池的大小本节来讨论一个比较重要的话题：如何合理配置线程池大小，仅供参考。 一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。]]></content>
      <tags>
        <tag>Java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown入门指南]]></title>
    <url>%2F2019%2F05%2F12%2FUntitled%2F</url>
    <content type="text"><![CDATA[标题而在 Markdown 中，你只需要在文本前面加上 # 即可，同理、你还可以增加二级标题、三级标题、四级标题、五级标题和六级标题，总共六级，只需要增加 # 即可，标题字号相应降低。例如：123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 注：# 和「一级标题」之间建议保留一个字符的空格，这是最标准的 Markdown 写法。 列表列表格式也很常用，在 Markdown 中，你只需要在文字前面加上 - 就可以了，例如： 代码： 123- 文本1- 文本2- 文本3 效果： 文本1 文本2 文本3 如果你希望有序列表，也可以在文字前面加上 1. 2. 3. 就可以了，例如：代码： 1231. 文本12. 文本23. 文本3 效果： 文本1 文本2 文本3 注：-、1.和文本之间要保留一个字符的空格。 插入链接和图片在 Markdown 中，插入链接不需要其他按钮，你只需要使用 显示文本 这样的语法即可，例如：代码： 1[简书](http://www.jianshu.com) 效果： 简书 在 Markdown 中，插入图片不需要其他按钮，你只需要使用 这样的语法即可，例如：代码： 1![](http://upload-images.jianshu.io/upload_images/259-0ad0d0bfc1c608b6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 效果： 注：插入图片的语法和链接的语法很像，只是前面多了一个 ！。 引用在我们写作的时候经常需要引用他人的文字，这个时候引用这个格式就很有必要了，在 Markdown 中，你只需要在你希望引用的文字前面加上 &gt; 就好了，例如： 代码： 1&gt; 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 效果： 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 注：&gt; 和文本之间要保留一个字符的空格。 粗体和斜体Markdown 的粗体和斜体也非常简单，用两个 包含一段文本就是粗体的语法，用一个 包含一段文本就是斜体的语法。例如： 代码： 1*一盏灯*， 一片昏黄；**一简书**， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 效果： 一盏灯， 一片昏黄；一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 最终显示的就是下文，其中「一盏灯」是斜体，「一简书」是粗体： 代码引用12需要引用代码时，如果引用的语句只有一段，不分行，可以用 ` 将语句包起来。如果引用的语句为多行，可以将```置于这段代码的首行和末行。 表格相关代码： 12345| Tables | Are | Cool || ------------- |:-------------:| -----:|| col 3 is | right-aligned | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 | 显示效果： Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 显示链接中带括号的图片代码如下:12![][1][1]: http://latex.codecogs.com/gif.latex?\prod%20\(n_&#123;i&#125;\)+1 效果如下：![][1][1]: http://latex.codecogs.com/gif.latex?\prod%20\(n_{i}\)+1 参考 http://itmyhome.com/markdown/index.html https://www.jianshu.com/p/q81RER]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>makedown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo入门到进阶]]></title>
    <url>%2F2019%2F05%2F12%2FHexo%20%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[什么是 Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 官方文档(这里主要记录本人在使用Hexo过程中遇到的问题和解决方案) 基本使用init1$ hexo init [folder] 新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。 new1$ hexo new [layout] &lt;title&gt; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。 1$ hexo new "post title with whitespace" generate1$ hexo generate 生成静态文件。 选项 描述 -d, --deploy 文件生成后立即部署网站 -w, --watch 监视文件变动 该命令可以简写为1$ hexo g publish1$ hexo publish [layout] &lt;filename&gt; 发表草稿。 server1$ hexo server 启动服务器。默认情况下，访问网址为： http://localhost:4000/。 选项 描述 -p, --port 重设端口 -s, --static 只使用静态文件 -l, --log 启动日记记录，使用覆盖记录格式 该命令可以简写为1234567$ hexo s`## deploy``` bash$ hexo deploy 部署网站。 参数 描述 -g, --generate 部署之前预先生成静态文件 该命令可以简写为：1$ hexo d render1$ hexo render &lt;file1&gt; [file2] ... 渲染文件。 参数 描述 -o, --output 设置输出路径 migrate1$ hexo migrate &lt;type&gt; 从其他博客系统 迁移内容。 clean1$ hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。 在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。 list1$ hexo list &lt;type&gt; 列出网站资料。 version1$ hexo version 插件Hexo-admin插件介绍hexo-admin 是一个Hexo博客引擎的管理用户界面插件。这个插件最初是作为本地编辑器设计的，在本地运行hexo使用hexo-admin编写文章，然后通过hexo g或hexo d（hexo g是本地渲染，hexo d是将渲染的静态页面发布到GitHub）将生成的静态页面发布到GitHub等静态服务器。如果你使用的是非静态托管服务器，比如自己买的主机搭建的hexo，那么一定要设置hexo-admin 的密码，否则谁都可以编辑你的文章。 插件安装首先进入hexo创建的博客项目的根目录下，执行 123456789 npm install --save hexo-admin``` mac可能需要root权限，前面加个sudo 就可以了。如果报错缺少组件，则缺少什么安装什么，npm install 加缺少的组件。运行下列命令启动hexo-admin ：``` bash hexo server -d 打开 http://localhost:4000/admin/ 就可以访问到hexo-admin管理页面了。 密码保护打开setting，点击Setup authentification here输入用户名，密码，密钥，下面会自动生成配置文件，复制加在hexo根目录下的_config.yml中： 1234admin: username: username password_hash: be121740bf988b2225a313fa1f107ca1 secret: secret 重启hexo，就可以看到登录页面了 发布文章进入后台之后点击Deploy，里面的Deploy按钮是用来执行发布脚本的，所以我们先在博客根目录下新建个目录admin_script，然后在目录中新建一个脚本hexo-g.sh，里面写下下面代码然后保存. 1hexo g &amp;&amp; hexo d 然后在_config.yml中的admin下添加 12345admin: username: username password_hash: be121740bf988b2225a313fa1f107ca1 secret: secret deployCommand: ./admin_script/hexo-g.sh 设置发布执行的脚本，点击Deploy就会执行这个命令并提交到GitHub上。 Hexo给NexT主题内添加页面点击出现爱心的效果 创建js文件 在/themes/next/source/js下新建文件clicklove.js，接着把下面的代码拷贝粘贴到clicklove.js文件中。 1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 修改_layout.swig 在\themes\next\layout_layout.swig文件末尾添加： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type="text/javascript" src="/js/clicklove.js"&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
